[<div class="page"><p></p>
<p>Tue Herlau, Mikkel N. Schmidt and Morten Mørup
</p>
<p>Introduction to Machine Learning
and Data Mining
</p>
<p>Lecture notes, Fall 2019, version 1.0
</p>
<p>This document may not be redistributed. All rights belongs to
</p>
<p>the authors and DTU.
</p>
<p>September 2, 2019
</p>
<p>Technical University of Denmark</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>Notation cheat sheet
</p>
<p>Matlab var. Type Size Description
</p>
<p>X Numeric N ×M Data matrix: The rows correspond to N
data objects, each of which contains M at-
tributes.
</p>
<p>attributeNames Cell array M × 1 Attribute names: Name (string) for each of
the M attributes.
</p>
<p>N Numeric Scalar Number of data objects.
M Numeric Scalar Number of attributes.
</p>
<p>R
e
g
r
e
s
s
io
</p>
<p>n y Numeric N × 1 Dependent variable (output): For each
data object, y contains an output value
that we wish to predict.
</p>
<p>C
la
</p>
<p>s
s
if
ic
</p>
<p>a
t
io
</p>
<p>n
</p>
<p>y Numeric N × 1 Class index: For each data object, y con-
tains a class index, yn ∈ {0, 1, . . . , C − 1},
where C is the total number of classes.
</p>
<p>classNames Cell array C × 1 Class names: Name (string) for each of the
C classes.
</p>
<p>C Numeric Scalar Number of classes.
</p>
<p>C
r
o
s
s
-v
</p>
<p>a
li
d
a
t
io
</p>
<p>n All variables mentioned above appended
with train or test represent the corre-
sponding variable for the training or test
set.
</p>
<p>? train — — Training data.
? test — — Test data.
</p>
<p>This book attempts to give a concise introduction to machine-learning concepts. We believe
this is best accomplished by clearly stating what a given method actually does as a sequence of
mathematical operations, and use illustrations and text to provide an intuition. We will therefore
make use of tools from linear algebra, probability theory and analysis to describe the methods,
focusing on using as small a set of concepts as possible and strive towards maximal consistency.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>VI
</p>
<p>In the following, vectors will be denoted by lower-case roman letters x,y, . . . and matrices by
bolder, upper case roman letters A,B, . . . . A superscript T denote the transpose. For instance
</p>
<p>A =
</p>
<p>[
−1 0 2
1 1 −2
</p>
<p>]
and if x =
</p>
<p>−14
1
</p>
<p> then xT = [−1 4 1] .
The ith element of a vector is written as xi and the i, j’th element of a matrix as Aij (and sometimes
Ai,j to avoid ambiguity). In the preceding example, x2 = 4 and A2,3 = −2. During this course the
observed data set, which we feed into our machine learning methods, will consist of N observations
where each observation consist of a M dimensional vector. For instance if we have N observations
x1, · · · ,xN then any given observation will consist of M numbers:
</p>
<p>x =
[
x1 . . . , xM
</p>
<p>]T
.
</p>
<p>For convenience, we will often combine the observations into an N ×M data matrix X
</p>
<p>X =
</p>
<p>x
T
1
...
xTN
</p>
<p>
in which the ith row of X corresponds to the row vector xTi . We will use this notation for our
data matrix and the rows of X will correspond to N observations and the M columns of X will
correspond to M attributes. Often each of the observations xi will come with a label or target yi
corresponding to a feature of xi which we are interested in predicting. In this case we will collect
the labels in a N -dimensional vector y and the pair (X,y) will be all the data available for the
machine learning method. A more comprehensive translation of the notation as used in this book
and in the exercises can be found in the table on the previous page. Finally, the reader should be
familiar with the big-sigma notation which allows us to conveniently write sums and products of
multiple terms:
</p>
<p>n∑
i=1
</p>
<p>f(i) = f(1) + f(2) + · · ·+ f(n− 1) + f(n)
</p>
<p>n∏
i=1
</p>
<p>f(i) = f(1)× f(2)× · · · × f(n− 1)× f(n).
</p>
<p>As an example, if f(i) = i2 and n = 4 we have
</p>
<p>4∑
i=1
</p>
<p>f(i) = 12 + 22 + 32 + 42 = 30,
</p>
<p>4∏
i=1
</p>
<p>f(i) = 12 × 22 × 32 × 42 = 576.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Course Reading Guide
</p>
<p>It is our experience that when students have difficulties understanding a topic of this course, most
often probability theory is the culprit. A reason for this is probability theory can be notationally
challenging. For instance:
</p>
<p>P (X = x|Y = y) (0.1)
</p>
<p>is, all being equal, a fairly unusual way to use an equality sign. Another reason is many students
last encountered probability theory in conjunction with an introductory statistics course, where
the main theorems are presented using the notation of stochastic variables and measure spaces.
Especially when such a course has an applied focus, there is a tendency terms such as stochastic
variables end up playing a mnemonic role; i.e. as a shorthand for which theorems or rules are
supposed to be used in a given situation. This makes it difficult for students to map their notation
onto probabilistic primitives such as events, in particular for multivariate distributions.
</p>
<p>To overcome these problems, both chapter 5 and chapter 6 will be concerned with probability
theory. The idea is to provide a ground-up introduction to probability theory with a focus on
distributions that can be represented using well-behaved density functions. We advice a reader to
make absolutely sure he or she understands the definitions in the green boxes in these chapters.
</p>
<p>The disadvantage of this approach is the amount of reading material for the first weeks may
seem excessively long, and we will therefore use stars, i.e. F, to signify a particular section (including
subsections) is of less significance, perhaps because it recaps material from other courses (such as
the introduction to linear algebra), or because it is technical in nature and is supposed to give a
more in-depth idea of what is going on (c.f. section 5.5 and section 5.4.6).
</p>
<p>We obviously advice a reader to do the assigned homework problems (see course website), but
failing that, we strongly encourage a reader to read the homework problems to get an idea about
what parts of the material is more likely to occur at the exam. The focus in the exam is to either
be able to understand the material well enough to make common-sense inferences about how they
apply in particular situations, or to concretely apply the methods/definitions to concrete situations.
Note solutions to the homework problems are included at the end of this book.
</p>
<p>Based on feedback in previous semesters, we have begun implementing colored boxes as an aid
for the reader. These boxes are used as follows:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>VIII
</p>
<p>Method: Key definitions or summaries
</p>
<p>Summarizing a method or particular relevant result. Should be fairly self-contained and
relevant as a how-to resource. Make sure you understand the content.
</p>
<p>Example: Illustration of how to do something
</p>
<p>A small (concrete) example of how to calculate something, either because it is exam relevant,
or to test how certain definitions are used in practice.
</p>
<p>Technical note: A warning or derivation
</p>
<p>Used to provide additional details which may be technical, confusing or simply a lot of work.
Easily (and sometimes best) skipped.
</p>
<p>Note the use of boxes is still work in progress and, as with all other aspects of this note, we will
be very happy to get feedback on how to best make use of them.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Contents
</p>
<p>Notation cheat sheet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . V
</p>
<p>Course reading guide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VI
</p>
<p>Part I Data: Types, Features and Visualization
</p>
<p>1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1 What is machine learning and data mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
</p>
<p>1.1.1 Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1.2 Data mining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.1.3 Relationship to artificial intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.1.4 Relationship to other disciplines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.1.5 Why should I care about machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
</p>
<p>1.2 Machine learning tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2.1 Supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.2.2 Unsupervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
1.2.3 Reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.2.4 The machine-learning toolbox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
</p>
<p>1.3 Basic terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.3.1 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.3.2 A closer look at what a model doesF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
</p>
<p>1.4 The machine learning workflow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
</p>
<p>2 Data and attribute types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
2.1 What is a dataset? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
</p>
<p>2.1.1 Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.1.2 Attribute types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
</p>
<p>2.2 Data issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
2.3 The standard data format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.4 Feature transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
</p>
<p>2.4.1 One-out-of-K coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25</p>
<p></p>
</div>, <div class="page"><p></p>
<p>X Contents
</p>
<p>2.4.2 Binarizing/thresholding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
</p>
<p>3 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.1 Projections and subspacesF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
</p>
<p>3.1.1 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.1.2 Projection onto a subspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
</p>
<p>3.2 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
3.3 Singular Value Decomposition and PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
</p>
<p>3.3.1 The PCA algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.3.2 Variance explained by the PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
</p>
<p>3.4 Applications of principal component analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.4.1 A high-dimensional example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
3.4.2 Uses of PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
</p>
<p>4 Summary statistics and measures of similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
4.1 Attribute statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
</p>
<p>4.1.1 Covariance and Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4.2 Term-document matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
4.3 Measures of distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
</p>
<p>4.3.1 The Mahalanobis Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
4.4 Measures of similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
</p>
<p>5 Discrete probabilities and information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5.1 Probability basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
</p>
<p>5.1.1 A primer on binary propositionsF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.1.2 Probabilities and plausibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
5.1.3 Basic rules of probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
5.1.4 Marginalization and Bayes’ theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
5.1.5 Mutually exclusive events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
5.1.6 Equally likely events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
</p>
<p>5.2 Discrete data and stochastic variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
5.2.1 Example: Bayes theorem and the cars dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.2.2 Generating random numbersF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
5.2.3 Expectations, mean and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
</p>
<p>5.3 Independence and conditional independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
5.4 The Bernoulli, categorical and binomial distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
</p>
<p>5.4.1 The Bernoulli distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
5.4.2 The categorical distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
5.4.3 Parameter transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.4.4 Repeated events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.4.5 A learning principle: Maximum likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
5.4.6 The binomial distributionF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
</p>
<p>5.5 Information TheoryF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
5.5.1 Measuring information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Contents XI
</p>
<p>5.5.2 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
5.5.3 Mutual information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
5.5.4 Normalized mutual information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
</p>
<p>6 Densities and models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
6.1 Probability densities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
</p>
<p>6.1.1 Multiple continuous parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
6.2 Expectations, mean and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
6.3 Examples of densities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
</p>
<p>6.3.1 The normal and multivariate normal distribution . . . . . . . . . . . . . . . . . . . . . . . . 97
6.3.2 Diagonal covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
6.3.3 The Beta distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
6.3.4 The central limit theoremF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
</p>
<p>6.4 Bayesian probabilities and machine learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
6.4.1 Choosing the prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
</p>
<p>6.5 Bayesian learning in general . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
</p>
<p>7 Data Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.1 Basic plotting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.2 What sets apart a good plot? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
7.3 Visualizing the machine-learning workflowF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
</p>
<p>7.3.1 Visualizations to understand loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
7.3.2 Use visualizations to understand mistakes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
7.3.3 Visualization to debug methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
7.3.4 Use visualization for an overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
7.3.5 Illustration of baseline and ceiling performance . . . . . . . . . . . . . . . . . . . . . . . . . . 125
7.3.6 Visualizing learning curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128</p>
<p></p>
</div>, <div class="page"><p></p>
<p>XII Contents
</p>
<p>Part II Supervised learning
</p>
<p>8 Introduction to classification and regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
8.1 Linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
</p>
<p>8.1.1 Training the linear regression model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
8.2 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
</p>
<p>8.2.1 The confusion matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
8.3 The general linear modelF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
</p>
<p>9 Tree-based methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
9.1 Classification trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
</p>
<p>9.1.1 Impurity measures and purity gains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
9.1.2 Controlling tree complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
</p>
<p>9.2 Regression trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
</p>
<p>10 Overfitting and performance evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
10.1 Cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
</p>
<p>10.1.1 A simple example, linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
10.1.2 The basic setup for cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
10.1.3 Cross-validation for quantifying generalization . . . . . . . . . . . . . . . . . . . . . . . . . . 164
10.1.4 Cross-validation for model selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
10.1.5 Two-layer cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
</p>
<p>10.2 Sequential feature selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
10.2.1 Forward Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
10.2.2 Backward Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
</p>
<p>10.3 Cross validation of time-series dataF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
10.3.1 The setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
10.3.2 Cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
10.3.3 Two-layer cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
</p>
<p>10.4 Visualizing learning curvesF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
10.4.1 The setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
</p>
<p>11 Performance evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
</p>
<p>12 Nearest neighbour methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
12.1 K-nearest neighbour classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
</p>
<p>12.1.1 A Bayesian view of the KNN classifierF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
12.2 K-nearest neighbour regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
</p>
<p>12.2.1 Higher-order KNN regressionF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
12.3 Cross-validation and nearest-neighbour methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Contents XIII
</p>
<p>13 Bayesian methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
13.1 Discriminative and generative modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
</p>
<p>13.1.1 Bayes classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
13.2 Näıve-Bayes classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
</p>
<p>13.2.1 Robust estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
13.3 Bayesian networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
</p>
<p>13.3.1 A brief comment on causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
</p>
<p>14 Regularization and the bias-variance decomposition . . . . . . . . . . . . . . . . . . . . . . . . . 211
14.1 Least squares regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
</p>
<p>14.1.1 The effect of regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
14.2 Bias-variance decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
</p>
<p>15 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
15.1 The feedforward neural network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
</p>
<p>15.1.1 Artificial neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
15.1.2 The forward pass in details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
</p>
<p>15.2 Training neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
15.2.1 Gradient DescentF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
</p>
<p>15.3 Neural networks for classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
15.3.1 Neural networks for binary classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
15.3.2 Neural networks for multi-class classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
15.3.3 Multinomial regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
15.3.4 Flexibility and cross-validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
</p>
<p>15.4 Advanced topicsF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
15.4.1 Mini-batching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
15.4.2 Convolutional neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
15.4.3 Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
15.4.4 Recurrent neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
15.4.5 Serious neural network modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
</p>
<p>16 Performance evaluation and class imbalance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
16.1 Dealing with class imbalance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
</p>
<p>16.1.1 Resampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
16.1.2 Penalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
</p>
<p>16.2 Area-under-curve (AUC) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
16.2.1 The confusion matrix and thresholding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
</p>
<p>17 Ensemble methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
17.1 Introduction to ensemble methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
17.2 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
17.3 Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
17.4 Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256</p>
<p></p>
</div>, <div class="page"><p></p>
<p>XIV Contents
</p>
<p>17.4.1 AdaBoost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
17.4.2 Properties of the AdaBoost algorithmF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Contents XV
</p>
<p>Part III Unsupervised learning
</p>
<p>18 Distance-based clustering techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
18.1 Types of clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
</p>
<p>18.1.1 The distance-based cluster types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
18.1.2 More elaborate cluster types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
</p>
<p>18.2 K-means clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
18.2.1 A closer look at the K-means algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
18.2.2 Practical issues with the K-means algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
</p>
<p>18.3 Hierarchical agglomerative clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
18.3.1 Selecting linkage function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
</p>
<p>18.4 Comparing partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
18.4.1 Rand index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
18.4.2 Jaccard similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
18.4.3 Comparing partitions using normalized mutual information . . . . . . . . . . . . . . . 283
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
</p>
<p>19 Mixture models for unsupervised clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
19.1 The Gaussian mixture model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
19.2 The EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
</p>
<p>19.2.1 Why the EM algorithm worksF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
19.2.2 Some problems with the EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
19.2.3 Selecting K for the GMM using Cross-validation . . . . . . . . . . . . . . . . . . . . . . . . 298
</p>
<p>Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
</p>
<p>20 Density estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
20.1 The kernel density estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
</p>
<p>20.1.1 Selecting the kernel width λ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
20.2 Average relative density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
</p>
<p>21 Association rule learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
21.1 Basic concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
</p>
<p>21.1.1 Itemsets and association rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
21.1.2 Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
21.1.3 Confidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
</p>
<p>21.2 The Apriori algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
21.2.1 An example of the Apriori algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
</p>
<p>21.3 Using the Apriori algorithm to find itemsets with high confidence . . . . . . . . . . . . . . . . 319
21.4 Some limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
</p>
<p>Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323</p>
<p></p>
</div>, <div class="page"><p></p>
<p>XVI Contents
</p>
<p>A Mathematical Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
Elementary notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
</p>
<p>References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Part I
</p>
<p>Data: Types, Features and Visualization</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>1
</p>
<p>Introduction
</p>
<p>In this chapter, we will try to define machine learning and data mining, as well as give a high-level
grouping of the various machine-learning methods. Understanding the different machine learning
tasks is essential in order to determine which method is suitable in a given situation.
</p>
<p>1.1 What is machine learning and data mining
</p>
<p>How can we build intelligent machines? More than 65 years ago Alan Turing made this question the
subject of his famous essay “Computing machinery and intelligence” [Turing, 1950]. Alan Turing
suggested that when we phrase the question in this manner, we unavoidably get bogged down in
the definition of the word “intelligence”. Instead, he proposed we should rather consider a different
question: Can we construct a machine that can do the same things a human can do? This may
ultimately be as hard to answer as the first question, but at least we don’t have to begin our efforts
by defining intelligence. A second part of Turing’s essay discuss how we might build such a human-
imitating machine. Turing proposed that instead of writing a computer program that behaves like
a human from scratch, we should build a machine which initially cannot do a great many things
but which can learn from past experience. For instance, if we wished to construct a machine which
translate from English to French, we should instead construct a machine which is able to learn how
to translate by observing examples of translated sentences in both languages, much like how a child
acquires language.
</p>
<p>1.1.1 Machine Learning
</p>
<p>Machine learning is the implementation of Turing’s idea: The study of algorithms which can learn
to do interesting things. The learning is based on observed data, whether from a spreadsheet, a
sensor attached to a robot or human instructions. The goal of machine learning is therefore to use
past experience to learn how to accomplish a task in such a way this learned ability generalize to
future situations of the same type, and we will simply refer to this process as learning. The focus
of machine learning is on automatic and general methods. In other words, the goal is to learn as
much as possible with as little as possible human intervention, preferably none at all.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>4 1 Introduction
</p>
<p>1.1.2 Data mining
</p>
<p>Data mining refers to the discovery of patterns or relationships in data and translating these into a
useful structure. The datasets are usually considered to be very large, possibly undergoing change
and too complicated for any human to sit down and understand them. For instance, an insurance
company may continuously collect information about its customers relating to their spending habits
and life situation. Making predictions about future events in the customers’ life, or finding similar
groups of customers, are tasks ideally suited for machine learning. In the following sections we will
nearly exclusively discuss “machine learning methods” and a student may wonder what happened
to data mining; this is partly to simplify the vocabulary and the student should keep in mind the
methods are suited for various data mining tasks.
</p>
<p>1.1.3 Relationship to artificial intelligence
</p>
<p>Artificial intelligence is the construction of intelligent, thinking machines. Machine learning is an
important subarea of artificial intelligence in that it is nearly unthinkable to have an intelligent
system that cannot learn from past experience. Furthermore, it is arguably true that machine
learning is the research area where most progress towards truly intelligent artificial systems is
currently being made.
</p>
<p>1.1.4 Relationship to other disciplines
</p>
<p>Machine learning draws on a number of disciplines including most importantly mathematics, com-
puter science and statistics. A basic knowledge of these subjects is required to get started, and
specialization in machine learning will require good knowledge in at least one (though not nec-
essarily all) of these subjects. However, this course will focus on the underlying machine-learning
concepts to make the course material as accessible as possible for non-expert students.
</p>
<p>It is worth mentioning that biology has an important role in machine learning and researchers
are inspired both by evolution as an information-creating principle (this is known as evolutionary
computing) as well as how the human brain processes and store information. The latter is known
as (artificial) neural computing or artificial neural networks.
</p>
<p>Furthermore, machine learning is a very broad subject which caters to very different types of
researchers. At the same conference there will be mathematicians who present theoretical results
(such as a mathematical analysis of a particular algorithm), very practically oriented computer
scientists who have implemented a neural network on a low-power smartphone, a neuroscientist
who uses machine learning to analyse brain data and a biologist who works with cancer genetics
just to mention a few examples.
</p>
<p>1.1.5 Why should I care about machine learning
</p>
<p>We might not notice, but machine learning is becoming more and more pervasive in our society
these years. Today, a person can use automatically trained speech recognition to order a product on
an online shop which he learned about in an advertisement, which was specifically tailored to him by
a machine-learning recommender system, and pay with a credit card that is automatically checked
for fraud. All these steps involve machine learning; however it is just the tip of the iceberg. Artificial
intelligence systems for self-driving cars can accomplish many transportation tasks, computers can</p>
<p></p>
</div>, <div class="page"><p></p>
<p>1.2 Machine learning tasks 5
</p>
<p>learn to automatically play video games better than humans and beat the grandmaster of Jeopardy.
They can correctly recognize if an image contains a rock or an armadillo and learn to translate
sentences from two languages with no expert input or initial knowledge of grammar. These are just
a few examples of things that can be accomplished today!
</p>
<p>We might still think these tasks, impressive and useful as they may be, have little to do with
us in our professional lives. However, since the machine learning methods are general, the same
algorithms that can classify observations into 20 000 categories can be used to solve much simpler
data analysis problems we might encounter in our every-day life. To give an example, suppose Susi
is an electrical engineer who is in charge of maintaining a hundred wind turbines. The wind turbines
already register a lot of data (wind speed, amount of electricity generated, vibrations, etc.), and
Susi notices that if the vibrations for a wind-turbine exceed a certain threshold even if the wind is
not very strong, the turbine is likely to become faulty in the near future. Accordingly, she writes a
small program: If vibrations exceed level x on a day where the wind is no greater than y, call in
a technician to check the turbine. By putting this simple program in place, the downtime of the
windfarm is reduced by 10%. However even in this simple case, Susi is faced with important choices:
What should x and y be? Are there other things that are relevant to determine if the wind turbine
should be monitored? If she comes up with another rule, how does she prove it is better (or worse)?
Will this rule be suitable for the land-based windfarm?
</p>
<p>Susi can try to work out these questions on her own. However there is a simpler option: She could
apply standard machine-learning methods to learn x, y from the data. Or even better, she could
apply standard tools such as logistic regression which we learn about in chapter 8 for modelling
the break-down probability given all available variables and she could use proven techniques for
validating her models such as cross-validation which we learn about in chapter 10. This will lead
to better and more trustworthy predictions and, more importantly, it will save Susi a lot of time.
</p>
<p>In general, the amount of data that is readily available in any given domain is growing at a rapid
rate. When we as engineers consider why machine learning is important to us it is not necessarily
because it will allow us to build the new self-driving car, discover the cure for cancer, or build a
bridge-building robot, but because it will provide simple, off-the-shelf tools which will allow us to
make efficient use of data which is already available. This book will provide an introduction to these
tools.
</p>
<p>In the following sections, we will introduce basic terminology surrounding machine learning and
data mining. We will provide an overview of various forms of machine learning problems for later
reference and discuss the basic machine-learning workflow.
</p>
<p>1.2 Machine learning tasks
</p>
<p>Some machine-learning terminology such as supervised or unsupervised learning is not fully settled
in the literature and specific definitions often become overly technical. We will therefore first provide
some examples of various tasks and types of learning before stating more exact definitions.
</p>
<p>1.2.1 Supervised learning
</p>
<p>In supervised machine learning the task is to predict a quantity based on other quantities. It is
useful to distinguish between classification and regression:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>6 1 Introduction
</p>
<p>Setosa
</p>
<p>Versicolor
</p>
<p>Virginica
</p>
<p>Petal Length
</p>
<p>P
et
a
l
W
</p>
<p>id
th
</p>
<p>1 2 3 4 5 6 7
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>Fig. 1.1: A classification problem where we are given observations (the points) and class labels (the
colors) and the goal is to come up with a rule for determining which class a point belongs to (one
such rule is indicated by the lines). The rule can then be applied to new points.
</p>
<p>Classification
</p>
<p>In classification we are given observed values x and have to predict a discrete response y. I.e., we
are given discrete observations of some object and have to determine what class the object belongs
to, see fig. 1.1. Examples include:
</p>
<p>• We are given examples of hand-written digits and have to determine what number (between
0 and 9) is contained in an image. This is a multi-class classification problem since there are
multiple categories to choose from.
</p>
<p>• We are given the hospital records for a patient and have to determine if the patient will survive
for one more year. This is a binary classification problem since there are only two choices
(survives or dies).
</p>
<p>• We are given a short sound-signal and have to determine which word is spoken. This is a
classification problem but there are as many classes as there are words (perhaps about 20 000).
</p>
<p>• We are given the social Facebook graph for a large group of people and have to determine how
likely it is any two random people in the graph will form a friendship (or remain friends) in
the next year (binary classification problem as response is discrete, i.e. link /not a link between
people).
</p>
<p>• We are shown pairs of images of faces and have to determine if the images are of the same
person.
</p>
<p>Regression
</p>
<p>In regression problems we are given observed values x and have to predict a continuous response
y, see fig. 1.2. Examples include:
</p>
<p>• We are given historical data of the stock market and have to predict the performance of a single
stock the coming Monday (prediction of a single variable).</p>
<p></p>
</div>, <div class="page"><p></p>
<p>1.2 Machine learning tasks 7
</p>
<p>Training data
</p>
<p>Fitted curve
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>0.35
</p>
<p>0.4
</p>
<p>0.45
</p>
<p>0.5
</p>
<p>0.55
</p>
<p>Fig. 1.2: A one-dimensional regression problem where we have to predict the y-values based on the
x-values. The fitted regression model is indicated by the black line.
</p>
<p>• We are given a person’s height and have to determine his or her weight (prediction of a single
variable from another).
• We are given the performance of all stocks in the past and have to predict the performance of
</p>
<p>all stocks for the next five days (massive regression problem with many output variables).
• We are given weather information from yesterday and have to predict the temperature in five
</p>
<p>major cities tomorrow (regression of five variables).
• We are given the hospital information of a person and have to determine how many days he is
</p>
<p>going to survive (prediction of a single variable).
</p>
<p>Notice these problems are quite different. In principle we could imagine we could solve the weather
prediction problem perfectly provided our model was good enough and we had enough measure-
ments, however we could not dream of being able to exactly predict a person’s weight from his or
her height, thus our prediction would in this case be guaranteed to be imperfect.
</p>
<p>The commonality of all these tasks is that we are in all instances trying to determine a mapping
where we are given observations (for instance an image of a digit) as well as examples of what
the observation should map to (for instance the digit 4) or, in the case of regression, observations
of past historical data of the patients along with observations of how long the patients survived.
These examples are therefore attempts to directly generalize from past experience and in that sense
we know what task we have to solve as well as whether we solved it correctly or not. Machine
learning problems where we have both observed observations and observed target values is known
as supervised learning problems or simply supervised learning.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>8 1 Introduction
</p>
<p>1.2.2 Unsupervised learning
</p>
<p>The oppositive of supervised learning is unsupervised learning. Consider an example dataset con-
sisting of images of animals. We may immediately consider building a machine learning method
which tries to discover what animal is in the picture (a duck, a lion, an elephant, etc.). However, for
most images on the internet we do not know what animal is actually in the image. Surely, we can sit
down and label a few thousand of the images ourselves, train a supervised method (a classification
method in this case) on the labelled images, and then use this method to determine the labels of
all other animal pictures on the internet – but this is very boring and not reflective of how humans
actually learn.
</p>
<p>Unsupervised learning tries to solve this and similar problems where we do not have access to
any “ground-truth” label information (such as the identity of the animal in the image) but we try
to discover this labelling from the data alone. See fig. 1.3 an example of clustering where the goal
is to cluster (label) the gray points in the left-hand pane and an example clustering is indicated in
the right-hand pane by the coloring. Examples of clusterings include:
</p>
<p>Clustering
</p>
<p>• In the animal example, the goal was to group images into clusters such that each cluster
represented a given type of animal.
</p>
<p>• Given genetic sequence data from a number of bacteria, try to find natural groupings of the
genomes (roughly corresponding to species).
</p>
<p>• Given a large collection of documents, try to determine clusters of similar documents corre-
sponding to topics.
</p>
<p>Density estimation
</p>
<p>In density estimation we try to quantify how likely (or unlikely) a given future observation is given
past observations, i.e. the probability distribution of the data, see fig. 1.4. Consider the hand-
written digit example. Suppose you are shown four images of hand-written digits and are told they
are from the same person. Suppose then you have to determine if a fifth (until now) unobserved
digit is written by the same person. This task involves estimating the relative variability in the
person’s hand-writing and how plausibly it is he has written a given digit – this is known as density
estimation. Other examples include:
</p>
<p>• You are at a large archeological dig and told where scientists have found archeological remains
in the past. You have to decide the next place to excavate. Estimating the place with the highest
probability of a new find from past finds is a density estimation problem.
</p>
<p>• You are working for an oil company and try to estimate the drill site with the highest chance
of finding oil based on past drilling.
</p>
<p>• You are a microbiologist and you are trying to find out how typical a particular cell is given
other observed cells of the same type. Being able to detect atypical cells could be relevant to
determine diseases such as cancer.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>1.2 Machine learning tasks 9
</p>
<p>x1
</p>
<p>x
2
</p>
<p>0 5 10
</p>
<p>−8
</p>
<p>−6
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>x1
</p>
<p>x
2
</p>
<p>0 5 10
</p>
<p>−8
</p>
<p>−6
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>Fig. 1.3: A 2D clustering example. A clustering is given a dataset (here the 2D dataset shown in the
left-hand pane) and has to estimate plausible divisions of the observations into clusters as indicated
in the right-hand pane.
</p>
<p>Anomaly detection
</p>
<p>Anomaly detection is figuring out which observations significantly deviate from other observations.
While what constitutes a significant deviation is highly dependent on the context, humans neverthe-
less have a natural ability to carry out this task. We may for instance consider the red observation
in fig. 1.5 to be significanly different from the other observations from a number of perspectives.
This includes that it is simply quite far away from it’s nearest neighbour relative to the other
observations distance to their neighbours or that it appears not to follow the same “tendency”
(the banana-shaped curve) as the other observations. Anomaly detection can be considered closely
related to density estimation since an observation which is very implausible (low density) could be
considered an outlier. Anomaly detection is relevant in a number of situations including:
</p>
<p>• You work for a credit-card company and have to determine if an transaction deviates from
common transactions in order to detect fraud.
• You supervise a windmill farm and based on past behaviour have to determine if a windmill is
</p>
<p>beginning to behave differently indicating it may require repair.
</p>
<p>Association mining (rule-induction)
</p>
<p>Association mining (or rule-induction) is figuring out rules which hold approximately in a data set,
see fig. 1.6. Suppose you work for an online book seller and you are given a large dataset consisting
of which books different people bought. In order to show relevant adds, you want to come up with</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10 1 Introduction
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 0 2 4 6 8
</p>
<p>−6
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>x1
</p>
<p>x2
p
(x
)
</p>
<p>−2
0
</p>
<p>2
4
</p>
<p>6
8
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>0
</p>
<p>0.01
</p>
<p>0.02
</p>
<p>0.03
</p>
<p>0.04
</p>
<p>Fig. 1.4: A 2D density estimation example. Given the black points, the density is estimated and
plotted in the right-hand pane.
</p>
<p>rules such as: “If the person bought both book X and book Y, then he is likely to buy book Z”. This
is known as association mining or rule discovery. Other applications are:
</p>
<p>• Given which items people have historically bought in a supermarket, discover what other items
each customer is likely interested in.
</p>
<p>• Given a number of patients’ medical history, determine which past illnesses and conditions
imply high risk for other, future illnesses: “Common cold and operations imply high risk of
pneumonia”.
</p>
<p>• Given past life experience, figure out that drinking the past night implies hangover.
</p>
<p>Dimensionality reduction
</p>
<p>In dimensionality reduction, we try to discover a simpler representation of a very high-dimensional
dataset, see fig. 1.7. Consider for instance a dataset of faces. In one example the dataset is in a
modest resolution (the images may be 500×500 pixels) and in the other example the same images are
in a very high resolution (say 5000× 5000). The later dataset contains 100 times more information
than the former, however to a human they contain the same information: We can recognize the
identity of the people from both datasets, their age, race or gender, their emotional state etc. If we
think about it M = 750 000 = 500 × 500 × 3 numbers (there are 3 color channels) is still a lot of
data. If we had access to a large set of numbers such as the distance between the eyes, length of
the nose, eyes and mouth, width and height of the face, the color of the skin, the curvature of the
mouth, etc. we might retain most of the relevant information in the faces while using far less than
M numbers. Thus dimensionality reduction is learning a representation of an M dimensional object
which uses M ′ &lt; M numbers while retaining most of the relevant information. Other examples are:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>1.2 Machine learning tasks 11
</p>
<p>x1
</p>
<p>x
2
</p>
<p>0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
</p>
<p>0.4
</p>
<p>0.45
</p>
<p>0.5
</p>
<p>0.55
</p>
<p>0.6
</p>
<p>0.65
</p>
<p>0.7
</p>
<p>0.75
</p>
<p>0.8
</p>
<p>Fig. 1.5: Anomaly detection tries to discover observations that differ from the rest of the dataset.
</p>
<p>{Bread, Coke, Milk}
{Beer, Bread}
{Beer, Coke, Diaper, Milk}
{Beer, Bread, Diaper, Milk}
{Coke, Milk}
</p>
<p>Training Set
</p>
<p>{Milk} → {Coke}
</p>
<p>Rules Discovered
</p>
<p>{Diaper, Milk} → {Beer}
</p>
<p>Fig. 1.6: Given examples of what people have previously bought, association mining tries to discover
rules for what they will likely buy in the future. For instance a person who buys milk is likely to
also buy coke.
</p>
<p>• Lossy compression.
• Finding a summary of a sentence or book.
</p>
<p>1.2.3 Reinforcement learning
</p>
<p>Finally, for the sake of completeness reinforcement learning is worth mentioning. Reinforcement
learning corresponds to the case where a computer has to control a robot based on sensory input</p>
<p></p>
</div>, <div class="page"><p></p>
<p>12 1 Introduction
</p>
<p>Fig. 1.7: Dimensionality reduction. High-dimensional images (each image contains 28 × 28 pixels
corresponding to 784 dimensions) are mapped onto a 2D domain, that is compressed into a 2-
dimensional vector. Colors indicate different digits (0, 1, . . . , 9). Notice, digits that are the most
dissimilar such as 0 and 1 are mapped to points far apart.
</p>
<p>and a reward signal. That is, at any given time the robot observes the state of the world (for
instance a screenshot if the robot should learn to play a video game), selects an action (for instance
pushing a particular button) and possibly receives a reward, for instance simply if the robot loses
the video game or not. Reinforcement learning can be seen as a type of regression (i.e. supervised
learning) where we are trying to predict the reward based on the current action and input signal.
However, as rewards are rarely observed it is usually considered to be different from supervised
learning. This course will not consider reinforcement learning, however, many of the techniques
used for reinforcement learning will in fact be introduced in this course.
</p>
<p>1.2.4 The machine-learning toolbox
</p>
<p>The above taxonomy is not exhaustive or set in stone and there are many problems which does not
exactly fit into the above categories. Consider a system which has to translate from English sentences
to French sentences. In some sense it is a classification problem, however there are infinitely many
sentences to choose from and treating it as a generic classification problem is not helpful. Or suppose
you want the computer to learn if two variables are causally related, i.e. that smoking causes cancer
in a medical records dataset. We could naively imagine treating this as a market-basket problem,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>1.3 Basic terminology 13
</p>
<p>x5 001 =
</p>
<p>y5 001 = 1y1 = 0 y10 001 = 2 y50 000 = 9
</p>
<p>x50 000 =x10 001 =x1 =
</p>
<p>Fig. 1.8: Four observations (images of handwritten digits) from the MNIST dataset containing a
total of N = 60 0000 handwritten digits. Each observation consist of a 784-dimensional vector xi
and a label yi which can be either 0, 1, . . . , 9.
</p>
<p>however if this is taken serious we would have to believe that, say, putting milk in someones market
basket really cause them to put coke in it as well and this should hint causation is something more.1
</p>
<p>This might feel discouraging: Why take this course if I don’t learn about the methods for the
tasks I really think are cool. Fortunately, even the most advanced applications of machine learning
rely on re-using and combining simpler tools, nearly all of which are introduced in this course
in some form. A useful analogy is building a machine where this course supplies the wrench, the
screwdriver, the soldering iron and the other tools required to get started.
</p>
<p>Secondly, it is worth emphasizing that the difference between the techniques supplied in this
course and the absolute forefront is not as great as in other disciplines, say a basic book on mechanics
and a book on general relativity. The basic architecture of one of the absolute best image-recognition
network (Inception v3) is a variant of the humble simple feedforward network introduced in chap-
ter 15.
</p>
<p>1.3 Basic terminology
</p>
<p>Let’s make the above discussion more concrete by considering a realistic problem. Consider the
handwritten digits from the MNIST dataset shown in fig. 1.8. Each digit consists of a 28× 28 pixel
image which, since the images are black and white, can be represented as a vector x consisting of
M = 784 real numbers2 and we write this as x ∈ RM . The goal is to build a machine which takes
such an image x and outputs the identity of the digit, i.e. if it is 0, 1, . . . , 9. This is a non-trivial
problem since there is a huge variability in how people write digits (stroke, style, open or closed
digits, rotation, translation, etc.), however it is a trivial problem for humans. Our goal is therefore
to construct a function f which takes a M -dimensional vector as input and returns 0, 1, . . . , 9
depending on which digit it believes the image contains.
</p>
<p>If we adopt a machine learning approach our goal is to construct a system which can learn how
to solve the above problem. The system learns from experience. In this case the past experience
</p>
<p>1 This is not to say that automatic translation or inference of causation is beyond machine learning. See
for instance: https://en.wikipedia.org/wiki/Neural_machine_translation and [Pearl et al., 2016].
</p>
<p>2 The real numbers are all numbers such as 5,−1, π, 1
3
, · · · . For instance the pixel values can lie in the
</p>
<p>interval [0, 1].</p>
<p></p>
<div class="annotation"><a href="https://en.wikipedia.org/wiki/Neural_machine_translation">https://en.wikipedia.org/wiki/Neural_machine_translation</a></div>
</div>, <div class="page"><p></p>
<p>14 1 Introduction
</p>
<p>Dtrain
</p>
<p>Training data Model Prediction rule
</p>
<p>The model learns
</p>
<p>Future data
</p>
<p>M
</p>
<p>f(x) yi = f(xi)the prediction rule f
from the training data
</p>
<p>Training phase Test phase
</p>
<p>Fig. 1.9: A model in supervised learning. Given a set of training data, the model learns a prediction
rule f(x) in the training phase. Later, in the test phase, this rule can be applied to new, unobserved
data.
</p>
<p>(i.e. the data) would consist of a number of example images for each type of digit. For instance
we would have 5 000 examples of the digit 0, x1,x2, . . . ,x5000, then 5 000 examples of the digit 1:
x5001,x5002, . . . ,x10 000 and so on up to image x50 000 of the digit 9. Let N = 50 000 be the number
of examples, we collect all this information in an N ×M matrix X and a N -dimensional vector y:
</p>
<p>X =
</p>
<p> x
T
1
...
</p>
<p>xT50 000
</p>
<p> and y =
0...
</p>
<p>9
</p>
<p> ,
where yi is either 0, . . . , 9 depending on the digit in image xi. Taken together we let Dtrain = (X,y)
denote all data available for training the machine learning method andDtrain is known as the training
set .
</p>
<p>1.3.1 Models
</p>
<p>Given the training set Dtrain in the above example, machine learning then consists of constructing
a program which takes the training data Dtrain and returns a function
</p>
<p>f : RM → {0, . . . , 9}
Learning the function f from the data is known as the training phase or alternatively as the learning
phase or simply learning, see fig. 1.9. Once this function is learned it can be used to determine the
identity of unobserved images of digits. For instance if for an image x we have that f(x) = 5 this
means the algorithm predicts that the image contains the digit 5. These new observations used to
test the model is known as the test set denoted Dtest.
</p>
<p>How well a model performs when evaluated on previously unseen data is known as the gener-
alization error, and this key quantity in machine learning is what ultimately decides which model
is better. The generalization error should be distinguished from the training error which is the
average error on the training set; we will have much more to say about the generalization error in
chapter 10. Notice that since the training set was used to train the model, we should expect the
training error to be lower than the generalization error.
</p>
<p>A computer program (along with the assumptions it relies upon) which carries out the above
steps –i.e. based on a training set it constructs a function f– is known as a model . Different models
will be denoted M1, . . . ,MS .</p>
<p></p>
</div>, <div class="page"><p></p>
<p>1.3 Basic terminology 15
</p>
<p>Training set Validation set
</p>
<p>“Good” learned representation
</p>
<p>“bad” learned representation
</p>
<p>Fig. 1.10: In the broadest sense, a machine learning method takes a number of observations (here,
images of cars) and build an (approximate) internal representation that captures statistical relation-
ships in the images. One should think of each image as bringing a little piece of information about
this relationship and the goodness of a method as how well it can make use of this information. Since
the method does not know what it should focus on, it can either build a successfull representation
(center, top), or focus on spurious properties of the training set (center, bottom). For this reason
evaluating the method with data not used to train the method (the validation set) is important
as these images are unlikely to conform to the (spurious) representation and will therefore offer an
accurate idea about how well the method performs.
</p>
<p>1.3.2 A closer look at what a model doesF
</p>
<p>The above description present an accurate but somewhat abstract view of what machine-learning
attempts to do. In this section we will try to provide an intuition of what happens “inside” the
function f so as to get an understanding of what may go wrong. Note these details are obviously
particular to the machine-learning method used to construct f and the discussion should therefore
not be taken too literally.
</p>
<p>For the purpose of illustration, we will consider an example problem quite similar to the digit
classification problem from the previous section, except we are now considering larger images and
we wish to categorize them into classes such as cars, cities, and so on.
</p>
<p>In fig. 1.10 we have illustrated an approximate view of what a machine learning method does in
such a setting: We start out with a dataset comprised of a collection of images3 (and their labels)
and the goal of the method is then to predict which category the images fall within on hitherto
unseen data. This is done by somehow learn a useful internal representation based on what these
images have in common. Speaking in broad terms, these learned representation of for instance
the cars-class will correspond to a cobbled together collection of car-like elements and an implicit
statistical relationship between them. A machine learning method trained to detect cars is likely to
be sensitive to wheels, but will often be insensitive to the number of wheels and only have a very
rough idea about where wheels are supposed to be located relative to each other, as illustrated in
fig. 1.10 (center, top).
</p>
<p>3 Images obtained from https://www.pexels.com</p>
<p></p>
<div class="annotation"><a href="https://www.pexels.com">https://www.pexels.com</a></div>
</div>, <div class="page"><p></p>
<p>16 1 Introduction
</p>
<p>Fig. 1.11: Left: With more data we should expect the generalization error to drop, however this
drop can be expected to occur sooner for less flexible models (blue and red curve) as they are
less prone to learn spurious relationships in the data, however, it will be sustained longer for more
flexible methods (yellow) which are able to learn a more powerful representation. Right: When a
model is trained on little data, we should as a rule expect it to learn all the particulars of the
dataset, including spurious relationships. It will therefore perform exceedingly well on the training
set (overfit), but generalize very poorly. With more data, these two curves will approach each other
as the training error increases and the generalization error drops.
</p>
<p>The role of data
</p>
<p>Let us state a trivial but important point: these learned features/relationships are learned from the
data. In other words, think about each observation as containing a small piece of the overall true
meaning of car.
</p>
<p>Machine-learning method can pick out these pieces at varying degrees of efficiency, and as a
rule less efficient than humans. The advantage machine-learning methods have over humans is in
making use of more data than a human can easily comprehend. It is therefore the ability to make use
of lots of data, rather than intrinsic sophistication, which makes machine-learning methods work:
That is why data, specially lots of high-quality data, is so important.
</p>
<p>When learning fails
</p>
<p>The goal of machine learning is to find models which generalize well to future data (i.e., data
the model is not trained upon), and this is measured by the generalization error which is what
ultimately decide which model is better.
</p>
<p>A model may have a high generalization error for three reasons: Firstly, it may be misapplied, but
we will leave that asides. Secondly, the method can be too weak (i.e., inflexible) to learn a sufficiently
rich representation to solve the problem, and thirdly, since the machine-learning methods do not
have any intrinsic idea about what to focus on in a set of images, they might focus on the wrong
things. This is illustrated in fig. 1.10 (center, below) where the method here think that the car-
images has to do with things common for the images (roads, sky, tree) and not the car itself. This</p>
<p></p>
</div>, <div class="page"><p></p>
<p>1.4 The machine learning workflow 17
</p>
<p>Large neural network
</p>
<p>Less complex
</p>
<p>Small neural networkDecicion treeLinear/logistic regression
</p>
<p>K-nearest neighbour methods Random forrest
</p>
<p>More complex
Regularization Bagging/boosting
</p>
<p>(Chapter 12) (Chapter 15)
</p>
<p>(Chapter 13)
</p>
<p>(Chapter 15)
</p>
<p>(Chapter 8)
</p>
<p>(Chapter 10)
</p>
<p>(Chapter 7)
</p>
<p>Fig. 1.12: A rough overview of the relative complexity/flexibility of models encountered in this book;
note this illustration is somewhat subjective and depends on assumptions about how the methods
are applied. Techniques such as regularization serves to make a model less flexible (more robust),
whereas ensemble techniques such as bagging/boosting has the opposite goal.
</p>
<p>leads to a rule which is useful to categorize images in the training set, but which generalize poorly,
hence high generalization error.
</p>
<p>There are two things which determine how prone a given method is to degenerate behavior,
both illustrated in fig. 1.11, namely the amount of data and how flexible the machine learning
method is. In fig. 1.12 we have tried to give a rough indication of how some of the methods in
this book are sorted according to complexity. One can think of any type of model in terms of
how flexible a representation it can ultimately learn. With little data, a very flexible model will be
able to learn any number of representations and will therefore often fail, and we say they tend to
overfit. This means less flexible models, such as linear models or the like, will often out-compete
more sophisticated choices, and methods such as regularization (which is a general technique for
reducing the flexibility of a model which we will learn about in chapter 14) will be relevant. However
when more data becomes available this becomes less of an issue, and so the more flexible models
will outcompete the less flexible simply because the more flexible model is able to learn a more
sophisticated, and therefore more accurate, representation. The simplicity of which we can heap
extra neurons into an artificial neural network (chapter 15) makes them the poster child of extremely
flexible models and that, together with the availability of large datasets and powerful computers,
has driven the neural network revolution of the past decade.
</p>
<p>1.4 The machine learning workflow
</p>
<p>To summarize the above discussion we will now present the workflow of a machine learning practi-
tioner fig. 1.13. Suppose we are presented with a problem for the first time. The first thing we have
to understand is what type of problem it is: Is it a supervised problem or unsupervised problem?
Regression or classification? etc. Secondly, we should try to understand the available data: Data
is the life blood of any machine learning method and without having a good handle on what our
dataset is, if there are issues with our dataset and if the task appears feasible given the dataset it is
difficult to proceed. This is the data mining step and during this course we will present a number
of techniques and terms for working with a dataset including visualization.
</p>
<p>Having analysed the data and problem we must select an appropriate method and possibly apply
modifications to the method. This is step 2 where we ensure the method is implemented correctly
and produce an output. At this point we have our first model: M1. To take the handwritten digit
task as an example, the method may produce a prediction rule which can be applied to test data.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18 1 Introduction
</p>
<p>Fig. 1.13: The machine learning workflow used in this course. In the first step the problem is analysed
and one obtains an overview of the data including potential issues. In the next step an appropriate
machine-learning method is implemented and potentially modified and in the third step the model
is evaluated. The lessons learned are used to improve on what happened in step 1 and 2 to produce
a new model which is compared to the first. The process then repeats. At all times knowledge of
the domain should be exploited.
</p>
<p>This brings us to step three which is absolutely critical but often neglected: Evaluation and
dissemination. For a model M1 we must evaluate how well the model performs (generalizes) on
previously unseen data and quantify this numerically. We should also look at examples where the
model failed and try to get a idea on why this happens and how we might do better. This is also
the step where we disseminate the results to be used by others either in a report or as part of a
more general workflow in a scientific team or company. When this step is completed we essentially
start over with whatever lessons we may have learned and see if we can do better at step 1 and 2.
The result may be a new model M2 which must again be tested to see if it is better or worse than
M1.
</p>
<p>During this course, we will learn techniques which apply to all these steps. The first section
of the course relates to the first step; the two next sections treat the second and third step of
the workflow, both for supervised machine-learning techniques and unsupervised machine-learning
techniques.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>2
</p>
<p>Data and attribute types
</p>
<p>Without data there would be no need for machine learning. In this chapter, we will define what we
mean by a data set, attribute types and discuss commonly encountered data issues such as missing
values. Common data issues such as missing values have been present from the day humans first
began recording data but is perhaps particularly treated in depth in for instance clinical psychology
where standardized treatment of corrupted data is a major concern and it is also within clinical
psychology the distinction of attribute types (ration, interval, etc. which we will introduce below)
was originally introduced by Stanley Smith Stevens in 1946 [Stevens, 1946].
</p>
<p>2.1 What is a dataset?
</p>
<p>Data, or a dataset, is a collection of electronically stored information. Quite simply, it is what we
have to work with. Examples could be:
</p>
<p>• Biomedical information about patients in a hospital.
• A collection of text files, for instance corresponding to news stories.
• The temporal development of 10 stocks on the New Your stock market over 50 days.
• A collection of 3D models, each model being defined as a (varying) collection of points on its
</p>
<p>surface.
• A social graph, for instance from a social network (who is friends with who).
</p>
<p>Often, and in all cases considered in this book, a dataset can be thought of as standardized measure-
ments of objects of the same basic type. For instance, measurements of patients, cars or documents.
Each such measurement of a single object is called an observation. For instance, for the dataset
comprised of biomedical information each observation corresponds to a patient, in the text-files
example each observation is a text file, in the stock market each observation is the value of the
stocks on a given day, for the 3d model case each observation is a particular model, and finally for
the social graph each observation is an edge. The number of observations in a dataset is denoted
N .
</p>
<p>It is useful to distinguish between datasets which are simple and those which are complex. A
simple dataset is a dataset of N observations where each observation corresponds to a fixed number
of recorded values. For instance in the patients dataset, we can imagine that for each observation
we record the patients name, age, weight, blood pressure and so on. Each recorded value is called</p>
<p></p>
</div>, <div class="page"><p></p>
<p>20 2 Data and attribute types
</p>
<p>Table 2.1: Ten entries from the Cars dataset comprised of N = 142 observations and M = 9 features
</p>
<p>ID MPG Cylinders Horsepower Weight Year Safety Acceleration Origin
</p>
<p>1 18 8 150 3436 70 4 11 France
2 28 4 79 2625 82 4 18.6 USA
3 26 4 79 2255 76 3 17.7 USA
3 29 4 70 1937 76 1 14.2 Germany
4 NaN 8 175 3850 70 2 11 USA
5 24 4 90 2430 70 3 14.5 Germany
6 17.5 6 95 3193 76 4 17.8 USA
7 25 4 87 2672 70 -100 17.5 France
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>142 15 8 198 4341 70 2 10 USA
</p>
<p>an attribute or feature and the number of features will be denoted by M . In table 2.1 is shown an
example of a simple dataset corresponding to N = 142 observations of cars where for each car we
record M = 8 features.
</p>
<p>A non-simple dataset is a dataset where there is additional structure or complexity. This could
be:
</p>
<p>Temporal If for instance the observations are made sequentially over time (such as the stock
market).
</p>
<p>Varying number of features If each observation has a varying complexity. For instance the text
files has a varying number of words, and the 3d models a varying number of surface points.
</p>
<p>Self-referential structure For instance in the social graph, the edges refer to the same group of
people.
</p>
<p>These definitions are not set in stone and require some common sense. For instance, we could
claim that the patients dataset was temporal since patients will arrive at different dates, however
what matters is if this temporal ordering is considered important for the machine learning task. In
the patients example, no doctor would consider it important if a patient arrives in May or June
in terms of making his diagnoses, whereas in the stock market example there will typically be
important upward/downward trends and causal structure which makes the temporal ordering a key
feature.
</p>
<p>In this course we will consider simple datasets of N observations and M features such as the
Cars dataset. This may sound very restrictive; however, the techniques suitable for more complicated
datasets can often be seen as specialization of the tools we here consider for the simple case. In
addition, the simple case covers nearly all data that can be plugged into a spreadsheet and as we
will see, it is often possible to transform data that at first glance may appear complex (for instance
images or text) into the simple format.
</p>
<p>2.1.1 Attributes
</p>
<p>If we consider the cars dataset in table 2.1 we immediately notice the attributes are different. For
instance the Origin feature is one of three text strings, the ID is an increasing sequence, presumably
only used for bookkeeping purposes, safety is a number of stars (from 1 to 5) and so on. It is useful</p>
<p></p>
</div>, <div class="page"><p></p>
<p>2.1 What is a dataset? 21
</p>
<p>to introduce some general vocabulary to describe different types of attributes. The most simple
distinction is between attributes that are continuous and discrete. In particular, we define:
</p>
<p>Continuous: A continuous attribute is one which, if it can take values a and b, then it can also
take any value between a and b.
</p>
<p>Discrete: A discrete feature is one where if it can take a value a, then there is a positive minimum
distance to the nearest other value b it can take
</p>
<p>Binary: A binary feature is a discrete feature which can only take two values. Usually these are
denoted 0 (false) or 1 (true).
</p>
<p>For instance the weight of the car is continuous since if a car can weight 1000 pounds and 2000
pounds then presumably it can also take any weight between 1000 and 2000 pounds. Notice contin-
uous does not imply it can take any decimal value since the weight cannot for instance be negative
and this is why the definition is somewhat technically sounding.
</p>
<p>The definition of discrete tries to capture the intuition that the variable changes in discrete
steps. Most commonly these steps are integer values such as 1, 2, 3, 4, 5 (the safety rating is such an
example), however the steps need not be integer valued. For instance a safety rating which could
take the possible values 0, 14 ,
</p>
<p>2
4 ,
</p>
<p>3
4 and 1 would also be discrete.
</p>
<p>2.1.2 Attribute types
</p>
<p>Machine learning methods operate on numbers and not text strings. Accordingly, to apply a
machine-learning method to the Cars dataset in table 2.1 we must therefore translate the country of
origin (which can be USA, France and Germany) into numbers (1, 2, 3). Notice that which country
is assigned which number is our convention and should be irrelevant: If we choose to assign USA
to 3 (and Germany to 1) this should not affect our subsequent computations (or so we hope!). This
makes the country of origin different from the safety rating since a safety rating of 5 is larger than
a safety rating of 3 and this feature should inform our machine-learning method. In other words
the safety rating is ordered (smaller, larger) whereas the country of origin is not ordered. This is
an example of two variables of different types. By convention it is common to distinguish between
four different types of attributes:
</p>
<p>Nominal: If the variable is not ordered and only uniqueness matters. An example is the country
of origin or the id.
</p>
<p>Ordinal: If the variable is ordered (smaller, larger). An example is the safety rating.
Interval: If the variable is ordered and the relative magnitude of the variable has a physical
</p>
<p>meaning. The year is interval since the difference between say 82 and 85 (three years) has the
same meaning as the difference between years 77 and 80, however the safety rating is not interval
since a difference in safety rating of 2 and 4 and 3 and 5 does not have a physical meaning.
</p>
<p>Ratio: If the value 0 of the variable has a specific, physical meaning. I.e. it makes sense to say
one value of the variable is “twice as large” as another. The year is not ratio since it has no
particular meaning to say that 62 is twice as large as 31, whereas the volume of the engine
does have a particular physical meaning since a value of 0 means the combustion chamber has
volume 0 cm3.
</p>
<p>Notice the types are a hierarchy. That is, if a variable is interval it must also be ordinal and
nominal (but is not categorized as such). It is distinguished from ordinal and nominal as the relative
magnitude of the variable also has a physical meaning and therefore categorized as interval. Ratio</p>
<p></p>
</div>, <div class="page"><p></p>
<p>22 2 Data and attribute types
</p>
<p>and interval are the variables which are most easily confused. Suppose for instance we record the
longitude of cities as an attribute (the longitude is the east-west location on the globe in degrees).
Is this variable ratio? One could reason the answer is yes, since a longitude of 0 has the “meaning”
that we are on a particular place on earth, in this case the north-south line that passes through the
city of Greenwich in England. However, what the definition has in mind is that the physical meaning
is not purely “by convention” but actually refers to a physical property of the globe. Since the city
of Greenwich is just a convention, the correct answer is that the longitude is interval and not ratio.
If one is in doubt if a variable is ratio or interval it is often useful to imagine that civilization had to
start over and come up with the same sort of variable. If the new civilization would assign “zero”
the same meaning as we do the variable is ratio, else it is interval. In the case of the longitude, if
we had to come up with the longitude system anew the longitude of 0 might as well lie anywhere
on the globe, and similar for the year where we could start our calendar from any other year than
the year where medieval monks estimated Jesus to have been born.
</p>
<p>If the variable is continuous or discrete is often independent of whether the variable is ordinal,
nominal, interval or ratio. For instance the variable Cylinders is discrete ratio (zero cylinders has
a specific meaning) and acceleration is continuous ratio. To summarize the variable types for the
Cars example:
</p>
<p>Origin: Discrete, nominal.
Safety: Discrete, ordinal.
Cylinders: Discrete, ratio.
Year: Discrete, interval.
Miles/Gallon, Horsepowers, Weight and Acceleration: Continuous, ratio.
</p>
<p>2.2 Data issues
</p>
<p>Now that we have described the features of the Cars dataset table 2.1 we turn our attention to the
actual values and immediately notice some oddities: Car ]7 has a safety rating of −100, and for car
4 the Miles/Gallon is NaN 1. Unfortunately, such issues are surprisingly common especially when
humans have to enter values and we distinguish between the following issues:
</p>
<p>Irrelevant or spurious attributes: The ID column is irrelevant as it only depends on the or-
dering of the data.
</p>
<p>Outliers: The safety rating of −100 must be due to some kind of error. We call such observations
outliers.
</p>
<p>Missing data: The Miles/Gallon attribute for car ]4 is missing.
</p>
<p>As a rule, attributes which can be known to be spurious or irrelevant such as the ID should simply
be discarded, and in doing so we will reduce the number of dimensions of the dataset (M) and
make the machine-learning problem easier. Missing data can be treated in four main ways:
</p>
<p>• Some machine learning methods such as Bayesian learning methods can account for missing
data if applied correctly, however, most cannot.
</p>
<p>• If the missing data is isolated to one particular attribute and that attribute is deemed non-
essential, we can choose to discard the attribute.
</p>
<p>1 NaN stands for “Not a number” and is one way to denote the value is ill-formed.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>2.3 The standard data format 23
</p>
<p>Table 2.2: Processed version of the Cars dataset consisting of M = 8 attributes and N = 142
observations (only 10 are shown).
</p>
<p>MPG Cylinders Horsepower Weight Year Safety Acceleration Origin
</p>
<p>18 8 150 3436 70 4 11 3
28 4 79 2625 82 4 18.6 1
26 4 79 2255 76 3 17.7 1
29 4 70 1937 76 1 14.2 2
15.32 8 175 3850 70 2 11 1
24 4 90 2430 70 3 14.5 2
17.5 6 95 3193 76 4 17.8 1
25 4 87 2672 70 3 17.5 3
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
15 8 198 4341 70 2 10 1
</p>
<p>• If we have many observations and only few have missing observations, we can discard the
observations with missing values.
• If we want to keep the affected attributes and observations, we can impute the missing values
</p>
<p>with some kind of neutral guess.
</p>
<p>For the last method, imputation, a neutral guess can be obtained in several ways. In the case of the
Miles/Gallon attribute, we can impute the missing value with the mean of all observed instances
of the Miles/Gallon attribute (in this case 15.32 Miles/Gallon). For the safety rating, it may be
undesirable to impute the mean value as the safety rating is an integer. In this case we can choose
to either round the safety rating, or impute the most commonly encountered safety rating in the
dataset, in this case 3.
</p>
<p>As a rule, changing the dataset by for instance throwing away “outliers” without clear reasons
for doing so is a cause for concern since this may affect conclusions drawn from the data. Outliers
should therefore only be removed if there are strong reasons to think the observations are erroneous
such as a negative safety rating.
</p>
<p>2.3 The standard data format
</p>
<p>We conclude this section by relating the table to the standard data format used in the course and
that we briefly saw in the introduction. Suppose we implement the changes to the Cars dataset
discussed above, i.e. we remove the ID, replaced the country of origin with an (ordinal) numeric
coding and imputed the missing values. We then obtain the new Cars dataset shown in table 2.2
The way we will represent such a dataset is as an N ×M matrix X where N = 142 observations
and M = 8 features. The corresponding matrix is simple:
</p>
<p>X =
</p>
<p>
18 8 150 3436 70 4 11 3
28 4 79 2625 82 4 18.6 1
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
15 8 198 4341 70 2 10 1
</p>
<p> (2.1)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>24 2 Data and attribute types
</p>
<p>Each observation in the dataset, i.e. a single car, is a row in X. We will write xi for the i’th
observation, for instance the second observation is the M = 8 dimensional vector (notice the
transpose):
</p>
<p>x2 =
[
28 4 79 2625 82 4 18.6 1
</p>
<p>]T
We will refer to element i, j of the matrix X as Xij or Xi,j . In this way X2,4 = 2625 meaning
that the second car weights 2625 pounds. To complete the discussion, recall from the introduction
we might have access to additional information yi about each feature, corresponding to either a
continuous regression (target) value or a discrete class label, and we collected all these values as a
N -dimensional vector y. All in all this means any dataset considered in this course will be an N×M
matrix X and (if available) a N -dimensional vector y denoting the response or output variable (we
will get back to the use of y in part II of the course when we cover supervised learning). To apply
any of the machine-learning methods discussed in this book to a given dataset therefore consists of
putting it in this X,y format.
</p>
<p>2.4 Feature transformations
</p>
<p>Feature transformations refers to the operation of changing an existing feature or adding a new
feature to our dataset. There are three principal reasons why we may want to do this:
</p>
<p>• Many machine-learning methods such as principal component analysis and K-means are sensi-
tive to outliers or features that reside on a different scale and it may therefore be a good idea
to standardize (change) features.
</p>
<p>• Expert knowledge can be used to compute new features from existing ones, thereby (hopefully)
making a simpler machine-learning method more powerful.
</p>
<p>• Ordinal values such as Origin might not be appropriately encoded as an integer (see below).
To provide examples of the first type, suppose we consider the Cars dataset of table 2.2 (or equiv-
alently, the processed version as the X matrix in eq. (2.1)), the variable Safety is less than 10
whereas Weight is between about 2000 to 5000 and both features have a mean value greater than
zero. This difference in scale can confuse some machine-learning methods and it is therefore common
to standardize the features by either subtracting the mean, or subtracting the mean and dividing by
the standard deviation. This is done by first computing the empirical mean and standard deviation
for each column j:
</p>
<p>µ̂j =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>Xij , σ̂j =
</p>
<p>√√√√ 1
N − 1
</p>
<p>N∑
i=1
</p>
<p>(Xij − µj)2
</p>
<p>and then updating each column j of X of the new matrices X̃ as either:
</p>
<p>X̃ =
</p>
<p>
· · · X1j − µ̂j · · ·
· · · X2j − µ̂j · · ·
</p>
<p>...
· · · XNj − µ̂j · · ·
</p>
<p> or X̃ =

· · · (X1j − µ̂j)/σ̂j · · ·
· · · (X2j − µ̂j)/σ̂j · · ·
</p>
<p>...
· · · (XNj − µ̂j)/σ̂j · · ·
</p>
<p>
It is easy to verify that in the first case the mean of column j in the updatedX-matrix will be 0, and
in the second case the mean will be 0 and the standard deviation 1. While subtracting mean and</p>
<p></p>
</div>, <div class="page"><p></p>
<p>2.4 Feature transformations 25
</p>
<p>standardizing columns are the most common transformations, many other could be considered. For
instance, suppose X represent observations about animals where one column is the weight. Some
animals are very small (for instance weighing a few grams) whereas others will weight hundreds of
kilos, and the same difference in weight will often be much more informative when comparing two
small animals compared to comparing two large ones. In other words, if an animal weights 1020g
(compared to 20g) you can tell it is not a mouse, whereas if it weight 1001kg (compared to 1000kg)
this extra kilo will not tell you that it is not a rhino, however, the difference in weight in both cases
is just 1kg. A possible way to make the machine-learning method respect this difference in scale is
by applying the logarithm to the column:
</p>
<p>X̃ =
</p>
<p>
· · · logX1j · · ·
· · · logX2j · · ·
</p>
<p>...
· · · logX3j · · ·
</p>
<p>
because in this case the difference in weight is: log(1.02)− log(0.02) ≈ 3.93 compared to log(1001)−
log(1000) ≈ 0.0001 and thus much more informative for the smaller animal. A feature transformation
consisting of adding and multiplying a feature column xj with constants a, b, xi 7→ axi+ b, is called
linear (subtracting the mean and dividing with the standard deviation being the prototypical
example) whereas all other transformations, such as computing the logarithm, are called non-linear.
</p>
<p>In addition to changing a feature a new feature can be added to the dataset computed from
existing ones. Suppose we consider a hospital records dataset X consisting of M attributes and
that two of these features, j and k, corresponds to weight and height of the patient. Since small
patients are often light and tall patients are often heavy it may be beneficial to add a new feature
to the dataset which takes this into account such as the Body Mass Index (BMI). Recall the BMI
is the weight divided by the square of the height, in other words we add a new column:
</p>
<p>X̃ =
</p>
<p>
· · · X1jX−21k · · ·
· · · X2jX−22k · · ·
</p>
<p>...
· · · XNjX−2Nk · · ·
</p>
<p>
to X thereby obtaining an N × (M + 1) dataset. Adding new features in this manner can make a
simple learning method more powerful and will be used extensively when we discuss linear regression
in chapter 8.
</p>
<p>2.4.1 One-out-of-K coding
</p>
<p>The final type of feature transformation we will consider is one where the type of a feature is
changed. An example of this type of transformation which will play a central role several places
in this book is one-out-of-K coding. Suppose we have a discrete ordinal variable x which takes K
discrete values, for instance K = 4 and x = 1, 2, 3 or 4. A one-out-of-K coding of x corresponds to
a vector z of dimension K where entry i is 1 only if x = i and otherwise 0. For instance if x = 3
</p>
<p>then z =
[
0 0 1 0
</p>
<p>]T
. If we consider the Origin-feature in the Cars dataset and recall this variable
</p>
<p>could take three possibly values (USA, Germany and France) and so a one-out-of-K coding is the
mapping:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>26 2 Data and attribute types 
</p>
<p>3
1
1
2
1
2
1
3
...
1
</p>
<p>
↔
</p>
<p>
</p>
<p>0 0 1
1 0 0
1 0 0
0 1 0
1 0 0
0 1 0
1 0 0
0 0 1
</p>
<p>...
1 0 0
</p>
<p>
(2.2)
</p>
<p>What is the advantage of one-out-of-K coding? Later, we will see this representation makes certain
machine-learning methods easier to describe. However, for now notice coding the country as an
integer implied an ordering in the countries. For many machine-learning methods this means that
it matters if for instance we let Germany correspond to 2 or 3 even though this assignment is only
our convention. If we apply a one-out-of-K coding, the assignment is symmetric since no machine-
learning technique will depend on the ordering of the features. For this reason a one-out-of-K coding
is recommendable when one has ordinal variables which in fact represent different categories and
apply a machine-learning method where the (relative magnitude) of the variable will affect what
the method does. In other words, we replace the ordinal variable with three binary variables.
</p>
<p>2.4.2 Binarizing/thresholding
</p>
<p>Another feature transformation where the type of the feature is changed is binarizing or thresholding.
Suppose we select a constant θ, then binarizing a number x at θ is simply to replace x with 1 if
x ≥ θ and otherwise 0. More formally for a vector x this is written as
</p>
<p>x↔
</p>
<p>
1[θ,∞[(x1)
1[θ,∞[(x2)
</p>
<p>...
1[θ,∞[(xN )
</p>
<p> (2.3)
where
</p>
<p>1A(x) =
</p>
<p>{
1 if x ∈ A
0 if x /∈ A (2.4)
</p>
<p>is known as the indicator function.
With all these transformations available it may appear difficult to determine what we should
</p>
<p>actually do with our data. The rule of thumb is to initially do as little as we can get away with.
Typically this means we should somehow treat missing values or (gross) outliers if they exist in
our data, and then if the method we wish to apply is known to be affected by features being badly
scaled or not in a 1-out-of-K coding (this would be the case with for instance PCA in the next
chapter) we can apply these transformations. Once this is over, we can begin to work with our data
and later, once we have set up methods for evaluating the performance of our methods, possibly
consider additional transformations.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>2.4 Feature transformations 27
</p>
<p>Problems
</p>
<p>2.1. Fall 2011 question 1: Consider the data set de-
scribed in Table 2.3. Which statement about the at-
tributes in the data set is correct?
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Age of Mother in Whole Years Age
x2 Mothers Weight in Pounds MW
x3 Race (1 = Other, 0 = White) Race
x4 History of Hypertension (1 = Yes, 0 = No) HT
x5 Uterine Irritability (1 = Yes, 0 = No) UI
x6 Number of Physician Visits First Trimester PV
</p>
<p>y Birth Weight in Kilo Grams BW
</p>
<p>Table 2.3: Attributes in a study on risk factors associated
with giving birth to a low birth weight (less than 2.5 kg)
baby [Hosmer and Lemeshow, Applied Logistic Regres-
sion, 1989]. The data we consider contains 189 observa-
tions, 6 input attributes x1–x6, and one output variable
y.
</p>
<p>A Race, HT and UI are ordinal.
B Age and PV are ratio.
C Age is continuous and ratio.
D MW is discrete whereas PV is continuous.
E Don’t know.
</p>
<p>2.2. Fall 2013 question 1: We consider a dataset about
the Galápagos islands which is the famous group of is-
lands studied by Charles Darwin situated at the equator
in the Pacific Ocean. The data is taken from http://
www.statsci.org/data/general/galapagos.html and
comprises several measurements of characteristics of
twenty nine of the islands in the Galápagos. We will
presently consider a subset of this data given by the seven
attributes outlined in Table 2.4.
</p>
<p>Which one of the following statements is correct?
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Number of plant species Plants
x2 Number of endemic plant species E-Plants
x3 Area of island (in km
</p>
<p>2) Area
x4 Max. elevation above sea-level (in m) Elev
x5 Distance to nearest island (in km) DistNI
x6 Distance to Santa Cruz Island (in km) StCruz
x7 Area of adjacent island (in km
</p>
<p>2) AreaNI
</p>
<p>Table 2.4: The seven attributes of the data on a selection
of 29 of the Galápagos islands.
</p>
<p>A All the attributes are ratio and continuous.
B All the attributes are interval and discrete.
C Only two of the attributes are discrete but all at-
</p>
<p>tributes are ratio.
D Some of the attributes can take negative values.
E Don’t know.
</p>
<p>2.3. Fall 2014 question 1:
</p>
<p>No. Attribute description
</p>
<p>x1 Liters of gasoline consumed by an engine per hour
x2 Charge, as measured by number of ionized atoms
x3 Order in which runners finish a marathon
x4 Brand of car (Toyota, VW, BMW, etc.)
x5 Longitudinal position of a city on earth
</p>
<p>Table 2.5: Five different attributes
</p>
<p>Consider the six attributes with their description in
table 2.5. Which of the following statements about the
type of the attributes is true?
</p>
<p>A x1 is continuous interval, x2 is discrete ratio and x3
and x4 are ordinal.
</p>
<p>B x2 is discrete interval, x3 is discrete ordinal and x5
is continuous ratio.
</p>
<p>C x1 and x5 are continuous ratio and x4 is nominal.
D x1 is continuous ratio, x2 discrete ratio and x5 is
</p>
<p>continuous interval.
E Don’t know.</p>
<p></p>
<div class="annotation"><a href="http://www.statsci.org/data/general/galapagos.html">http://www.statsci.org/data/general/galapagos.html</a></div>
<div class="annotation"><a href="http://www.statsci.org/data/general/galapagos.html">http://www.statsci.org/data/general/galapagos.html</a></div>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>3
</p>
<p>Principal Component Analysis
</p>
<p>Principal component analysis (PCA) is a widely applicable technique where the goal is to find a
lower-dimensional representation of a high-dimensional dataset. A lower-dimensional representa-
tion is useful in a variety of circumstances. For instance (lossy) compression, as a pre-processing
step of very high-dimensional data or as a powerful visualization technique. PCA was invented in
1901 by the Statistician Karl Pearson[Pearson, 1901], but has been re-discovered numerous times
in other fields under different names such as the Kosambi-Karhunen-Loève transform in signal
processing [Kosambi, 1943], the Hotelling transform in quality control [Hotelling, 1933].
</p>
<p>PCA builds on simpler and much earlier discovered tools from linear algebra, most importantly
the Singular Value Decomposition that was discovered independently by mathematicians Eugenio
Beltrami and Camille Jordan discovered in 1873 and 1874 [Stewart, 1993].
</p>
<p>Before introducing PCA we will first recap standard definitions from linear algebra. A reader
familiar with subspaces and projections can skip this section.
</p>
<p>3.1 Projections and subspacesF
</p>
<p>Recall an M -dimension vector space is simply the set of M -dimensional vectors x where
</p>
<p>x =
</p>
<p>
x1
x2
...
xM
</p>
<p> , (3.1)
and we write this as x ∈ RM . Further, recall that if x,y ∈ RM and a, b are real numbers then the
vector
</p>
<p>z = ax+ by
</p>
<p>also belongs to RM and we say that RM is closed under linear transformations. Finally, recall that
the transpose of a vector x is written as xT and corresponds to flipping the vector along its diagonal:
</p>
<p>xT =
[
x1 x2 · · · xM
</p>
<p>]
.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>30 3 Principal Component Analysis
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−5 −4 −3 −2 −1 0 1 2 3 4 5
</p>
<p>−5
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>w2
</p>
<p>w1
</p>
<p>x1
</p>
<p>x2
</p>
<p>x
3
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>0
</p>
<p>1
</p>
<p>Fig. 3.1: Examples of a one- and two-dimensional subspace of R2 and R3 respectively. The subspaces
can be thought of as generated by all linear combinations of the basis vectors shown as the red
arrows.
</p>
<p>3.1.1 Subspaces
</p>
<p>A subspace of a vector space RM is a line, a plane, or their higher-dimensional generalizations. The
key property of a subspace is that it is closed under linear transformations. Thus, a subspace V of
RM is a set of M -dimensional vectors such that if x,y ∈ V then
</p>
<p>ax+ by ∈ V,
</p>
<p>for any values of a and b. A simpler way to think of a subspace is that the subspace is generated
by a set of vectors. Suppose x1, . . . ,xn ∈ RM is any set of n vector in RM , we can then define the
span of x1, . . . ,xn as all vectors z that can be written as
</p>
<p>z = a1x1 + a2x2 + · · ·+ anxn (3.2)
</p>
<p>where a1, . . . , an are arbitrary. We write this set of vectors as V = span(x1, . . . ,xn) and it is easy
to show that V is a subspace of RM . To consider a simple example, suppose we consider the span
of a single vector
</p>
<p>V1 = span
</p>
<p>([
1
1
2
</p>
<p>])
, (3.3)
</p>
<p>then V1 corresponds to all vectors that can be written as
</p>
<p>[
x
y
</p>
<p>]
= a1
</p>
<p>[
1
1
2
</p>
<p>]
(for arbitrary a1) and
</p>
<p>they are shown as the black line in fig. 3.1 (left pane) where the red arrow is the vector
</p>
<p>[
1
1
2
</p>
<p>]
. A
</p>
<p>slightly more elaborate example is shown in the right pane of fig. 3.1 where we consider the plane
V2 = span(w1,w2) where</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.1 Projections and subspacesF 31
</p>
<p>w1 =
</p>
<p> 1−1
0
</p>
<p> and w2 =
 10
</p>
<p>0.3
</p>
<p> . (3.4)
Notice, nothing prevents the span of M vectors to be all of RM . To take a few more definitions,
remember the length of a vector x is
</p>
<p>‖x‖ =
√
xTx =
</p>
<p>√
x21 + x
</p>
<p>2
2 + · · ·x2M (3.5)
</p>
<p>and two vectors x,y are orthogonal if xTy = 0. For instance[
1
−1
</p>
<p>]T [
1
1
</p>
<p>]
= 0
</p>
<p>and the reader can check these two vectors are indeed orthogonal by drawing them on a sheet of
paper. A set of vectors x1, . . . ,xn are said to be linearly independent if
</p>
<p>0 = a1x1 + a2x2 + · · ·+ anxn
</p>
<p>implies a1 = a2 = · · · = an = 0. Otherwise, they are said to be linearly dependent.
This brings us to the first central definition: A basis of a subspace V is a set of vectors v1, . . . ,vn
</p>
<p>such that
span(v1, . . . ,vn) = V
</p>
<p>and v1, . . . ,vn are linearly independent.
The definition of a basis simply means that v1, . . . ,vn are sufficient to generate V and at the
</p>
<p>same time V cannot be generated by fewer than n vectors. Notice it is always possible to find
an alternative basis for a subspace (for instance, just multiply one of the basis vectors with −1).
However, the number of vectors in the basis n will always be the same. We can therefore say that
n is the dimension of the subspace. If we return to fig. 3.1, the red vectors form a basis for the two
spaces shown in the two panes and they have dimension 1 and 2 respectively corresponding to a
line and a plane.
</p>
<p>It is often convenient that the vectors in the basis are of length 1 and pairwise orthogonal, i.e.
vTi vj = 0 for i 6= j. If this is satisfied for a basis v1, . . . ,vn the basis is said to be orthonormal. It
is always possible to find an orthonormal basis for a subspace.
</p>
<p>3.1.2 Projection onto a subspace
</p>
<p>Suppose we consider any vector x in a subspace V with orthonormal basis v1, . . . ,vn. Then by the
definition of a subspace x can be written as
</p>
<p>x = a1v1 + a2v2 + · · ·+ · · · anvn,
</p>
<p>for suitable choice of a1, a2, . . . , an. The reason why an orthonormal basis is so important is that it
allows us to easily compute the numbers a1, a2, . . . , an. Say for instance that we wish to compute
ai, then simply multiply both sides of the above equation with v
</p>
<p>T
i to obtain:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>32 3 Principal Component Analysis
</p>
<p>v2
</p>
<p>v1
</p>
<p>x
</p>
<p>y
</p>
<p>z
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>x
T
v1
</p>
<p>x
T
v
2
</p>
<p>0 1 2 3 4
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>4
</p>
<p>Fig. 3.2: Left pane: Projection of a 3D point x (blue circle) onto the subspace spanned by the
orthonormal basis v1,v2. The projection, shown as the red square, lies within the subspace and the
right pane shows the point in the 2D coordinate system given by the basis vectors of the subspace.
</p>
<p>vTi x = a1v
T
i v1 + · · ·+ aivTi vi + anvTi vn
</p>
<p>= a1 · 0 + · · ·+ ai · 1 + · · ·+ an · 0
= ai (3.6)
</p>
<p>and so we can find ai by simply computing ai = x
Tvi. However, what if x does not lie in V ? We
</p>
<p>can still compute the n numbers
</p>
<p>b1 = x
Tv1, b2 = x
</p>
<p>Tv2, . . . bn = x
Tvn
</p>
<p>and then form a new vector x′ which does lie in V :
</p>
<p>x′ = b1v1 + · · ·+ bnvn. (3.7)
One can show x′ is the point in V closest to x and we say that x′, defined above, is the projection
of x onto V . We also say the n-dimensional vector
</p>
<p>b =
</p>
<p>
b1
b2
. . .
bn
</p>
<p>
is the coordinates of x in the subspace V . Notice, this is an n-dimensional vector whereas the space
that x was in is M -dimensional.
</p>
<p>In the left pane of fig. 3.2 is shown the projection of the blue point in R3 onto the space spanned
by the two basis vectors v1,v2 shown as arrows. In the right pane we have plotted the projected
point, x′, with the coordinates in the new space V .</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.2 Principal Component Analysis 33
</p>
<p>x1
</p>
<p>x
2
</p>
<p>0 0.5 1 1.5 2
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>Fig. 3.3: (Left:) A simple 2D dataset X comprising two features x1 and x2 that are noisy observa-
tions of the same quantity. Therefore, the dataset in fact only contains one “data-dimension” even
though it is two-dimensional. (Right:) A more elaborate dataset example corresponding to N = 9
observations each corresponding to 48 × 48 pixel images of the same digit but rotated. From the
perspective of the matrix X the dataset contains M = 2304 attributes, (corresponding to the num-
ber of pixels), however from another perspective only one dimension matters, namely the rotation.
PCA is able to discover the lower-dimensional representation of the dataset X in the first example
but not in the second.
</p>
<p>Finally, suppose we collect the basis vectors v1,v2, . . . ,vn in a matrix V :
</p>
<p>V =
[
v1 v2 . . . vn
</p>
<p>]
(3.8)
</p>
<p>We can then express the coordinates of x in the space V as
</p>
<p>bT = xTV (3.9)
</p>
<p>Thus, suppose the original blue points was x ∈ R3, we can then express it’s new two-dimensional
coordinates (b1, b2) plotted in the right-pane of fig. 3.2 as the red square:
</p>
<p>bT =
[
b1 b2
</p>
<p>]
= xT
</p>
<p>[
v1 v2
</p>
<p>]
</p>
<p>3.2 Principal Component Analysis
</p>
<p>Suppose we consider a two-dimensional dataset, however, unbeknownst to us the two dimensions
are just noisy measurements of the same quantity. If we plot each observation in the dataset as a</p>
<p></p>
</div>, <div class="page"><p></p>
<p>34 3 Principal Component Analysis
</p>
<p>point we obtain a figure similar to fig. 3.3 (left pane) where the points nearly lie on a straight line.
Even though the dataset is two-dimensional, there is really only a single dimension that matters,
namely the line along which the observations lie. For a more ambitious example, consider a dataset
comprised of N = 9 observations of the same image of the digit 1 but rotated around the center
shown in fig. 3.3 (right pane). Each image is 48 × 48 pixels large so if we consider each pixel as a
feature we can represent an image as one long vector in a M = 2304 dimensionsal space. However,
from a more human perspective the true number of dimensions is really only one, namely the angle
of rotation.
</p>
<p>That the number of “true” dimensions in the dataset is often much lower than the number of
observed dimensions is a common feature of many machine learning problems and the goal of prin-
cipal component analysis is to discover a lower-dimensional representation of the high-dimensional
data set where it is assumed the lower-dimensional representations are linear. This is the case for
2D example in fig. 3.3 (left pane) which PCA can solve, however, not the case for the rotated digits
example in the right pane which would require more advanced methods than will be considered
here.
</p>
<p>To make the discussion concrete, assume we are in the standard setting encountered previ-
ously where we are given N observations x1,x2, . . . ,xN ∈ RM each consisting of M features or
dimensions. In principal component analysis we select a number n and then we wish to find a new
n-dimensional representation
</p>
<p>b1, b2, . . . , bN ∈ Rn
</p>
<p>where n ≤M and such that bi “represents” the observation xi. To return to the previous example
in fig. 3.3 (left pane) M = 2 (the number of observed features) and the representation we are
interested in would have n = 1.
</p>
<p>The simplest way to transform vectors from a M dimensional space to a n ≤ M -dimensional
space is to select an orthonormal basis v1, . . . ,vn of a n-dimensional subspace V and define each
bi as the projection of xi onto V . Since we want the projection to be invariant under addition of a
constant we first subtract the mean from each xi. The general layout of the PCA algorithm is then:
</p>
<p>• Compute the mean m = 1N
∑N
i=1 xi
</p>
<p>• Subtract the mean from xi: x̃i = xi −m (and collect all x̃i into an N ×M matrix X̃)
• Project onto V : bTi = x̃Ti V
</p>
<p>where
V =
</p>
<p>[
v1 v2 · · · vn
</p>
<p>]
is the projection matrix corresponding to the basis v1, . . . ,vn. Notice the centering step where the
mean is subtracted is the same scheme that we encountered in the previous chapter.
</p>
<p>So how do we select the projection matrix V ? We will first consider the simplest case in which
n = 1, i.e. we are projecting onto a line, and later consider the general case n &gt; 2. It is useful
to consider a concrete example to get some intuition about what different choices of v1 implies.
In fig. 3.4 (top panes) we have shown the projection of the same 2D dataset onto three different
choices of v1 and in the bottom panes we have plotted the projected coordinates bi = x̃
</p>
<p>T
i v1, i.e.
</p>
<p>the 1-dimensional “representation” of each x̃i. From an intuitive point of view the first projection
is worse than the last since it lumps the observations together with large residuals (indicated by
the blue and red lines). The same pattern is repeated in fig. 3.5, here we consider a 3d dataset and
still project it onto different 1-dimensional subspaces. The first projection has large residuals and
also lumps all classes together while in the third the residuals are much smaller and the data in the</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.2 Principal Component Analysis 35
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>x
T
v1
</p>
<p>−5 0 5
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>x
T
v1
</p>
<p>−5 0 5
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>x
T
v1
</p>
<p>−5 0 5
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>Fig. 3.4: An example 2D dataset colored for our convenience (the dots in the top panes are the same
in all 3 panes) is projected onto three different 1D subspaces corresponding to different choices of
basis vector v1. The dataset in the projected coordinate system is shown in the bottom pane. As
can be seen, different choices of basis vectors preserve different amounts of information, with the
vector corresponding to the right-most pane preserving the most information since the dataset is
the most spread out.
</p>
<p>projection thereby more spread out thus preserving more information about the data. What these
three projections have in common is that the observations, in the projected coordinates b1, . . . , bN ,
are more spread out (thereby providing smaller residuals indicated by the lines connecting the
points to the 1-dimensional subspace). The degree to which the projected observations are spread
out can be measured by the variance of b1, . . . , bN scaled by a factor of N
</p>
<p>1:
</p>
<p>W = N ×Variance[b1, . . . , bN ] = N
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>(
bi − b̄
</p>
<p>)2
=
</p>
<p>N∑
i=1
</p>
<p>(
bi − b̄
</p>
<p>)2
, where: b̄ =
</p>
<p>1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>bi. (3.10)
</p>
<p>The reason for multiplying with N will be apparent later. This immediately gives us an idea for
selecting v1: We simply make it the goal of PCA to select v1 to be the vector which maximizes the
</p>
<p>1 We here consider the biased estimate of variance where we divide by N , see chapter 4.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>36 3 Principal Component Analysis
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>x
T
v1
</p>
<p>−5 0 5
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>x
T
v1
</p>
<p>−5 0 5
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>x
T
v1
</p>
<p>−5 0 5
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>Fig. 3.5: Similar to the example fig. 3.4, a 3D dataset (colored for our convenience) is projected
onto three different 1D subspaces corresponding to different choices of basis vector v1. The dataset
in the projected coordinate system is shown in the bottom pane. As can be seen, different choices
of basis vectors preserve different amounts of information, with the vector corresponding to the
right-most pane preserving the most information since the dataset is the most spread out.
</p>
<p>spread-outness of the projected data and would therefore select the right-most figure in fig. 3.4 and
fig. 3.5 over the other. To put it formally,
</p>
<p>v1 = The vector of length 1 that maximize W
</p>
<p>= arg max
vT v=1
</p>
<p>W.
</p>
<p>In the next section we will solve this maximization problem and then consider the general case
where n &gt; 1.
</p>
<p>Maximizing the variance with respect to the first principal component
</p>
<p>First notice that the mean of the projected data b̄ is zero since we have subtracted the mean from
X:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.2 Principal Component Analysis 37
</p>
<p>b̄ =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>bi =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>x̃Ti v1 =
</p>
<p>(
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>x̃i
</p>
<p>)T
v1 =
</p>
<p>([
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>xi
</p>
<p>]
−m
</p>
<p>)T
v1 = 0 (3.11)
</p>
<p>If we define the matrix S = X̃
T
X̃ we can re-write the variance W to be:
</p>
<p>W =
</p>
<p>N∑
i=1
</p>
<p>bTi bi =
</p>
<p>N∑
i=1
</p>
<p>(x̃Ti v1)
T x̃Ti v1 =
</p>
<p>N∑
i=1
</p>
<p>vT1 x̃ix̃
T
i v1 = v
</p>
<p>T
1 X̃
</p>
<p>T
X̃v1 = v
</p>
<p>T
1 Sv1 (3.12)
</p>
<p>For v1 to be an orthonormal basis it has to have norm 1, ‖v1‖2 = vT1 v1 = 1. The maximization
of eq. (3.12) under this constraint can be done by introducing the Lagrangian multiplier λ and
maximizing the Lagrangian 2
</p>
<p>L = W + λ(1− ‖v1‖2) = vT1 (S − λI)v1 + λ (3.13)
</p>
<p>with respect to λ and v1. Taking the derivatives with respect to the vector and λ we obtain:
</p>
<p>∂
</p>
<p>∂λ
L = 1− vT1 v1 = 0 (3.14)
</p>
<p>∇v1L = (S − λI)v1 = 0. (3.15)
</p>
<p>From the first equation we simply observe that v1 should be normalized. The second equation can
be re-written as:
</p>
<p>Sv1 = λv1 (3.16)
</p>
<p>This means that v1 should be an eigenvector of S with eigenvalue λ. So which eigenvector should we
choose? If we look at the cost function eq. (3.12) which we wish to maximize it can be re-written:
</p>
<p>W = vT1 Sv1 = v
T
1 λv1 = λ.
</p>
<p>Thus, we should select v1 as the eigenvector of S corresponding to the largest eigenvalue.
This solves the case n = 1. The case n ≥ 2 is similar but requires slightly more work. First we
</p>
<p>collect the bi’s into an N × n matrix
</p>
<p>BT =
[
b1 b2 · · · bN
</p>
<p>]T
.
</p>
<p>The projection can then be written as
B = X̃V
</p>
<p>Previously we defined the spread-outness of B by taking the sum of squares of the entries in B.
We will simply re-use this idea for the case n &gt; 1 and thereby define the Frobenius norm:
</p>
<p>W = ‖B‖2F =
N∑
i=1
</p>
<p>n∑
j=1
</p>
<p>B2ij
</p>
<p>2 If you are unfamiliar with Langrangian multipliers see Appendix A and https://en.wikipedia.org/
wiki/Lagrange_multiplier.</p>
<p></p>
<div class="annotation"><a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">https://en.wikipedia.org/wiki/Lagrange_multiplier</a></div>
<div class="annotation"><a href="https://en.wikipedia.org/wiki/Lagrange_multiplier">https://en.wikipedia.org/wiki/Lagrange_multiplier</a></div>
</div>, <div class="page"><p></p>
<p>38 3 Principal Component Analysis
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>x
T
v1
</p>
<p>x
T
v
2
</p>
<p>−4−2024
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>x
T
v1
</p>
<p>x
T
v
2
</p>
<p>−4−2024
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>x
T
v1
</p>
<p>x
T
v
2
</p>
<p>−4−2024
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>Fig. 3.6: This time, the same 3D dataset as in fig. 3.5 is projected onto three different 2D planes
corresponding to the three panes in the top row. The inserts in the bottom row is the points in
the coordinate system corresponding to the subspace. We again see the right most pane, where the
points are the most spread out, seems to better preserve the information in the dataset.
</p>
<p>We can therefore simply maximize W where we ensure vTi vj = 0 for i 6= j since we are looking for
an orthonormal basis. To put it formally,
</p>
<p>v1, · · · ,vn = The n orthonormal vectors that maximize W.
</p>
<p>It turns out the solution to this maximization problem is to select v1, . . . ,vn as the n orthonormal
eigenvectors of S with the n largest eigenvalues. The case n = 2 is illustrated in fig. 3.6 where the
right-most plane corresponds to the two largest eigenvectors and the other two panes corresponds
to different choices of basis. We again see the choice of eigenvectors ensures the observations are the
most spread out. Now that we have defined v1, · · · , vn this formally completes the PCA algorithm.
However, there is a simple (and natural) way to represent the eigenvectors using the Singular value
decomposition (SVD) which makes PCA much easier to compute. We will therefore introduce the
SVD and explain its relationship to PCA.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.3 Singular Value Decomposition and PCA 39
</p>
<p>3.3 Singular Value Decomposition and PCA
</p>
<p>The singular value decomposition (SVD) provides an easy way to compute the n eigenvectors
corresponding to the n largest eigenvalues. For any N ×M matrix X, the SVD computes three
matrices
</p>
<p>Σ =
</p>
<p>
</p>
<p>σ1 0 · · · 0
0 σ2 0
...
</p>
<p>. . .
...
</p>
<p>0 0 · · · σM
0 0 · · · 0
...
</p>
<p>...
...
</p>
<p>0 0 · · · 0
</p>
<p>
, U =
</p>
<p>[
u1,u2, . . . ,uN
</p>
<p>]
V =
</p>
<p>[
v1,v2, . . . ,vM
</p>
<p>]
</p>
<p>such that
UΣV T = X
</p>
<p>and V TV = I, UTU = I and σ1 ≥ σ2 ≥ · · · ≥ σM are known as the singular values of X. Notice
these conditions implies that vTi vj = 0 if i 6= j and otherwise 1; in other words the columns of V
are orthonormal. This has some interesting consequences. For instance if we compute:
</p>
<p>V Tvi =
</p>
<p>
</p>
<p>vT1
...
vTi
...
vTM
</p>
<p>vi =

</p>
<p>vT1 vi
...
</p>
<p>vTi vi
...
</p>
<p>vTMvi
</p>
<p> =

</p>
<p>0
...
1
...
0
</p>
<p> = ei
</p>
<p>This can be used to show:
</p>
<p>(XTX)vi = (V Σ
TUTUΣV T )vi = V Σ
</p>
<p>TΣei = V σ
2
i ei = σ
</p>
<p>2
i vi
</p>
<p>So each vi is an eigenvector of X
TX with associated eigenvalue σ2i and the eigenvectors are sorted
</p>
<p>according to their eigenvalues. This should sound very familiar. Indeed, by our previous definition
the first n columns of V
</p>
<p>V n =
[
v1 v2 · · · vn
</p>
<p>]
,
</p>
<p>are by definition the first n eigenvectors and, by simply using the definition of projection onto a
subspace eq. (3.9), the projection of a single observation xTi onto the subspace spanned by the first
n principal components can therefore be written as bTi = x
</p>
<p>T
i V n.
</p>
<p>3.3.1 The PCA algorithm
</p>
<p>Gathering the previous discussion, we can define the PCA algorithm on a matrix X where the
dataset is projected onto the first n components as follows:
</p>
<p>• Subtract the mean: x̃i = xi −m, m = 1N
∑N
i=1 xi
</p>
<p>• Divide by standard deviation (Optional): x̃ij = x̃ijsk , where sk =
√
</p>
<p>1
N−1
</p>
<p>∑N
i=1 x̃
</p>
<p>2
ik</p>
<p></p>
</div>, <div class="page"><p></p>
<p>40 3 Principal Component Analysis
</p>
<p>• Compute the SVD: UΣV T = X̃
• The n first principal components are v1, . . . ,vn and coordinates of observation i when projected
</p>
<p>onto the subspace spanned by the first n principal components are
</p>
<p>bTi = x̃
T
i V n or alternatively B = X̃V n
</p>
<p>where V n =
[
v1, . . . ,vn
</p>
<p>]
.
</p>
<p>In the above steps are included the optional step indicated in gray to not only center the data but
also normalize each attribute by further dividing each attribute by its standard deviation. This
optional step may be relevant when the attributes have different scales, i.e. if one attribute have
very large variance compared to the other attributes the first principal component will be highly
driven by this attribute in order to account for as much of the variance as possible in the data.
</p>
<p>3.3.2 Variance explained by the PCA
</p>
<p>The vectors bi in the matrix B represents the coordinates of the vector xi when it is projected onto
the n-dimensional subspace, i.e. the bottom row in fig. 3.5. What if we want to know what vector
</p>
<p>in the original space bi =
[
bi1 bi2 · · · bin
</p>
<p>]T
corresponds to? I.e. the intersection with the line in the
</p>
<p>top-row of fig. 3.5? Similar to eq. (3.7) we can find the projected coordinates in the original space
as:
</p>
<p>x′i = bi1v1 + · · ·+ binvn = V nbi,
or if we choose this can be written more condensed for all observations as
</p>
<p>X ′ =
(
V nB
</p>
<p>T
)T
</p>
<p>= X̃V nV
T
n . (3.17)
</p>
<p>The case n = M is worth mentioning. In this case by definition V n = V and so, by orthonormality
of V , we obtain:
</p>
<p>X ′ = X̃V V T = X̃I = X̃
</p>
<p>that is, if all M principal directions are used the projection does not “lose” any information. In this
case the projected matrix B is still different from X̃ – it is exactly corresponding to “rotating”
all observations (in M dimensions!) and then, when translating back to X ′, we just “rotate back”
without losing information.
</p>
<p>In general, the reconstructed matrix X ′ will have lost information if n &lt; M (or alternatively,
variability in the data) compared to X̃ — in linear algebra terms we lose all variability of X̃
which is orthogonal to the space V n because we project those directions away. A natural way to
measure how much variance — or information about X̃ — is retained in a reconstruction based on
n principal components is the variance explained computed using the Fröbenius norm:
</p>
<p>Variance Explained =
‖X ′‖2F
‖X̃‖2F
</p>
<p>Using the fact that ‖X‖2F = trace(XTX) and plugging in the definition of the SVD (exploiting
(AB)T = BTAT and trace(AB) = trace(BA)) one can show that ‖X ′‖2F =
</p>
<p>∑n
i=1 σ
</p>
<p>2
i and ‖X̃‖2F =∑M
</p>
<p>i=1 σ
2
i . Plugging this in we get the formula for the variance explained by the n first principal
</p>
<p>components:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.4 Applications of principal component analysis 41
</p>
<p>x1
</p>
<p>x2
</p>
<p>x
3
</p>
<p>5.5
</p>
<p>6
</p>
<p>6.5
</p>
<p>2.6
2.8
3
</p>
<p>3.2
3.4
</p>
<p>3.6
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>4
</p>
<p>4.5
</p>
<p>5
</p>
<p>v1
</p>
<p>v2
</p>
<p>v3
</p>
<p>x1
</p>
<p>x2
</p>
<p>x
3
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>Fig. 3.7: The Fisher Iris dataset consisting of N = 150 observations of three types of flowers. In the
left pane the dataset is plotted as a scatter plot whereas in the right pane the mean of the dataset
has been substracted to obtain the centered matrix X̃ and the three principal directions are plotted
as unit vectors.
</p>
<p>Variance Explained =
</p>
<p>∑n
i=1 σ
</p>
<p>2
i∑M
</p>
<p>i=1 σ
2
i
</p>
<p>.
</p>
<p>As we expect the case where n = M guarantees that all variance will be conserved, however, if
some σi are zero we can still perfectly represent all variance with fewer than M directions. Why?
This case corresponds to the dataset residing in a subspace of V having dimension less than M . In
two dimensions, this could be if the observations fall exactly on a line.
</p>
<p>3.4 Applications of principal component analysis
</p>
<p>For our first application of PCA we will consider the Fisher Iris dataset consisting of N = 150
observations of three different types of flowers. The original data contains four features but we will
presently only consider three of the features in order to visualize the data prior to the PCA, thus
each observation consists of M = 3 features. The dataset, labelled according to flower type, is shown
in fig. 3.7 (left pane). The first step of the PCA analysis is to subtract the mean of the dataset
</p>
<p>to obtain the centered matrix X̃ (X̃ij = Xij − 1N
∑N
i=1Xij). Then, a SVD is performed to obtain
</p>
<p>the decomposition X̃ = UΣV T . The mean vector m and the principal component directions, i.e.
columns of V =
</p>
<p>[
v1 v2 v3
</p>
<p>]
, are approximately:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>42 3 Principal Component Analysis
</p>
<p>X̃v1
</p>
<p>−2 −1 0 1 2
</p>
<p>X̃v1
</p>
<p>X̃
v
2
</p>
<p>−1.5 −1 −0.5 0 0.5 1 1.5
</p>
<p>−1.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>X̃v1
X̃v2
</p>
<p>X̃
v
3
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 3.8: The Fisher Iris dataset of fig. 3.7 projected onto the principal directions. In the first pane
only the first principal direction is used corresponding to a large loss of information. In the second
pane two principal directions are used better preserving the structure of the data and finally the
third pane use all principal directions and therefore only corresponds to a rotation.
</p>
<p>m =
</p>
<p>5.83.1
3.8
</p>
<p> , v1 =
 0.4−0.1
</p>
<p>0.9
</p>
<p> , v2 =
−0.6−0.7
</p>
<p>0.2
</p>
<p> , v3 =
−0.70.7
</p>
<p>0.3
</p>
<p> ,
and are shown in fig. 3.7 as colored vectors. Notice the first principal direction follows the main
direction of variability in the data.
</p>
<p>Let’s consider the projections onto the principal directions. These can be found in fig. 3.8 where
for instance the middle figure shows the projection onto the first and second principal direction
found by computing the vectors X̃v1 and X̃v2 and plotting these as a scatter plot. As can be
seen the right-most figure, corresponding to using all principal directions, simply corresponds to
rotating the dataset in fig. 3.7 until the PCA directions are oriented along the axis. The other
plots are obtained by projecting away principal direction v3 and then v3 and v2 and therefore lose
information. Finally let’s turn to the variance explained by the principal directions. This can be
computed from Σ and in our example
</p>
<p>Σ =
</p>
<p>11.7 0 00 3.0 0
0 0 1.5
</p>
<p> ,
and the variance explained by each component can be seen in fig. 3.9. For instance the first com-
</p>
<p>ponent alone explains 11.7
2
</p>
<p>11.72+32+1.52 ≈ 92% of the variance.
</p>
<p>3.4.1 A high-dimensional example
</p>
<p>The previous illustrations of principal component analysis has considered 1- and 2-dimensional
subspaces in 2- or 3-dimensional spaces. These applications are appealing since we can visualize each
step, however, the utility of PCA stems from its applicability to very high-dimensional problems.
</p>
<p>We will consider the MNIST dataset and first limit ourselves to N = 1000 observations corre-
sponding to images of the digits 0 and 1. Recall the MNIST dataset contains 28× 28 pixel images</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.4 Applications of principal component analysis 43
</p>
<p>By v1 alone
</p>
<p>By v2 alone
</p>
<p>By v3 alone
</p>
<p>Principal Direction
</p>
<p>F
ra
ct
io
n
o
f
v
a
ri
a
n
ce
</p>
<p>ex
p
la
in
ed
</p>
<p>1 1.5 2 2.5 3
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>By v1
By v1,v2
By v1,v2,v3
</p>
<p>Principal Directions
</p>
<p>F
ra
ct
io
n
o
f
v
a
ri
a
n
ce
</p>
<p>ex
p
la
in
ed
</p>
<p>1 1.5 2 2.5 3
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 3.9: (Left:) Variance explained by each of the three principal directions from the Fisher Iris
example of fig. 3.7. Nearly all variation is explained by the first and second principal directions
which can be interpreted as the original dataset residing on a 2D plane spanned by the first and
second principal directions. (Right:) Cumulative variance explained by the subspaces spanned by
only v1, then v1 and v2 and finally v1, v2 and v3. Compare to fig. 3.8.
</p>
<p>and thus the dataset considered corresponds to a matrix X of size N ×M where M = 784. Sup-
pose we compute the first two principal components of X̃ using the PCA algorithm and plot X̃
projected onto these first two components, X̃V 2. A plot of the projected data can be seen in the
top of fig. 3.10 where the red dots correspond to 1’s and the blue dots to 0’s. From the plot we
learn that the first two principal components, and in fact only the first which is plotted along the
x-axis, is enough to distinguish these two classes fairly well. To get an idea about what the two
principal components consist of we place a grid on top of the projected dots (top right pane) and
select those observations nearest to the grid points. The corresponding observations are plotted in
the bottom left pane of fig. 3.10. From this plot we learn that the first principal component (as
we suspected) captures the variability between 1 and 0, meanwhile the second principal component
appears to capture how slanted the digit is (this is particular apparent for the digit 1) and how
bold the digit is. In the bottom right-most pane we have plotted the same images as in the left
pane but projected back into the original space using eq. (3.17) (we have added back the mean
value for easier visualization). This plot provides a visualization of what we “see” when we consider
only the first two principal components. The plot confirms our interpretation from before, however,
we also see that the two first principal components arguably correspond to a significant loss of
information, exactly as we can expect when we project a high dimensional dataset onto only the
first two principal components.
</p>
<p>To get an idea about how much information we lose, we consider the full MNIST dataset
containing all 10 classes of digits. In the top row of fig. 3.11 we plot 10 randomly selected digits
from each of the 10 classes, and in the following rows we plot the PCA reconstruction based on
n = 2, 5, 20, 50, 100 principal directions. As expected, the reconstructions are awful when only a
few directions are used. Around n = 20 directions most of the digits can be distinguished and for
n = 50 (corresponding to a compression level of almost 16) the digits are clearly distinguishable.
For n = 100 only a small amount of smudge separate the reconstruction from the true images; this
corresponds to a compression level of almost 8. In fig. 3.12 and fig. 3.13 we have plotted the data
projected onto the first two principal components as well as when projected onto the space spanned</p>
<p></p>
</div>, <div class="page"><p></p>
<p>44 3 Principal Component Analysis
</p>
<p>x
T
v1
</p>
<p>x
T
v
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>x
T
v1
</p>
<p>x
T
v
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>Fig. 3.10: N = 1000 observations from the MNIST dataset of the digits 0 and 1 are projected
onto the first 2 principal directions as shown in the top-left pane. The red dots correspond to the
digit 1 and we see the first principal direction easily allows us to distinguish the digits. If we select
the observations the closest to the grid points in the top right pane and plot the observations in
the bottom left pane we see the first principal direction seperates 1’s from 0’s while the second
corresponds to slanting and bolding (vertical direction). In the bottom-right pane, we have plotted
the PCA projections; we see much of the information is lost when projecting onto the first 2 principal
directions.
</p>
<p>by component 1 and 3 and 2 and 3. We again see the first plot easily allows 0 and 1 to be separated,
however, the remaining PCA components largely overlap.
</p>
<p>3.4.2 Uses of PCA
</p>
<p>PCA can be used in a variety of circumstances and ways. For instance</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.4 Applications of principal component analysis 45
</p>
<p>Fig. 3.11: Top row: 10 randomly chosen digits from the MNIST dataset. They are then reconstructed
from n = 2, 5, 20, 50, 100 PCA components in the next 5 rows. We see that about 50 principal direc-
tions is enough to make a fairly good reconstruction of the images corresponding to a compression
factor of about 16.
</p>
<p>Visualization PCA is a easily applicable tool for data visualization. For instance in the MNIST
dataset it allows us to distinguish the easy classification tasks (0 vs. 1) and the harder tasks.
</p>
<p>Feature extraction Applying PCA provides new features that can be used for other machine-
learning techniques.
</p>
<p>Compression As we saw in the MNIST case, PCA provides a very simple yet efficient lossy
compression method.
</p>
<p>Despite these valid points, it is important to remember that PCA, when n &lt; M , implies a loss
of information. In the MNIST case this loss of information is very significant when n &lt; 50, and
one should therefore not automatically apply PCA. We will later consider more in depth how to
validate machine-learning techniques and this will provide insight into how to determine if PCA is
applicable and also how the number of principal components n should be selected. Note, as PCA
is optimal for data compression optimized to describe as much of the data variance as possible the
principal components do not necessarily provide features that are good for classification.
</p>
<p>As a final comment, the normalization step in PCA can be of great importance. If the coordinates
represent the same type of quantity and are on the same scale, for instance pixel intensities as in this
example, the normalization step should likely be excluded. However, if the coordinates represent
different things and are on vastly different scales the normalization step is important. For instance,
suppose one dataset record the height of a person in meters as well as the person’s annual income</p>
<p></p>
</div>, <div class="page"><p></p>
<p>46 3 Principal Component Analysis
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>8
</p>
<p>9
</p>
<p>x
T
v1
</p>
<p>x
T
v
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−6
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>Fig. 3.12: N = 2000 observations of the MNIST dataset projected onto principal direction v1 and
v2
</p>
<p>in USD. PCA will then accurately detect that the variance in the dataset is primarily in the annual
income, after all this quantity will vary with many thousands between subjects while the height will
only vary with about 1 meter. In this manner, the first PC (trivially) becomes the income and PCA
will not tell us anything about the interaction between the variables. Meanwhile, normalizing this
dataset with the variance will ensure the height and income vary with roughly the same amount
and PCA will easier pick out potential correlations.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.4 Applications of principal component analysis 47
</p>
<p>x
T
v1
</p>
<p>x
T
v
3
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−6
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>x
T
v2
</p>
<p>x
T
v
3
</p>
<p>−5 0 5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>Fig. 3.13: N = 2000 observations of the MNIST dataset projected onto principal direction v1,v3
(left pane) and v2,v3 (right pane)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>48 3 Principal Component Analysis
</p>
<p>Problems
</p>
<p>3.1. Spring 2012 question 4: The first and second
principal components directions of the data in the RAT
dataset (considering only the attributes x1 − x13) in Ta-
ble 3.1 are:
</p>
<p>v1 =
</p>
<p>
</p>
<p>0.0247
−0.0388
−0.3288
−0.2131
</p>
<p>0.0477
−0.4584
</p>
<p>0.2683
−0.0838
−0.5020
−0.0200
−0.3091
−0.2588
</p>
<p>0.3714
</p>
<p>
</p>
<p>, v2 =
</p>
<p>
</p>
<p>−0.0764
0.5675
−0.0550
</p>
<p>0.2449
0.3115
−0.1999
</p>
<p>0.1738
0.3668
−0.0737
</p>
<p>0.2988
0.0628
0.4446
0.1051
</p>
<p>
</p>
<p>.
</p>
<p>and in Figure 3.14 the data projected onto the first two
principal components is plotted against the average con-
sumer ratings (RAT). Which of the following statements
is correct?
</p>
<p>Fig. 3.14: The output RAT plotted against the first and
second principal component respectively.
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Type TYPE
(0 = served cold, 1 = served hot)
</p>
<p>x2 Calories per serving CAL
x3 Grams of protein PROT
x4 Grams of fat FAT
x5 Milligrams of sodium SOD
x6 Grams of dietary fiber FIB
x7 Grams of complex carbohydrates CARB
x8 Grams of sugars SUG
x9 Milligrams of potassium POT
x10 Vitamins and minerals in 0%, 25%, VIT
</p>
<p>or 100% of FDA recommendations
x11 Shelf position SHELF
</p>
<p>(1, 2, or 3, counting from the floor)
x12 Weight in ounces of one serving WEIGHT
x13 Number of cups in one serving CUPS
x14 Name of cereal brand NAME
</p>
<p>y Average rating of the cereal RAT
(from 0 to 100)
</p>
<p>Table 3.1: Attributes in a study of ce-
reals (i.e. breakfast products, taken from
http://lib.stat.cmu.edu/DASL/Datafiles/Cereals.html).
The data we consider has 74 observations (i.e., the
original data has 77 observations but three observations
have been removed due to missing values). The data has
14 input attributes x1–x14 and one output variable y
which defines the average rating of the cereal product
given by the consumers.
</p>
<p>A Relatively high values of CAL, PROT, FAT, FIB,
SUG, POT, VIT, SHELF, and WEIGHT and low
values of TYPE, SOD, CARB, and CUPS will re-
sult in a negative projection onto the first principal
component.
</p>
<p>B PCA2 primarily discriminates between relatively low
values of PROT and high values of SHELF.
</p>
<p>C The data projected onto the second principal compo-
nent (i.e., PCA2) is positively correlated with RAT.
</p>
<p>D The principal component directions are not guaran-
teed to be orthogonal to each other since the data
has been standardized.
</p>
<p>E Don’t know.
</p>
<p>3.2. Fall 2011 question 2: Consider the data set de-
scribed in Table 3.2. Each attribute in the data set is
standardized, and we carry out a principal component
analysis (PCA) on the standardized input data, x1–x6.
The singular values obtained are: σ1 = 17.0, σ2 = 15.2,
σ3 = 13.1, σ4 = 13.0, σ5 = 11.8, σ6 = 11.3. The first and
second principal component directions are:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>3.4 Applications of principal component analysis 49
</p>
<p>v1 =
</p>
<p>
0.5238
0.5237
−0.3491
0.1981
−0.3369
0.4204
</p>
<p> , v2 =

−0.2948
0.3452
0.3584
0.6808
−0.3049
−0.3302
</p>
<p> .
</p>
<p>Which one of the following statements is incorrect?
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Age of Mother in Whole Years Age
x2 Mothers Weight in Pounds MW
x3 Race (1 = Other, 0 = White) Race
x4 History of Hypertension (1 = Yes, 0 = No) HT
x5 Uterine Irritability (1 = Yes, 0 = No) UI
x6 Number of Physician Visits First Trimester PV
</p>
<p>y Birth Weight in Kilo Grams BW
</p>
<p>Table 3.2: Attributes in a study on risk factors associated
with giving birth to a low birth weight (less than 2.5 kg)
baby [Hosmer and Lemeshow, Applied Logistic Regres-
sion, 1989]. The data we consider contains 189 observa-
tions, 6 input attributes x1–x6, and one output variable
y.
</p>
<p>A The first three principal component account for more
than 90% of the variation in the data.
</p>
<p>B Relatively heavy, old and white mothers that fre-
quently goes to the physician and have a history
of hypertension but do not have uterine irritability
will have a positive projection onto the first principal
component.
</p>
<p>C Relatively young, heavy mothers that are not white
and have a history of hypertension but infrequently
goes to the physician and do not have a uterine irri-
tability will have a positive projection onto the sec-
ond principal component.
</p>
<p>D Since the data is standardized we do not need to sub-
tract the mean when performing the PCA but can
directly carry out the singular value decomposition
on the standardized data.
</p>
<p>E Don’t know.
</p>
<p>3.3. Fall 2013 question 3: All the attributes of the
Galápagos data are standardized and a principal compo-
nent analysis carried out on the standardized attributes
x1–x7. Using the singular value decomposition on the
standardized data matrix of size 29×7 we obtain for the
matrices S and V :
</p>
<p>S =
</p>
<p>
</p>
<p>9.7 0 0 0 0 0 0
0 6.7 0 0 0 0 0
0 0 5.7 0 0 0 0
0 0 0 3.7 0 0 0
0 0 0 0 3.0 0 0
0 0 0 0 0 1.3 0
0 0 0 0 0 0 0.7
</p>
<p>
</p>
<p>V =
</p>
<p>
</p>
<p>0.50 −0.02 0.31 −0.26 0.22 −0.47 0.57
0.51 −0.04 0.26 −0.30 0.18 0.05 −0.74
0.45 −0.01 −0.02 0.78 −0.32 −0.26 −0.11
0.51 −0.15 −0.25 0 −0.02 0.75 0.32
−0.06 −0.70 0.20 −0.25 −0.64 −0.05 0.01
−0.11 −0.70 −0.10 0.28 0.64 −0.07 −0.04
0.17 −0.06 −0.85 −0.29 −0.08 −0.37 −0.10
</p>
<p>
,
</p>
<p>Which one of the following statements regarding the
principal component analysis is correct?
</p>
<p>A The first principal component accounts for more than
55% of the variance in the data.
</p>
<p>B The first three principal components accounts for
more than 90% of the variance in the data.
</p>
<p>C The last principal component accounts for more than
1% of the variance in the data.
</p>
<p>D Five principal components are required in order to
account for more than 95% of the variance in the
data.
</p>
<p>E Don’t know.
</p>
<p>3.4. Fall 2014 question 3: A principal component
analysis is applied to a dataset composed of 1000 data
points. After applying the principal component analysis,
the datapoints (gray points) along with the right singu-
lar vectors (i.e. the principal directions) (black arrows)
are plotted. Which of the subplots A,B,C,D of figure
fig. 3.15 corresponds to this description?</p>
<p></p>
</div>, <div class="page"><p></p>
<p>50 3 Principal Component Analysis
</p>
<p>Figur DFigur C
</p>
<p>Figur BFigur A
</p>
<p>−1 −0.5 0 0.5 1−1 −0.5 0 0.5 1
</p>
<p>−1 −0.5 0 0.5 1−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 3.15: Dataset of 1000 data points along with four
possible choices of right singular vectors
</p>
<p>A Figure A
B Figure B
C Figure C
D Figure D
E Don’t know.
</p>
<p>3.5. Fall 2014 question 4: A principal component
analysis is carried out on a dataset comprised of three
data points x1,x2 and x3 collected in a N ×M matrix
X such that each row of the matrix is a data point. Sup-
pose the matrix X̃ corresponds to X with the mean of
each columns substracted i.e.
</p>
<p>X =
</p>
<p>3.00 2.00 1.004.00 1.00 2.00
0.00 1.00 2.00
</p>
<p> , X̃nm = Xnm − 1
N
</p>
<p>N∑
k=1
</p>
<p>Xkm.
</p>
<p>and suppose X̃ has the singular value decomposition:
</p>
<p>X̃ = UΣV &gt;, U =
</p>
<p>−0.26 0.77 0.58−0.54 −0.61 0.58
0.80 −0.16 0.58
</p>
<p> ,
Σ =
</p>
<p>2.96 0.00 0.000.00 1.10 0.00
0.00 0.00 0.00
</p>
<p> V =
−0.99 −0.13 −0.00−0.09 0.70 −0.71
</p>
<p>0.09 −0.70 −0.71
</p>
<p>
</p>
<p>What is the (rounded to two significant digits) coor-
dinates of the first observation x1 projected onto the 2-
Dimensional subspace containing the maximal variation?
</p>
<p>A
[
−3.06 0.31
</p>
<p>]&gt;
B
[
−0.78 0.85
</p>
<p>]&gt;
C
[
−1.07 0.21
</p>
<p>]&gt;
D
[
−3.16 0.23
</p>
<p>]&gt;
E Don’t know.
</p>
<p>3.6. Spring 2014 question 3: A principal component
analysis is carried out on the wholesale data based on
x1–x6. The mean is subtracted from each attribute and
the singular value decomposition (SVD) is applied to the
data matrix of size 440× 6. From the SVD we obtain for
the matrices S and V :
</p>
<p>S=105 ·
</p>
<p>
2.69 0 0 0 0 0
</p>
<p>0 2.53 0 0 0 0
0 0 1.05 0 0 0
0 0 0 0.83 0 0
0 0 0 0 0.49 0
0 0 0 0 0 0.31
</p>
<p>,
</p>
<p>V =
</p>
<p>
−0.98 −0.11 −0.18 −0.04 0.02 −0.02
−0.12 0.52 0.51 −0.65 0.20 0.03
−0.06 0.76 −0.28 0.38 −0.16 0.41
−0.15 −0.02 0.71 0.65 0.22 −0.01
0.01 0.37 −0.20 0.15 0.21 −0.87
−0.07 0.06 0.28 −0.02 −0.92 −0.27
</p>
<p>.
</p>
<p>We note that both S and V above have been rounded
to the first couple of significant digits. Which one of the
following statements regarding the principal component
analysis is correct?
</p>
<p>A The first principal component accounts for less than
40 % of the variance in the data.
</p>
<p>B The first three principal components account for
more than 95 % of the variance in the data.
</p>
<p>C The last two principal component account for more
than 2 % of the variance in the data.
</p>
<p>D The fourth principal component accounts for more
than 5 % of the variance in the data.
</p>
<p>E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>4
</p>
<p>Summary statistics and measures of similarity
</p>
<p>In this chapter, we will consider more elementary properties and definitions of a dataset. We will
consider ways to summarize (or describe) attributes of the dataset but more importantly ways to
compare observations.
</p>
<p>4.1 Attribute statistics
</p>
<p>When working with variables it is convenient to be able to summarize them using elementary
statistical measures such as the mean and variance. Suppose we record observations x1, . . . , xN of
a particular attribute x, for instance corresponding to the weight of N = 20 schoolchildren. Since
we only have access to a small sample of schoolchildren the average weight of the N observations
will only be an estimate of the true average weight of all schoolchildren and we therefore call the
average computed from the N = 20 schoolchildren, denoted by µ̂, the empirical mean and use the
hat-symbol to signify it is only a “best guess based on the available information”. In this manner
we define the empirical mean, variance and standard deviation as follows:
</p>
<p>Emperical mean of x: µ̂ ≈ 1
N
</p>
<p>N∑
i=1
</p>
<p>xi (4.1)
</p>
<p>Emperical variance of x: ŝ ≈ 1
N − 1
</p>
<p>N∑
i=1
</p>
<p>(xi − µ̂)2 (4.2)
</p>
<p>Emperical standard deviation of x: σ̂ ≈
√
ŝ (4.3)
</p>
<p>Notice that for the estimate of the variance (and therefore also for the standard deviation) we
divided by N − 1 and not N . This is because if we divide by N the estimate of the variance will
be unrealistically small as we have used the data to also estimate the mean value 1. The estimates
eq. (4.2) and eq. (4.3) are therefore called unbiased and for a small sample they are considered
superior to the biased estimators 2:
</p>
<p>1 For completeness it should be mentioned that if N = 1 it is common to set ŝ = σ̂ = 0.
2 Note, if the true mean value µ is provided and not empirically estimated from the data this estimator is
</p>
<p>no longer biased and we should therefore use this estimate based on dividing by N .</p>
<p></p>
</div>, <div class="page"><p></p>
<p>52 4 Summary statistics and measures of similarity
</p>
<p>ŝB ≈
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>(xi − µ̂)2 , σ̂B ≈
√
ŝB .
</p>
<p>The mean value provides important information about a sample, however, it is also affected by
outliers. A way to get around this is the median which corresponds to the value of x such that “half
the observations” are greater than x and “half” are lower; i.e. the value of x that’s “in the middle”
of the dataset. To put this formally, we first sort the values of x, x1, x2, . . . , xN in ascending order,
i.e. as x′1 ≤ x′2 ≤ · · · ≤ x′N . The median is then defined as the value of x such that “half” of the
observations is less than x and “half” of the observations are greater than x. If N is odd this is just
x′(N+1)/2, and if N is even we compute the average of the two middle values:
</p>
<p>Median of x: median[x] =
</p>
<p>{
x′(N+1)/2 if N is odd
</p>
<p>1
2
</p>
<p>(
x′N/2 + x
</p>
<p>′
N/2+1
</p>
<p>)
if N is even.
</p>
<p>(4.4)
</p>
<p>Percentile
</p>
<p>The concept of the median can be generalized to percentiles. The exact definition of percentiles is
somewhat technical, but the concept is easy to understand: Given a sample x, the p’th percentile
is a number xp such that p percent of the observations are less than xp. Consider for instance
N = 200 university students where xi denotes the grade average of student i,s i = 1, . . . , N . The
p = 90%’th percentile is then a value of the grade average xp=90%, for instance xp=90% = 11.7,
such that 180 students have a grade average less than xp=90% and 20 students have a grade average
greater than xp=90%. If we use the notation introduced for the median
</p>
<p>3 we might reasonably expect
xp=90% ≈ x′dNpe, compare this to the definition of median which is obtained when p = 50%. However,
we have to use the approximately equal sign because the definition is slightly ambiguous. If 180
students has a grade average less than 11.7, presumably the same 180 students has a grade average
less than 11.7001 and just as for the median we therefore has to select a reasonable value of xp=90%
somewhere between the grade of student xbNpc and xdNpe. There are different ways to accomplish
this interpolation in practice, all somewhat notationally involved, and the details need not concern
us here4.
</p>
<p>3 The notation dae rounds a upwards to the nearest integer whereas bac rounds a downwards to the nearest
integer. For instance d2.8e = 3 and b2.8c = 2.
</p>
<p>4 Nevertheless, as a punishment to the curious, a popular interpolation method is linear interpolation
defined as follows: Suppose the dataset is sorted as x′1 ≤ x′2 ≤ · · · ≤ x′N . The percentile function for
a percentile p can then be defined by first introducing the “granulated percentiles” pi =
</p>
<p>1
N
</p>
<p>(i − 1
2
) for
</p>
<p>i = 1, . . . , N and the function X(z):
</p>
<p>X(z) = x′bzc + (z − bzc)(x′bz+1c − x′bzc).
</p>
<p>Notice if z is an integer X(z) = x′z. The p’th percentile can then be defined as xp = X(z) where z is
selected as
</p>
<p>z =
</p>
<p>
Np+ 1
</p>
<p>2
if p1 ≤ p ≤ pN
</p>
<p>1 if 0 ≤ p &lt; p1
N if pN &lt; p ≤ 1.
</p>
<p>(4.5)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>4.1 Attribute statistics 53
</p>
<p>ˆcor = 1.00
</p>
<p>−4 −2 0 2 4
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>ˆcor = 0.83
</p>
<p>−4 −2 0 2 4
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>ˆcor = 0.08
</p>
<p>−4 −2 0 2 4
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>ˆcor = −0.85
</p>
<p>−4 −2 0 2 4
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>ˆcor = −1.00
</p>
<p>−4 −2 0 2 4
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>Fig. 4.1: Five two-dimensional datasets and their correlation as estimated from eq. (4.7).
</p>
<p>Mode
</p>
<p>We also define the mode as the most frequently occurring value of x1, . . . , xN . The mode may not
be unique, for instance for the dataset
</p>
<p>1, 2, 2, 4, 4
</p>
<p>both 2 and 4 occur two times. In this case we say both 2 and 4 are the mode of the dataset and
that the dataset is multimodal. For a value of x, we say that the number of times x occur in the
dataset is the frequency of x. In the previous example the frequency of 1 is 1, the frequency of 2
and 4 is 2 and the frequency of 7 is 0.
</p>
<p>4.1.1 Covariance and Correlation
</p>
<p>Covariance measures how much one variable y can be expected to change when another variable x
changes and visa-versa. Suppose we have a dataset containing two attributes x and y with recorded
values x1, x2, . . . , xN and y1, y2, . . . , yN . If we let µ̂x and µ̂y denote the empirical mean of the two
attributes the covariance of attribute x and y can be estimated as
</p>
<p>Empirical covariance of x, y: ˆcov[x, y] =
1
</p>
<p>N − 1
N∑
i=1
</p>
<p>(xi − µ̂x) (yi − µ̂y) . (4.6)
</p>
<p>Notice that cov[x, x] = Var[x]. Given a dataset of M attributes, x1, . . . , xM , we can compute the
pairwise covariance between any two attributes cov[xi, xj ] and collect all these in an M ×M matrix
Σ̃ where Σ̃ij = cov[xi, xj ]. This matrix is known as the covariance matrix. A drawback of the
covariance as a summary statistic is that it is affected by the scale of each attribute. This can be
overcome by standardizing with the empirical standard deviation of the two attributes, σ̂x and σ̂y,
leading to the correlation of x and y:
</p>
<p>Empirical correlation of x, y: ˆcor[x, y] =
ˆcov[x, y]
</p>
<p>σ̂xσ̂y
=
</p>
<p>1
</p>
<p>N − 1
N∑
i=1
</p>
<p>(xi − µ̂x) (yi − µ̂y)
σ̂xσ̂y
</p>
<p>. (4.7)
</p>
<p>Correlation tells us how (linearly) related attributes are. A correlation of 0 means that x tells us
nothing about y, a positive correlation tells us that when x is large y is also likely to be large and
a negative correlation tells us that if x is large y will typically be small.
</p>
<p>An instructive example is if we assume y = ax + b. In this case µ̂y = aµ̂x + b and σ̂y = |a|σ̂x.
We then obtain:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>54 4 Summary statistics and measures of similarity
</p>
<p>Table 4.1: A term document matrix corresponding to three lines from the tale
</p>
<p>assembl beauti court courtyard father gave hors king mount princess servant watch win woo
</p>
<p>0 1 0 0 1 1 1 0 0 1 0 0 1 0
0 0 1 0 0 0 0 1 0 1 0 0 0 1
1 0 0 1 0 0 1 0 1 0 1 1 0 0
</p>
<p>ˆcor[x, y] =
1
</p>
<p>N−1
∑N
i=1 (xi − µ̂x) (axi + b− (aµ̂x + b))
</p>
<p>σ̂xσ̂y
</p>
<p>=
1
</p>
<p>N−1
∑N
i=1 a (xi − µ̂x) (xi − µ̂x)
|a|σ̂xσ̂x
</p>
<p>= sign(a)
</p>
<p>So the correlation is −1 if a &lt; 0 and 1 if a &gt; 0. See fig. 4.1 for further examples.
</p>
<p>4.2 Term-document matrix
</p>
<p>Suppose we have a collection of documents (for instance news stories) and we wish to analyse them
using machine learning. A problem is that the news stories will contain a different number of words
and so they cannot naturally be represented as an N ×M matrix. A way to overcome this is to
represent the documents as what is known as the term-document matrix. Suppose as an example
we consider three “documents” corresponding to three lines from the tale Clumsy Hans by H.C.
Andersen
</p>
<p>d1 =
</p>
<p>{
”I shall win the Princess!” they both said, as their
father gave each one of them a beautiful horse.
</p>
<p>}
d2 = {”To the King’s court, to woo the Princess.}
</p>
<p>d3 =
</p>
<p>{
All the servants assembled in the courtyard to watch
them mount their horses,
</p>
<p>}
In the term-document representation, we count the number of times each word in our vocabulary
</p>
<p>occurs in the documents. Each document can then be represented as a vector (of the same length
as our vocabulary) where most of the entries are zero. In addition, it is common to remove words
that are very common such as an or the (these are called stop words) as well as remove the tense
of the words by removing the last letters (called stemming). For instance horse and horses are
considered to count as the same stem hors. Doing this we obtain the table seen in table 4.1 where
each row correspond to d1, d2 and d3. This table can then be considered as the dataset matrix X
(i.e. N corresponds to the number of documents and M , the number of features, to the number of
word-stems) and is known as the term-document matrix 5. For instance the first column of X is[
0 0 1
</p>
<p>]T
because only the last document contains the word “assembled”.
</p>
<p>5 The reader may wonder why it is not called the document-term matrix when it has dimensions documents
× terms. This is because it is common in text analysis represents documents as the transpose of X, and
we have decided to re-use the terminology.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>4.3 Measures of distance 55
</p>
<p>4.3 Measures of distance
</p>
<p>The concept of distance and similarity play a crucial role in machine learning. Suppose we have
to determine if an image contains a picture of a cat or a dog. One way to phrase this problem is
that we have to compare the image to what a cat ought to look like and what a dog ought to look
like and determine which of the two the image is the most similar to. A computer learning method
will often do something similar, and for that reason studying measure of distance and similarity
more explicitly is useful. No simple definition exist for what a distance measure is except it is some
function of two observations x,y such that the value is large when they are very dissimilar and
small when they are very similar, however we are usually interested in measures of distance d which
obey the following rules:
</p>
<p>non-negativity d(x,y) ≥ 0, (4.8)
identity of indiscernibles d(x,y) = 0 if and only if x = y, (4.9)
</p>
<p>symmetry d(x,y) = d(y,x), (4.10)
</p>
<p>triangle inequality d(x,y) ≤ d(x, z) + d(z,y). (4.11)
</p>
<p>For instance, the triangle inequality is saying the distance from home to the workplace is not
greater than the distance from the workplace to the baker and from the baker to home. A measure
of distance which obey the above rules is called a metric. A common way to define distances is as
the magnitude of the difference6 of observations, x − y which, in a vector space, is called a norm
and is denoted by ‖x‖. It must in turn obey:
</p>
<p>non-negativity ‖x‖ &gt; 0 if x 6= 0, (4.12)
scaling ‖ax‖ = |a|‖x‖ (4.13)
</p>
<p>triangle inequality ‖x+ y‖ ≤ ‖x‖+ ‖y‖. (4.14)
</p>
<p>Then we can define the distance from the norm as
</p>
<p>d(x,y) = ‖x− y‖. (4.15)
</p>
<p>No doubt, the most familiar norm is the Euclidian norm. Given a vector x it is defined as
</p>
<p>‖x‖ =
√
x21 + x
</p>
<p>2
2 + · · ·+ x2M . (4.16)
</p>
<p>More generally, we have the Lp norm (or simply p-norm) which, for any number p ≥ 1 is defined
as:
</p>
<p>‖x‖p = (|x1|p + |x2|p + · · ·+ |xM |p)
1
p . (4.17)
</p>
<p>It is common to extend this definition to p =∞ by the definition:
6 Naturally, this assume we can meaningfully subtract the two observations x, y from each other without
</p>
<p>getting into trouble. As an example of a way to get into troubles is if an attribute is nominal, such as
the Origin-attribute from the Cars-dataset we encountered in chapter 2, in which case the particular
(arbitrary) numeric encoding of the countries as integers should not affect their difference. The remedy
is ofcourse to apply a 1-of-K encoding to this attribute.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>56 4 Summary statistics and measures of similarity
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 4.2: Illustration of the p-distance for various values of p. A point x is colored red if it’s p distance
to the center 0 is less than 1, dp(x,0) ≤ 1. Top row: p = 12 , 1, 32 and bottom row: p = 2, 3,∞. The
red line is the decision boundary dp(x,0) = 1. Increasing p corresponds to “inflating” the red region.
</p>
<p>‖x‖∞ = max{|x1|, |x2|, . . . , |xM |}. (4.18)
</p>
<p>Based on the norm, we can define the p-distance as
</p>
<p>dp(x,y) = ‖x− y‖p =
</p>
<p>
(∑M
</p>
<p>i=1 |xi − yi|p
) 1
p
</p>
<p>if 1 ≤ p &lt;∞
max{|x1 − y1|, |x2 − y2|, . . . , |xM − yM |} if p =∞.
</p>
<p>Note in the particular case of the p = ∞ norm, the distances measures the largest difference in
coordinates.
</p>
<p>In fig. 4.2 we have plotted a large number of observations and colored those red which have a
p-distance less than 1 to (0, 0), i.e. dp(x,0) ≤ 1, for 6 different values of p. Note we have included
p = 12 for completeness (see also technical note 4.3.1).
</p>
<p>Finally, we should also mention the Fröbenius norm which we encountered earlier in chapter 3:
</p>
<p>‖X‖F =
</p>
<p>√√√√ N∑
i=1
</p>
<p>M∑
j=1
</p>
<p>X2ij =
</p>
<p>√
trace(XTX). (4.19)
</p>
<p>A useful way to think of the Fröbenius norm is as measuring the magnitude of the entire dataset:
‖X‖F =
</p>
<p>∑N
i=1 ‖xi‖2.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>4.4 Measures of similarity 57
</p>
<p>Technical note 4.3.1: The case p &lt; 1 of the p-norm
</p>
<p>For technical reasons the p &lt; 1 case is more difficult. In general, we define the p-distance
when 0 &lt; p &lt; 1 as:
</p>
<p>dp(x,y) = |x1 − y1|p + |x2 − y2|p + · · ·+ |xM − yM |p, (4.20)
</p>
<p>and for the particular case p = 0 it is common to define 00 = 0 and then call the function
</p>
<p>‖x‖0 = |x1|0 + |x2|0 + · · ·+ |xM |0, (4.21)
</p>
<p>which counts the number of non-zero coordinates of x the p = 0 norm. Note from a math-
ematical standpoint this is a misnomer since it does not obey the mathematical properties
of a norm.
</p>
<p>4.3.1 The Mahalanobis Distance
</p>
<p>Suppose we are given a covariance matrix Σ, for instance estimated from a dataset as in eq. (4.6).
We can then define the Mahalanobis distance as
</p>
<p>dM (x,y) =
</p>
<p>√
(x− y)TΣ−1(x− y)
</p>
<p>Notice, if Σ = I the Mahalanobis distance reduces to the Euclidian distance: dM (x,y) =√
(x− y)T I(x− y) = ‖x − y‖2. If Σ is estimated from a dataset as in eq. (4.6), what the corre-
</p>
<p>sponding Mahalanobis distance takes into account is (very roughly said) that the distance between
two points should be lower when the points lie within the point cloud of the dataset. For instance
in fig. 4.3 the distance between the two red points according to the Mahalanobis distance is 13 but
only 4.15 between the blue points, however in both cases the Euclidian distance is roughly 5.65.
</p>
<p>4.4 Measures of similarity
</p>
<p>A measure of similarity is a function which takes two observations x,y as input and is large when x
is very similar to y. Obviously, a measure of similarity can be constructed from a distance measure
by a simple mapping. The most simple way is to define s(x,y) = −d(x,y), however, often it is
desirable to have a measure of similarity on a scale that goes from 0 to 1 and so one could choose:
</p>
<p>s(x,y) =
a
</p>
<p>d(x,y) + a
,
</p>
<p>for a constant a &gt; 0. Defining measures of similarity from distance is arguably a bit silly, but
for some types of observations, measures of similarity may be easier to define than measure of
dissimilarity. Consider the important situation where x is binary, i.e. xi = 0, 1 for all i. Suppose we
define
</p>
<p>f11 = {Number of entries i where xi = 1 and yi = 1} ,
f10 = {Number of entries i where xi = 1 and yi = 0} ,
f01 = {Number of entries i where xi = 0 and yi = 1} ,
f00 = {Number of entries i where xi = 0 and yi = 0} .</p>
<p></p>
</div>, <div class="page"><p></p>
<p>58 4 Summary statistics and measures of similarity
</p>
<p>-2 0 2 4
</p>
<p>-3
</p>
<p>-2
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>Fig. 4.3: A simple 2D dataset to illustrate the Mahalanobis distance. If we estimate the covariance
matrix from the dataset, the Mahalanobis distance between the red points is 13 but only 4.15
between the blue points.
</p>
<p>Then notice M = f11 + f10 + f01 + f00 and we can then define the following measures:
</p>
<p>Simple Matching Coefficient SMC(x,y) =
f11 + f00
</p>
<p>M
(4.22)
</p>
<p>Jaccard Similarity J(x,y) =
f11
</p>
<p>f11 + f10 + f01
(4.23)
</p>
<p>Cosine similarity cos(x,y) =
f11
‖x‖‖y‖ (4.24)
</p>
<p>For general vectors x,y the cosine similarity and the extended Jaccard similarity can also be defined
as:
</p>
<p>Extended Jaccard Similarity EJ(x,y) =
xTy
</p>
<p>‖x‖2 + ‖y‖2 − xTy (4.25)
</p>
<p>Cosine similarity cos(x,y) =
xTy
</p>
<p>‖x‖‖y‖ (4.26)
</p>
<p>Why do we need three different measures of similarity? It is useful to consider the qualitative
difference between these measures of similarity in context of the term-document example of table 4.1.
We first notice the SMC makes no difference between 0 and 1; i.e. if we flip the zeros and ones it
makes no difference to the similarity: SMC(1−x, 1−y) = SMC(x,y). This is useful in some cases,
for instance if the zeros and ones arise only by convention, say, we record a “zero” if a patient is a
male and “one” if the patient is a female (or visa-versa), and we do not want our machine-learning
method to be influenced by this rather arbitrary choice.
</p>
<p>However, for some datasets what is a zero and what is a one has an assymetric meaning. Consider
the term-document example. For documents there will typically be many more 0s than 1s since a
document only use a fraction of the vocabulary and so, since the 0s are counted as “matches”,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>4.4 Measures of similarity 59
</p>
<p>the SMC will be large even for documents that have nothing in common, simply because they
don’t contain many of the same words: According to the SMC a recipe for ice-cream and the
US constitution is quite similar since they don’t use words like Armadillo, lumberjack or vacuum
cleaner.
</p>
<p>The Jaccard and cosine similarity gets around these problems by focusing on positive matches
(i.e. words the documents do have in common), however with an important twist: Suppose we
compare two documents x and y which are on the same topic, but x is much longer than y. In this
case x will (likely) contain many words not found in y simply because the author wrote more text
for document x, and the Jaccard similarity will pick up on this and believe the documents are quite
dissimilar, which may not be desirable. If we normalizing by the document length, as is done in the
Cosine similarity, this will somewhat correct for this problem and is therefore more suitable if some
vectors has much larger magnitude (in the binary case more 1s) than others and this difference is
not in itself considered very informative.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>60 4 Summary statistics and measures of similarity
</p>
<p>Problems
</p>
<p>4.1. Fall 2014 question 10:
In table 4.2 is given the pairwise cityblock distances
</p>
<p>between 8 observations along with a description of the
dataset. What can be concluded about the similarity of
observation o1 and o3?
</p>
<p>o1 o2 o3 o4 o5 o6 o7 o8
o1 0 4 7 9 5 5 5 6
o2 4 0 7 7 7 3 7 8
o3 7 7 0 10 6 6 4 9
o4 9 7 10 0 8 6 10 9
o5 5 7 6 8 0 8 6 7
o6 5 3 6 6 8 0 8 11
o7 5 7 4 10 6 8 0 7
o8 6 8 9 9 7 11 7 0
</p>
<p>Table 4.2: Pairwise Cityblock distance, i.e d(oi, oi) =
</p>
<p>‖xi − xj‖1 =
∑M
k=1 |xik − xjk|, between 8 observations.
</p>
<p>Each observation oi corresponds to a M = 15 dimen-
sional binary vector, xik ∈ {0, 1}. The blue observations
{o1, o2, o3, o4} belong to class C1 and the black observa-
tions {o5, o6, o7, o8} belong to class C2.
</p>
<p>A COS(o1, o3) = 0.533
B J(o1, o3) = 0.533
C SMC(o1, o3) = 0.533
D There is insufficient information to draw specific con-
</p>
<p>clusions.
E Don’t know.
</p>
<p>4.2. Spring 2013 question 18: We will let J(A,B),
SMC(A,B), and cos(A,B) denote the Jaccard Coeffi-
cient, Simple Matching Coefficient and Cosine Similar-
ity respectively between observation A and B. We will
consider the data in Table 4.3 containing 10 observa-
tions denoted NS1, NS2, NS3, NS4, NS5, AS1, AS2, AS3,
AS4, and AS5 such that the first observation is given
by NS1= {1, 0, 0, 1, 0, 1, 1, 0}. Which one of the following
statements is correct?
</p>
<p>CDY CDN ASTY ASTN SIY SIN HFY HFN
</p>
<p>NS1 1 0 0 1 0 1 1 0
NS2 0 1 1 0 1 0 1 0
NS3 1 0 0 1 0 1 1 0
NS4 0 1 1 0 0 1 1 0
NS5 1 0 1 0 1 0 1 0
AS1 0 1 1 0 0 1 1 0
AS2 0 1 1 0 0 1 1 0
AS3 0 1 1 0 0 1 1 0
AS4 0 1 0 1 1 0 0 1
AS5 1 0 1 0 0 1 1 0
</p>
<p>Table 4.3: Given are the first five subjects with normal
semen (denoted NS1, NS2, . . ., NS5) as well as the first
five subjects with abnormal semen (denoted AS1, AS2,
. . ., AS5) including whether these subjects have had a
childhood disease or not (CDY , CDN ), accident or seri-
ous trauma or not (ASTY , ASTN ), serious injury or not
(SIY , SIN ), and high fever or not (HFY , HFN ).
</p>
<p>A J(NS1,NS2) = SMC(NS1,NS2)
B cos(NS4,NS5) = 18
C J(NS5,AS5) = SMC(NS5,AS5)
D cos(NS5,AS5) = 34
E Don’t know.
</p>
<p>4.3. Fall 2013 question 18: We will let J(A,B),
SMC(A,B), and cos(A,B) denote the Jaccard Coef-
ficient, Simple Matching Coefficient and Cosine Simi-
larity respectively between observation A and B. We
will consider the data in Table 4.4 containing 10 ob-
servations denoted S1, S2, S3, S4, S5, NS1, NS2, NS3,
NS4, and NS5 such that the first observation is given
by S1= {1, 0, 1, 0, 1, 0}. Which one of the following state-
ments is correct?
</p>
<p>Y AY Y AN OAY OAN PAY PAN
S1 1 0 1 0 1 0
S2 1 0 1 0 0 1
S3 0 1 0 1 1 0
S4 0 1 1 0 1 0
S5 0 1 1 0 1 0
</p>
<p>NS1 0 1 1 0 1 0
NS2 0 1 0 1 1 0
NS3 1 0 0 1 0 1
NS4 0 1 1 0 1 0
NS5 0 1 1 0 1 0
</p>
<p>Table 4.4: Given are five subjects that survived in Haber-
man’s study (denoted S1, S2, . . ., S5) as well as the five
subjects that did not survive in Haberman’s study (de-
noted NS1, NS2, . . ., NS5) including whether these sub-
jects are young or old (Y AY , Y AN ), were operated af-
ter 1960 or not (OAY , OAN ), and had positive axillary
nodes or not (PAY , PAN ).
</p>
<p>A Using the Jaccard coefficient S1 is more similar to S2
than to NS1, i.e. J(S1, S2) &gt; J(S1, NS1).
</p>
<p>B Using the Simple Matching coefficient S1 is more
similar to S2 than to NS1, i.e. SMC(S1, S2) &gt;
SMC(S1, NS1).
</p>
<p>C The Jaccard coefficient between S1 and S2 is identi-
cal to the Cosine Similarity between S1 and S2, i.e.
J(S1, S2) = cos(S1, S2).
</p>
<p>D The Simple Matching coefficient between S1 and S2
is identical to the Cosine Similarity between S1 and
S2, i.e. SMC(S1, S2) = cos(S1, S2).
</p>
<p>E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5
</p>
<p>Discrete probabilities and information
</p>
<p>Correct reasoning is central to many areas of intellectual activity, including philosophy (how ought
we reason?), cognitive science (how do we reason?), science (what does reason tell us about theories
given experimental evidence?), and artificial intelligence (how do we build reasoning machines?).
</p>
<p>Probability theory has something to say about all these subjects, however, it is particularly
relevant for machine learning for two reasons:
</p>
<p>• All modern approaches to machine learning makes use of probabilities as will all chapters of
this book.
• Probabilities have something fundamental to say about reasoning under uncertainty. In certain
</p>
<p>important situations, if we want to build an optimal reasoning machine, it ought to reason
according to probability theory.
</p>
<p>The reader no doubt has some familiarity with probability theory, however it is our experi-
ence it is the single subject which causes the most difficulties. We will therefore provide a fairly
detailed introduction to probabilities and probabilistic concepts, with a focus on probabilities as
plausible reasoning. The reader should note we will introduce probabilities as the probability of
true/false statements (i.e., the probability it will rain tomorrow), rather than statements about sets
and stochastic variables, which is customary in statistics. This perspective might be a bit different
from how probabilities are usually introduced in statistics (usually, based on set theory), but we
believe this approach is both closer to how we think about probabilities informally, notationally
simpler, and not less formally correct.
</p>
<p>bibliographical remarks
</p>
<p>Bayes’ theorem was first described by Thomas Bayes who considered a problem very akin to the
binomial distribution example we will consider in 6.4 but his work was only published after his death
in 1763 [Bayes and Price, 1763], and the subject was significantly expanded by other early pioneers
such as Pierre-Simon Laplace and many others. The approach to probability theory, including the
interpretation as quantifying rational thought, was revitalized in the first half of the 20th century
by Bruno de Finetti [De Finetti, 1937, Barlow, 1992], Harold Jeffreys [Jeffreys, 1939] and Richard T.
Cox [Cox, 1946]. Information theory, which will be covered in the last section, is due to the seminal
work by Claude Shannon in 1948 [Shannon, 1948]. The normalized version of mutual information,
which builds onto Shannon’s work, is due to Strehl and Ghosh [2002].</p>
<p></p>
</div>, <div class="page"><p></p>
<p>62 5 Discrete probabilities and information
</p>
<p>Input Output
</p>
<p>Fig. 5.1: The robot is given a query in the form A|B (assume B is true, how plausible is A), the
robot then reason about the answer, and outputs the plausibility of A given B.
</p>
<p>5.1 Probability basics
</p>
<p>An important part of what and intelligent robot should do is to reason correctly in light of evidence.
Now, reasoning can mean many things, but the specific sense we are interested in is something akin
to the legal or scientific sense, in which multiple pieces of evidence are weighted so as to determine
the plausibility of a conclusion. In a legal context, the proposition we might be interested in could
be:
</p>
<p>G : The accused is guilty. (5.1a)
</p>
<p>E1 : A car similar to his was seen at the crime scene. (5.1b)
</p>
<p>E2 : His mom says he was home on the night. (5.1c)
</p>
<p>E3 : A large sum of money was found in his posession. (5.1d)
</p>
<p>E4 : His fingerprints was found at the door of the bank. (5.1e)
</p>
<p>If the robot was on the jury, we would ask it to determine the truth of G in light of the evidence
E1, . . . , E4. That is, the robot should assume E1, . . . , E4 are true, and based on this form a belief
about whether G is true or false1 , see fig. 5.1. We can even simplify this query by defining a new
proposition E as:
</p>
<p>E ≡ E1 and E2 and E3 and E4 (5.2)
</p>
<p>Since E is true only when E1, . . . , E4 are true we can simply ask the robot about G in light of E.
In fact, we will assume all inputs to the robot is of the form:
</p>
<p>A|B : Determine the plausiblity of A assuming B is true
</p>
<p>where it is assumed A and B can be any proposition which is either true or false. For instance, in
fig. 5.1, our query to the robot would be of the form G|E.
</p>
<p>1 It is assumed there exists a more complete description of the symbols G, E1, etc.. For instance, G would
refer to a particular bank heist, etc. etc.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.1 Probability basics 63
</p>
<p>5.1.1 A primer on binary propositionsF
</p>
<p>Since all queries to the robot are binary true/false propositions (sometimes also called Boolean
propositions), it is useful to introduce some basic notation governing these. In the following sections,
upper-case Latin letters A, B, C, etc. will denote binary propositions, i.e. statements that are either
true or false. We will introduce the following shorthand to represent the operations not/and/or:
</p>
<p>Negation: A (logical not; true if A is false)
</p>
<p>Conjunction: AB (logical and; True if A and B are both true)
</p>
<p>Disjunction: A+B (logical or; True if A or B is true)
</p>
<p>This notation allow us to write E from eq. (5.2) symbolically
</p>
<p>E = E1E2E3E4.
</p>
<p>The notation allows us to conveniently query the robot about hypothetical situations. For instance,
if the search on the suspects apartment had not turned up money, the evidence would be written
as E′ = E1E2E3E4 and we would make the query G|E′.
</p>
<p>In addition to these operations, we will also define the following two special propositions:2
</p>
<p>1 : A proposition which is always true (5.3a)
</p>
<p>0 : A proposition which is always false (5.3b)
</p>
<p>The following identities should be intuitively obvious:
</p>
<p>A1 = A, A+A = 1, A = A
</p>
<p>In addition, we have the distributive rule:
</p>
<p>A(B1 +B2 + · · ·+Bn) = AB1 +AB2 + · · ·+ABn
</p>
<p>It is easy to verify this is the case. If for instance the left-hand side is true, then both A and at
least one of the Bi’s must be true, but then the right-hand side is also true. The following identity
will also be useful:
</p>
<p>A+B = A B.
</p>
<p>It is easy to verify this by considering the different possibilities for A and B and observe both
sides of the equality sign agree in all four case. If, for instance, both A and B are false then:
</p>
<p>A+B = 0 + 0 = 0 and A B = 0 0 = 1 = 0.
</p>
<p>5.1.2 Probabilities and plausibility
</p>
<p>Continuing the trial example, we cannot logically deduce G is true based on the evidence E (after all,
there might be an innocent explanation of all the evidence), however certainly the evidence increases
</p>
<p>2 Note we could have used other symbols than 0 and 1, for instance K0 and K1. If these two propositions
cause the reader any issues, he or she can mentally substitute them for propositions which are indeed
always true or false, for instance 2 + 2 = 4 and 1 + 1 = 4.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>64 5 Discrete probabilities and information
</p>
<p>our confidence G is true. It is exactly this degree-of-belief in one proposition given another we model
based on probabilities. Specifically, we denote by the number
</p>
<p>P (G|E1E2E3E4)
</p>
<p>the degree-of-belief G is true given E1, E2, E3, and E4 are true. If we leave aside the issue of
how we compute the number, we can at least represent statements such as the fingerprints are
incriminating :
</p>
<p>P (G|E1E2E3E4) &gt; P (G|E1E2E3).
The degree of belief is a number between 0 and 1, with 0 and 1 representing certainty
</p>
<p>P (A|B) = 0 (interpretation: given B is true, A is certainly false)
P (A|B) = 1 (interpretation: given B is true, A is certainly true)
</p>
<p>along with the convention that the plausibility of something which is absolutely certain is 1:
P (1|A) = 1.
</p>
<p>It is worth stressing the symbol P (A|B) represents a state of knowledge of the agent, and to
be very careful symbols are not dropped. To take the guilty-example, the fact the suspects mom
provides an alibi counts against him being guilty. However, notice that:
</p>
<p>P (G|E1E2) &lt; P (G|E1) &lt; P (G|E1E2) (5.4)
</p>
<p>Why is this? It should be obvious the first number is smaller than the last, exactly because in the
first case we know his mom provide an alibi (E2), whereas in the last we know the mom did not
provide an alibi (E2), and all being equal an alibi is exonerating; in the case of the middle-most
number, the mom might or might not have provided an alibi, but the information is not specified
in the query. We stress the point is not that eq. (5.4) is a rule which is always true (it is not), but
rather that the reader take great care in following the rules when later manipulating probabilistic
statements.
</p>
<p>Technical note 5.1.1: Are probabilities and plausibility really the same?
</p>
<p>A reader might at this point have two concerns: firstly, that by saying probabilities represent
plausibility, we are abusing the meaning of probability and giving it a non-scientific meaning.
Secondly, Why should it be exactly probabilities that quantify plausibility? Couldn’t it be
some other mathematical theory? We offer the following points:
</p>
<p>• Isn’t it just true? When we speak about probability, it seems like we are making state-
ments about how plausible certain statements are given what we know. For instance: “It
is very probable/plausible Manchester United will not win Champions League 2024”.
• What is the alternative? The most common alternative interpretation of what a proba-
</p>
<p>bility is, is the number of occurrences divided by the number of repeats of an experiment.
But in the case of Manchester United, there is only a single, future event. Which two
numbers are we supposed to divide? [Hájek, 1997, 2009]
• More importantly, it can be shown that if we make certain assumptions about what
</p>
<p>plausible reasoning should obey, only probability theory can implement those assump-
tions [Cox, 1946, Jaynes, 2003].</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.1 Probability basics 65
</p>
<p>5.1.3 Basic rules of probability
</p>
<p>To summarize the rules of probability: A probability is always written in the form P (A|B) where A
and B can be any binary propositions, as long as B 6= 0. A probability is always a number between
0 and 1, which measures our degree-of-belief in A if we assume B is true. Finally, probabilities
always obey the following two rules:
</p>
<p>The sum rule: P (A|C) + P (A|C) = 1 (5.5a)
The product rule: P (AB|C) = P (B|AC)P (A|C) (5.5b)
</p>
<p>Where A, B and C can be any three propositions.
Note in particular we could select C as the logical constant true, C = 1. In that case, C has no
</p>
<p>bearing on the truth of A and B; after all, we always know the true constant is true, and we will
use the shorthand P (A|1) = P (A). We therefore have as a special case:
</p>
<p>P (A) + P (A) = 1, P (AB) = P (B|A)P (A).
</p>
<p>This is quite remarkable: Reasoning under uncertainty, and most of what we will learn about
machine learning in this course, will come down to these two simple rules applied in different ways,
and we invite the reader to vigilantly observe if we hold good on this promise.
</p>
<p>5.1.4 Marginalization and Bayes’ theorem
</p>
<p>We will begin by deriving Bayes’ theorem as an example of how non-trivial results can be obtained
from just the sum and product rule. First, with some creative application of the sum and product
rule we obtain:
</p>
<p>P (B|C) = P (B|C)
[
P (A|BC) + P (A|BC)
</p>
<p>]
= P (AB|C) + P (AB|C)
</p>
<p>= P (B|AC)P (A|C) + P (B|AC)P (A|C).
</p>
<p>Next, if we then apply the product rule twice to P (AB|C) we obtain
</p>
<p>The product rule: P (AB|C) = P (B|AC)P (A|C)
The product rule again: P (AB|C) = P (A|BC)P (B|C).
</p>
<p>Observing the two right-hand expressions are equal and dividing both sides with P (B|C) we obtain
our first non-trivial result:
</p>
<p>Bayes’ theorem: P (A|BC) = P (B|AC)P (A|C)
P (B|C)
</p>
<p>=
P (B|AC)P (A|C)
</p>
<p>P (B|AC)P (A|C) + P (B|AC)P (A|C) .
</p>
<p>Example 1: The taxicab accident
</p>
<p>So why is Bayes’ theorem so useful? Consider the following example due to Kahneman et al. [1982]:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>66 5 Discrete probabilities and information
</p>
<p>Output
</p>
<p>Fig. 5.2: Example of how our robot in fig. 5.1 might reason about the hit-and-run incident. First,
the available information is transformed into probabilities and then, the query is expressed using
the known probabilities and an answer is computed
</p>
<p>Example 5.1.1: The taxicab accident
</p>
<p>A cab was involved in a hit and run accident at night. Two cab companies, the Green and
the Blue, operate in the city. You are given the following data:
</p>
<p>• 85% of the cabs in the city are Green and 15% are Blue.
• A witness identified the cab as Blue. The court tested the reliability of the witness under
</p>
<p>the same circumstances that existed on the night of the accident and concluded that the
witness correctly identified each one of the two colors 80% of the time and failed 20% of
the time.
</p>
<p>What is the probability that the cab involved in the accident was Blue rather than Green?
</p>
<p>To solve the problem, we define the two binary propositions:
</p>
<p>B : The delinquent was a Blue cab.
</p>
<p>W : The Witness reported the car was blue.
</p>
<p>Since cabs can only be green and blue, B is the event the cab is green. We are interested in
computing the probability the cab was Blue given the witness said it was blue P (B|W ). We first
assign these numerical values using the information in the text (see also left-most pane of fig. 5.2),
then we use the rules of probability to write an expression for the probability we are interested in
(here, Bayes theorem, see middle pane of fig. 5.2), and finally compute an answer:
</p>
<p>P (B|W ) = P (W |B)P (B)
P (W |B)P (B) + P (W |B)P (B) (5.6)
</p>
<p>=
0.8× 0.15
</p>
<p>0.8× 0.15 + 0.2× 0.85 ≈ 41% (5.7)
</p>
<p>So despite the witness testimony, the hit-and-run cab is more likely to be Green than Blue, i.e.
P (B|W ) = 1− P (B|W ) = 0.59.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.1 Probability basics 67
</p>
<p>5.1.5 Mutually exclusive events
</p>
<p>Probability as considered so far is only defined for binary events, however we can easily expand
our notation to cover events with more than one outcome just using the basic rules of probability
theory. Suppose we roll an ordinary die. There are then six possible outcomes corresponding to the
six events
</p>
<p>A1 : The side face up. A2 : The side face up.
</p>
<p>A3 : The side face up. A4 : The side face up.
</p>
<p>A5 : The side face up. A6 : The side face up.
</p>
<p>(5.8)
</p>
<p>Since a die can only show one face up at a time, no two of these propositions can be true at the
same time, and they are said to be mutually exclusive. This means that for any values of i and j:
</p>
<p>AiAj =
</p>
<p>{
Ai if i = j
</p>
<p>0 if i 6= j
</p>
<p>In general, assuming n events A1, . . . , An are mutually exclusive, one can show the following gen-
eralization of the sum rule (see technical note 5.1.2):
</p>
<p>P (A1 +A2 + · · ·+An|C) = P (A1|C) + P (A2|C) + · · ·+ P (An|C). (5.9)
</p>
<p>As a special case, consider the case the events are also exhaustive, meaning that i.e. A1 + · · ·+
An = 1. This is the case for the die where we know one of the six propositions A1, . . . , A6 has to
be true because one side must be facing up. In this case the left-hand side of eq. (5.9) is equal to
P (1|C) = 1 and therefore:
</p>
<p>1 = P (A1|C) + P (A2|C) + · · ·+ P (An|C). (5.10)
</p>
<p>Furthermore, if A1, . . . , An are both mutually exclusive and exhaustive then, for any B and C:
</p>
<p>P (B|C) = P (B|C) · 1 = P (B|C)
[
</p>
<p>n∑
i=1
</p>
<p>P (Ai|BC)
]
</p>
<p>=
</p>
<p>n∑
i=1
</p>
<p>P (Ai|BC)P (B|C)
</p>
<p>=
</p>
<p>n∑
i=1
</p>
<p>P (BAi|C) =
n∑
i=1
</p>
<p>P (B|AiC)P (Ai|C). (5.11)
</p>
<p>This general procedure will be used many times in the following and is known as marginalization.
Our first use of marginalization will be to generalize Bayes theorem to many events: Suppose
A1, . . . , An is a set of mutually exclusive and exhaustive hypothesis and B is some piece of evidence,
we then have
</p>
<p>P (Ai|B) =
P (B|Ai)P (Ai)
</p>
<p>P (B)
=
</p>
<p>P (B|Ai)P (Ai)∑n
j=1 P (B|Aj)P (Aj)
</p>
<p>.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>68 5 Discrete probabilities and information
</p>
<p>Technical note 5.1.2: Derivation of the sum rule for multiple events
</p>
<p>Suppose A and B are two mutually exclusive events, i.e. both cannot happen at the same
time. Consider the event A or B occurs written as A+B. Recall this can be written as:
</p>
<p>A+B = A B
</p>
<p>thus, using the sum rule, P (A+B) = P (A B) = 1− P (A B). We can then compute
</p>
<p>P (A+B) = 1− P (A B) =1− P (A|B)P (B)
= 1−
</p>
<p>[
1− P (A|B)
</p>
<p>]
P (B) =P (B) + P (AB)
</p>
<p>= P (B) + P (B|A)P (A) =P (B) + [1− P (B|A)]P (A)
= P (A) + P (B)− P (AB). (5.12)
</p>
<p>In the case of the die this gives P (A1 +A2) = P (A1)+P (A2)−P (A1A2), however, since the
same die cannot show two faces up at once we know P (A1A2) = P (0) = 0 and this simplifies
to
</p>
<p>P (A1 +A2) = P (A1) + P (A2).
</p>
<p>Repeated applications of this result show that for n mutually exclusive events we get eq. (5.9)
</p>
<p>P (A1 +A2 + · · ·+An|C) = P (A1|C) + P (A2|C) + · · ·+ P (An|C).
</p>
<p>5.1.6 Equally likely events
</p>
<p>We have purposefully introduced probabilities without reference to specific numbers, exactly to
make it clear our two rules of probability are true regardless of what the propositions refer to or
what their specific values are. That being said, we obviously need a way to assign numerical values
to probabilities, which is what we will address here.
</p>
<p>There is a tendency, which we warn against, to think familiar counting rules such as
</p>
<p>P (Positive) =
{Number of positive cases}
{Number of trials} (5.13)
</p>
<p>has a fundamental status in probability theory. Obviously, we don’t want to say these rules are
false, but a rule such as the above is true only because we can somehow demonstrate it is true.
Doing that has the benefit of giving us a clear idea of when it is applicable, and when it is not.
</p>
<p>Let’s begin with a simple case, namely the die from the previous section. Now, it no doubt seems
intuitively obvious that the chance side or comes up is:
</p>
<p>P (A4) = P (A3) =
1
</p>
<p>6
</p>
<p>but why is this true? Let us carefully go through the steps involved:
</p>
<p>• We don’t have any information about the die, or how it was rolled, which makes it more plausible
A4 will occur than A3 or visa versa</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.1 Probability basics 69
</p>
<p>• As no information makes us prefer A4 over A3 (or visa-versa), they are equally plausible. As
probability measures plausibility, we conclude P (A4) = P (A3)
• Re-doing this argument for all pairs we conclude P (Ai) = P (Aj) for all i, j.
• Since the events are mutually exclusive, we know from eq. (5.10) that
</p>
<p>1 = P (A1) + · · ·+ P (A6) = 6P (A1)
</p>
<p>• Therefore, P (A1) = 16 , and P (Ai) = 16 for all i since they are equally plausible
Based on this example, we can conclude that in an experiment with N mutually exclusive outcomes,
where we have no information that makes us prefer one outcome over another, the outcomes have
probability 1N .
</p>
<p>Let us consider a slightly more elaborate example. Suppose we ask the probability the next roll
of the die will be a prime. If we denote this event by R, we see it can be written as:
</p>
<p>R = {Next roll is prime} = {Next roll is 2, 3 or 5} = A2 +A3 +A5
</p>
<p>Since these events are mutually exclusive, we can use eq. (5.9) to get:
</p>
<p>P (R) = P (A2 +A3 +A5) = P (A2) + P (A3) + P (A5) =
1
</p>
<p>6
+
</p>
<p>1
</p>
<p>6
+
</p>
<p>1
</p>
<p>6
=
</p>
<p>3
</p>
<p>6
=
</p>
<p>1
</p>
<p>2
. (5.14)
</p>
<p>Importantly, in this example we ended up doing exactly the same as eq. (5.13), however, the way
we arrived at the result was by showing each event was equally probably, and then applying the
rule for adding mutually exclusive probabilities.
</p>
<p>As another example, consider once more the cars example from table 2.1. Recall the dataset
consisted of data from N = 142 cars; if we focus on the Cylinders-attribute, and suppose there are
c4 = 95 cars with 4 cylinders, it seem intuitive to compute the probability a car has 4 cylinders as:
</p>
<p>P ({4 cylinders}) = c4
N
</p>
<p>=
95
</p>
<p>142
</p>
<p>Probabilities of this form is commonly called the empirical frequency or empirical estimates. But
what, exactly, does this probability refer to? In light of the previous example, this probability corre-
sponds exactly to the case where we select a cars-instance from the dataset with equal probability,
P ({Car i}) = 1N , ask the probability such an instance has four cylinders, written as
</p>
<p>F1 + F2 + · · ·+ F142
</p>
<p>(where Fi is the proposition car i has four cylinders) and then apply the same computation that
lead to eq. (5.14) to P (F1 + · · ·+ F142).
</p>
<p>This may seem like a rather long discussion to arrive at something intuitively obvious, however,
see example 5.1.2 for a non-trivial combination of simple counting arguments and the the basic rules
of probability, or section 5.2.1 for a continuation of the cars-example where we derive counting rules
for estimating conditional probabilities.
</p>
<p>Finally, the probability of four cylinders in the cars-dataset refers to a fact about the particular
dataset, and not in and by itself about the general prevalence of cylinders in cars, which is what we
really wish to know. This type of generalization can be though of as our first instance of learning,
and is one we will return to several times in the coming two chapters, see section 5.4 and section 6.4.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>70 5 Discrete probabilities and information
</p>
<p>Example 5.1.2: Two diceF
</p>
<p>Consider the following problem: Suppose we roll two dice. If at least one of the die show five,
what is the chance both show five?
We can easily compute this using the previous ideas. Let Ai be the event die 1 shows face
i and Bj the event die 2 shows j. Using the product rule, and the obvious fact if both dice
show five at least one must show five,
</p>
<p>P ({Both } | {At least one }) = P ({Both } {At least one )})
P ({At least one )})
</p>
<p>=
P ({Both })
</p>
<p>P ({At least one )})
</p>
<p>If we apply the marginalization rule eq. (5.11) twice we get:
</p>
<p>P ({At least one }) =
6∑
i=1
</p>
<p>6∑
j=1
</p>
<p>P ({At least one } |AiBj)P (AiBj)
</p>
<p>= P (A1B5) + P (A2B5) + P (A3B5) + P (A4B5) + P (A5B5) + P (A6B5)
</p>
<p>+ P (A5B1) + P (A5B2) + P (A5B3) + P (A5B4) + P (A5B6)
</p>
<p>=
11
</p>
<p>36
</p>
<p>where we used P (AiBj) = P (Ai)P (Bj) =
1
36 , and that when we know the outcome of both
</p>
<p>rolls, the conditional probability is either 0 or 1. A similar argument gives P ({Both }) = 136 ,
and therefore
</p>
<p>P ({Both } | {At least one }) =
1
36
11
36
</p>
<p>=
1
</p>
<p>11
.
</p>
<p>Example 2: The Monty Hall game showF
</p>
<p>As an illustration of the sum rule consider the following more elaborate problem originally posed
by Steve Selvin in 1975 [Selvin et al., 1975]:
</p>
<p>Example 5.1.3: The Monty Hall game show
</p>
<p>Suppose you’re on a game show, and you’re given the choice of three doors 1, 2, 3. Behind
one door is a car; behind the others, goats. You pick a door, say 1, and the host, who knows
what’s behind the doors, opens another door, say 3, which has a goat. He then says to you,
“Do you want to pick door 2?”. If you know the host never opens the door with a car is it
then to your advantage to switch your choice?
</p>
<p>It is tempting to solve the problem with reasoning along the following lines:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.1 Probability basics 71
</p>
<p>Independent of what door we choose, there is a 13 chance door 1 contains the car and
1
3 that
</p>
<p>door 2 contains the car. That the host later tells us something about door 3 does not shuffle
the goat and car around and so they remain equally likely to be behind the first two doors.
Thus the chance the car is behind door 1 is still 12 and there is no advantage in switching.
</p>
<p>This line of reasoning refers to several facts about the problem which are not in dispute (such as
the initial probabilities being 13 ). Do you think it is true? If the argument is true, it should not hurt
to examine it with more rigor. To do so, let us define the four variables:
</p>
<p>A1, A2, A3 : The car is behind door 1, 2 and 3 respectively
</p>
<p>Rg3 : The host reveals a goat behind door 3.
</p>
<p>Solving the riddle boils down to computing P (A1|Rg3), namely the probability the car is behind
door 1 given we initially selected door 1 and the host subsequently revealed the goat was behind
door 3, Using our newly derived version of Bayes theorem with mutually exclusive hypothesis we
get:
</p>
<p>P (A1|Rg3) =
P (Rg3|A1)P (A1)
</p>
<p>P (Rg3|A1)P (A1) + P (Rg3|A2)P (A2) + P (Rg3|A3)P (A3)
(5.15)
</p>
<p>Since the car is initially placed randomly we have P (A1) = P (A2) = P (A3). Then notice
</p>
<p>• If the car is behind door 1 and we selected door 1, then P (Rg3|A1) = 12 as the host choose
randomly between door 2 and 3 which both contains goats.
• If the car is behind door 2 and we selected door 1, then P (Rg3|A2) = 1 as the host cannot open
</p>
<p>our door (containing a goat) or the door with a car.
• If the car is behind door 3 and we selected door 1, then P (Rg3|A3) = 0 as the host will never
</p>
<p>open the door with a car
</p>
<p>If we plug this information into eq. (5.15) we have:
</p>
<p>P (A1|Rg3) =
P (Rg3|A1)P (A1)
</p>
<p>P (Rg3|A1)P (A1) + P (Rg3|A2)P (A2) + P (Rg3|A3)P (A3)
</p>
<p>=
P (Rg3|A1)
</p>
<p>P (Rg3|A1) + P (Rg3|A2) + P (Rg3|A3)
</p>
<p>=
1
2
</p>
<p>1
2 + 1 + 0
</p>
<p>=
1
</p>
<p>3
</p>
<p>Since the car is either behind the first or second door, then P (A2|Rg3) = 1 − P (A1|Rg3) = 23 and
so it is clearly in your best interest to switch doors.
</p>
<p>So what went wrong with the initial argument? The argument (subtly) relied on the idea that
probabilities referred to the actual state of the world and so should only change when the state of
the world changes (the goat and car cannot change places because of what the game host does). In
actuality, probabilities refer to our state of knowledge, and when we are given relevant knowledge
about the problem, our assignment of probability may change even if the world remains the same.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>72 5 Discrete probabilities and information
</p>
<p>5.2 Discrete data and stochastic variables
</p>
<p>Data will often come in numerical form, in which case it is common to express probabilities using
stochastic variables. This notation is often the cause of unnecessary pain, likely because the name
stochastic makes one think about change. This is not the case, rather, stochastic variables are simply
a notational shortcuts that makes it easier to define binary propositions of the kind we have already
encountered.
</p>
<p>More specifically, suppose in some situation we are measuring a quantity X. A way to think
about that is (for instance) a robot that has a sensor which keep track of the acceleration (on a
discrete scale), or in another example, a variable which corresponds to the number of children a
family has.
</p>
<p>Specifically, when we write X = x, where x is a number, we define that to be the binary
proposition Xx:
</p>
<p>Xx : {The binary event that the quantity X is equal to the number x} (5.16)
</p>
<p>In other words, whenever the reader encounters the statement X = x, simply make the above
mental substitution and we are back in the usual language of binary propositions.
</p>
<p>Let us make this concrete with a few examples. First, referring back to the Monty-Hall problem,
we can re-express the problem using the stochastic variables A and R:
</p>
<p>A = i ≡ {the car is behind door number i; equivalent to Ai}
R = j ≡ {the goat was revealed to be behind door number j}
</p>
<p>Using this notation, we can express the solution to the Monty Hall example eq. (5.15) as:
</p>
<p>P (A1|Rg3) ≡ P (A = 1|R = 3) =
P (R = 3|A = 1)P (A = 1)∑3
i=1 P (R = 3|A = i)P (A = i)
</p>
<p>.
</p>
<p>As another example, we will consider the silly die. Suppose we take an ordinary die and paint
each side with the numbers −2, 1, 1, 4, 4 and 10. When we then roll the silly die and read the number
on the side facing up, we thereby generate a random number. This outcome can be described as a
random variable X that takes one of the four different values
</p>
<p>x1 = −2, x2 = 1, x3 = 4, and x4 = 10
</p>
<p>the probability of each outcome being (see also fig. 5.3)
</p>
<p>p(X = x1) = p(X = x4) =
1
</p>
<p>6
, p(X = x2) = p(X = x3) =
</p>
<p>1
</p>
<p>3
(5.17)
</p>
<p>Note in this example, we chose to enumerate the events as x1, . . . , x4 rather than having to write
their numerical value again and again.
</p>
<p>This type of notation is so convenient we will use it from now one. Therefore, suppose X and
Y are stochastic variables and they each take values x1, x2, . . . and y1, y2, . . . respectively. We can
easily re-express for instance the product-rule to say that for any i and j:
</p>
<p>Product rule, normal version: P (XxiYyj ) = P (Xxi |Yyj )P (Yyj ) (5.18)
Stochastic variable version: P (X = xi, Y = yj) = P (X = xi|Y = yj)P (Y = yj) (5.19)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.2 Discrete data and stochastic variables 73
</p>
<p>-2 0 2 4 6 8 10
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>Fig. 5.3: Illustration of the density p(X) of the silly die, note the numbers sum to 1.
</p>
<p>Table 5.1: Summary counts of the cars dataset
</p>
<p>Origin Four cylinders Six cylinders Eight cylinders
</p>
<p>France 11 10 9
Germany 17 12 2
USA 28 21 32
</p>
<p>Where, once more, by Xxi we simply mean that the variable X, for instance the roll of the silly
die, took value xi.
</p>
<p>To make matters slightly more complicated, it is common to drop the stochastic variable if it is
clear from the context, and for instance write eq. (5.19) as:
</p>
<p>p(xi, yj) = p(xi|yj)p(yj).
</p>
<p>summary box 5.2.1 re-states the rules of discrete probabilities we have previously derived in this
notation.
</p>
<p>5.2.1 Example: Bayes theorem and the cars dataset
</p>
<p>To familiarize ourselves with the new notation, we will now re-visisit the Cars example from sec-
tion 5.1.6. Suppose an inspection of the data in table 2.1 reveal the counts in table 5.1. These should
be read as saying there are 2 cars which are both made in Germany and have eight cylinders. Now,
suppose we are interested in the question
</p>
<p>What is the probability a car is from Germany given it has eight cylinders?
</p>
<p>To solve this, the first thing we should do is define relevant stochastic variables O and C:
</p>
<p>country of origin: O = 1, 2, 3 (5.20)
</p>
<p>number of cylinders: C = 4, 6, 8. (5.21)
</p>
<p>Where O = 1 is USA, 2 is Germany, and 3 is France. The query of interest is then P (O = 2|C = 8).
Notice that according to the product rule, this can be written as:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>74 5 Discrete probabilities and information
</p>
<p>P (O = 2|C = 8) = P (O = 2, C = 8)
P (C = 8)
</p>
<p>.
</p>
<p>These two probabilities can be computed from the data using the methods in section 5.1.6, eq. (5.13);
i.e. we assume each car observation has a probability of 1142 and sum those together that matches
the query we are interested in. Specifically:
</p>
<p>P (O = 2, C = 8) =
2
</p>
<p>142
, P (C = 8) =
</p>
<p>9 + 2 + 32
</p>
<p>142
, P (O = 2|C = 8) =
</p>
<p>2
142
43
142
</p>
<p>=
2
</p>
<p>43
.
</p>
<p>Let us check this is consistent with Bayes’ theorem. To do so, we need to compute the probabilities:
</p>
<p>P (O = 1) =
11 + 10 + 9
</p>
<p>142
=
</p>
<p>30
</p>
<p>142
, P (O = 2) =
</p>
<p>17 + 12 + 2
</p>
<p>142
=
</p>
<p>31
</p>
<p>142
, P (O = 3) =
</p>
<p>28 + 21 + 32
</p>
<p>142
=
</p>
<p>81
</p>
<p>142
.
</p>
<p>In addition, we also need the conditional probabilities
</p>
<p>P (C = 8|O = 1) = 9
30
, P (C = 8|O = 2) = 2
</p>
<p>31
P (C = 8|O = 3) = 32
</p>
<p>81
.
</p>
<p>Therefore, we obtain:
</p>
<p>P (O = 2|C = 8) = P (C = 8|O = 2)P (O = 2)∑3
o=1 P (C = 8|O = o)P (O = o)
</p>
<p>=
2
31 × 31142
</p>
<p>9
30 × 30142 + 231 × 31142 + 3281 × 81142
</p>
<p>=
2
</p>
<p>43
.
</p>
<p>That these two ways of computing the probability agree match should not come as a surprise:
Fundamentally, it derives from our assumption each car-observation has a probability of 1142 , and
then applying the basic rules of probability. Since the rules of probability are always true, obviously
the result must be consistent!
</p>
<p>Technical note 5.2.1: More comments on notation
</p>
<p>The keen reader will observe we have changed to a lower-case p. This change reflects that
from a mathematical perspective, we can simply think of p(yj) and p(xi, yj) as the evaluation
of the function p(·) and p(·, ·) which compute the probability. Note the notation is heavily
overloaded, and unless it is evident from the context we will sometimes write
</p>
<p>pX(xi), pX,Y (xi, yj) and pX|Y (xi|xj)
</p>
<p>to represent the probabilities
</p>
<p>P (X = xi), P (X = xi, Y = yj) and P (X = xi|Y = yj)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.2 Discrete data and stochastic variables 75
</p>
<p>Algorithm 1: Generate a random sample from a discrete probability distribution
</p>
<p>Require: Probability of each outcome p(xi) = pi and T , the number of samples to generate
Ensure: Generate a random sample x̃1, . . . , x̃T ∼ p(·)
</p>
<p>for t = 1, . . . , T do
Generate u as a random number in the unit interval
Select k as the highest value such that
</p>
<p>∑k−1
i=1 pi ≤ u
</p>
<p>Set x̃t = xk
end for
</p>
<p>Summary 5.2.1: Rules of probability, discrete version
</p>
<p>Consider three stochastic variables X,Y , and Z and suppose xi, yj and zj are three numbers
representing values taken by each variable. Then
</p>
<p>p(xi|yj) ≡ P (X = xi|Y = yj) ≡ P (Xxi |Yyj )
</p>
<p>represents the probability that X takes value xi given that Y takes value yj . In this notation,
the sum/product rule is
</p>
<p>The sum rule:
</p>
<p>∞∑
i=1
</p>
<p>p(xi|zj) = 1
</p>
<p>The product rule: p(xi, yj |zk) = p(xi|yj , zk)p(yj |zk)
</p>
<p>As important special cases, we mention Bayes’ theorem and marginalization:
</p>
<p>p(yj |xi, zk) =
p(xi|yj , zk)p(yj |zk)∑∞
</p>
<p>j′=1 p(xi|yj′ , zk)p(yj′ |zk)
, p(xi|zk) =
</p>
<p>∞∑
j=1
</p>
<p>p(xi|yj , zk)p(yj |zk).
</p>
<p>Note in all these rules, zk may be omitted provided it is done on both sides of the equality
sign.
</p>
<p>5.2.2 Generating random numbersF
</p>
<p>Often, we will talk about a sample of random numbers generated from a probability distribution.
Suppose a random variable X have N possible outcomes x1, . . . , xN , and the probability of each
outcome is p(X = xi) = pi. We can imagine each pi is a small stick of length pi meters, and if we
place the N sticks next to each other they have a combined length of 1 meter. If we then pick a
random point uniformly within this large stick, it will select stick i with probability pi, and if we
do this T times we get our random sample, commonly written using the tilde-symbol:
</p>
<p>x̃1, . . . , x̃T ∼ pX(·).
</p>
<p>Where obviously x̃t is equal to one of the possible outcomes, x1, . . . , xN . A concrete implementation
of the procedure can be found in algorithm 1 and an example using the silly die in fig. 5.4.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>76 5 Discrete probabilities and information
</p>
<p>Fig. 5.4: Considering each bar in the histogram as a stick, we can generate a random sample x̃t
from the silly die (a roll) using a random uniform number u ∈ [0, 1].
</p>
<p>5.2.3 Expectations, mean and variance
</p>
<p>If we roll a die a large number of times and compute the average, the average will eventually get
arbitrarily close to the mean of a die roll which is
</p>
<p>1 + 2 + 3 + 4 + 5 + 6
</p>
<p>6
=
</p>
<p>7
</p>
<p>2
</p>
<p>This procedure can be generalized as follows: Suppose X is a discrete random variable taking the
possible values x1, . . . , xn and f is an arbitrary function. The expectation of f is then defined as:
</p>
<p>Expectation: E[f ] =
N∑
i=1
</p>
<p>f(xi)p(xi). (5.23)
</p>
<p>A useful intuition about the expectation is that if we let
</p>
<p>x̃1, . . . , x̃T ∼ pX
</p>
<p>be a random sample generated from p length T (see section 5.2.2), the simple average will approach
the expectation when T becomes large enough just as the case of the die:
</p>
<p>lim
T→∞
</p>
<p>1
</p>
<p>T
</p>
<p>T∑
t=1
</p>
<p>f̃(xt) = E[f ].
</p>
<p>Two expectations are of particular importance namely the mean and variance. These can be
obtained by setting f(x) = x and f(x) = (x−µ)2 where µ is the mean of x. In particular we write:
</p>
<p>mean: E[x] =
N∑
i=1
</p>
<p>xip(xi), Variance: Var[x] =
</p>
<p>N∑
i=1
</p>
<p>(xi − E[x])2p(xi). (5.24)
</p>
<p>We have briefly illustrated the mean/variance definitions above in example 5.2.1. Note that since
the expectation can be generalized to continuous variables, and the rules for expectations are the
same in both cases, useful identities will be given later in section 6.2.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.3 Independence and conditional independence 77
</p>
<p>Example 5.2.1: Mean and variance
</p>
<p>These definitions may appear somewhat abstract and so they are worth illustrating with
two examples. First, suppose all outcomes are equally probable such that p(xi) =
</p>
<p>1
N . In this
</p>
<p>case:
</p>
<p>E[x] =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>xi, Variance: Var[x] =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>(xi − E[x])2.
</p>
<p>As another example, consider the silly die from eq. (5.17) and recall the probability of each
outcome was:
</p>
<p>p(X = −2) = p(X = 10) = 1
6
, p(X = 1) = p(X = 4) =
</p>
<p>1
</p>
<p>3
</p>
<p>Using the definitions we can compute the mean and variance of the silly die as:
</p>
<p>E[x] =
1
</p>
<p>6
(−2) + 1
</p>
<p>3
1 +
</p>
<p>1
</p>
<p>3
4 +
</p>
<p>1
</p>
<p>6
10 = 3
</p>
<p>Var[x] =
1
</p>
<p>6
(−2− 3)2 + 1
</p>
<p>3
(1− 3)2 + 1
</p>
<p>3
(4− 3)2 + 1
</p>
<p>6
(10− 3)2 = 14.
</p>
<p>5.3 Independence and conditional independence
</p>
<p>Consider three random variables X,Y and Z and denote their values by xi, yj and zk respectively.
Independence and conditional independence is then the mathematical relationships:
</p>
<p>Independent: p(xi, yj) = p(xi)p(yj) (5.25a)
</p>
<p>Conditionally independent given zk : p(xi, yj |zk) = p(xi|zk)p(yj |zk) (5.25b)
</p>
<p>Which must be true for all i and j. In case of conditional independence, if the relationship hold
for a particular zk we say they are independent given Z = zk, and if it holds for all zk we say they
are independent given Z. If two variables are not (conditionally) independent, they are said to be
(conditionally) dependent.
</p>
<p>Conditional independence will later play an important role in structuring our machine-learning
models and will usually be based on assumptions about the data-generating mechanism. Note that
a reader should take great care not to assume that independence implies conditional independence
or visa-versa as example 5.3.1 illustrates.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>78 5 Discrete probabilities and information
</p>
<p>Example 5.3.1: Independence
</p>
<p>Since probabilities has to do with knowledge, conditioning on something can render seem-
ingly independence events dependent and visa-versa.
Independence therefore does not imply conditional independence: Consider two
events corresponding to the number of children of Bob and Maria; these two events can be
assumed to be independent:
</p>
<p>p(Bob has i children,Maria has j children) = p(Bob has i children)p(Maria has j children)
</p>
<p>But if we condition on the third variable Z, Bob and Maria are married, the number of
children each of them have will be highly correlated and therefore:
</p>
<p>p(i, j|Married = True) 6= p(i|Married = True)p(j|Married = True).
</p>
<p>Conditional independence does not imply independence: Alternatively, ones math-
ematical skill and height are two highly dependent variables, because young children are
usually small and poor at math. But if we condition on age, it is reasonable to think they
are independent.
</p>
<p>5.4 The Bernoulli, categorical and binomial distributions
</p>
<p>As we saw in section 5.1.6, one idea for obtaining probabilities is simply to estimate them from the
data matrix X. For instance, suppose M = 2 and the two columns correspond to random variables
</p>
<p>X1, X2 and denote their possible values by x
(1)
k , x
</p>
<p>(2)
k respectively, the empirical estimate is then
</p>
<p>P̃ (X1 = x
(1)
1 , X2 = x
</p>
<p>(2)
1 ) =
</p>
<p>{
Number of rows i where X1i = x
</p>
<p>(1)
1 and X2i = x
</p>
<p>(2)
2
</p>
<p>}
N
</p>
<p>. (5.26)
</p>
<p>While this approach is useful in some cases, it has at least three problems:
</p>
<p>• For all but the most trivial datasets, it is not feasible to compute/store this many numbers
• Even if we try, the estimates are likely to be very poor
• More importantly, the whole idea of learning is the dataset can be summarized by a few, well-
</p>
<p>chosen parameters.
</p>
<p>Probabilistic models, that is, models that depends on parameters, are a solution to these problems.
Obviously, how to build these models will be a subject we return to several times in later chapters,
however in this chapter we will be concerned with introducing a few building blocks which we will
use many times over when constructing more elaborate models.
</p>
<p>5.4.1 The Bernoulli distribution
</p>
<p>Consider a setting where we consider a single, binary variable b which can be either false, b = 0, or
true, b = 1.
</p>
<p>The prototypical example of a binary event is a coin flip where b = 0 denote the event the
coin landed tails and b = 1 the event the coin landed heads, but the setup applies to all simple</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.4 The Bernoulli, categorical and binomial distributions 79
</p>
<p>classification problems with two outcomes, for instance we could denote the event a treatment cures
a patient such that b = 0 if the patient is not cured and b = 1 if the patient is cured. The Bernoulli
distribution is the assumption the probability that b = 0 or b = 1 depends on a number 0 ≤ θ ≤ 1
as:
</p>
<p>Bernoulli distribution: p(b|θ) = θb(1− θ)1−b.
It is worth going over this in some detail. The left hand side says that given we know θ, then the
probability of b (which has two outcomes) is the expression on the right. Notice that:
</p>
<p>p(b = 0|θ) = θ0(1− θ)1−0 = 1− θ
p(b = 1|θ) = θ1(1− θ)1−1 = θ
</p>
<p>and therefore, we can interpret θ as simply being the probability b = 1. As an example, we can
compute the mean and variance of the Bernoulli distribution:
</p>
<p>E[b] =
1∑
b=0
</p>
<p>bp(b|θ) = 0× (1− θ) + 1× θ = θ, (5.27a)
</p>
<p>Var[b] = (0− θ)2 × (1− θ) + (1− θ)2 × θ = θ(1− θ). (5.27b)
A notational problem we can anticipate is that when we use p to denote all sorts of probability
</p>
<p>densities, it can become difficult to figure out exactly which we refer to. It is therefore common to
introduce special notation for familiar densities. We will therefore sometimes write:
</p>
<p>Bernouilli(b|θ) = p(b|θ)
to signify the Bernoulli distribution.
</p>
<p>5.4.2 The categorical distribution
</p>
<p>We will often consider situation with more than two outcomes, for instance a roll of a die (six
outcomes), multiple possible diagnosis (as many outcomes as there are diagnosis) or which category
an image belongs to (as many outcomes as there are image categories). Suppose there are M possible
outcomes, and let y = 1, . . . ,M denote the outcome of the experiment, the categorical distribution
is then defined as:
</p>
<p>Categorical distribution: Catagorical(y|θ) = θδy,11 θ
δy,2
2 × · · · × θ
</p>
<p>δy,K
K (5.28)
</p>
<p>where
∑K
k=1 θk = 1 and θk ≥ 0. We have here used that δy,k = 1 if y = k and zero otherwise. This
</p>
<p>notation is slightly cumbersome, but note once again it simply means that the chance y = k is
</p>
<p>p(y|θ) = θk
To simplify the notation, it is common to 1-out-of-K encode the variable k. Specifically, we introduce
K new variables, zi = δy,i, such that
</p>
<p>y = k ⇔
</p>
<p>
</p>
<p>z1
...
zk
...
zK
</p>
<p> =

</p>
<p>0
...
1
...
0
</p>
<p></p>
<p></p>
</div>, <div class="page"><p></p>
<p>80 5 Discrete probabilities and information
</p>
<p>in which case we can write the categorical distribution as:
</p>
<p>Categorical distribution: Catagorical(y|θ) =
K∏
k=1
</p>
<p>θzkk (5.29)
</p>
<p>5.4.3 Parameter transformations
</p>
<p>The parameter θ in the Bernoulli distribution is easy to interpret as the probability of b = 1.
A disadvantage is θ belongs to the interval [0, 1] and therefore, when we apply numerical methods
</p>
<p>this constraint has to be taken into account. A way around this problem is to replace θ with a
function of another parameter x, θ = h(x). As long as the domain of h is the unit interval, this
substitution lead to a well-defined probability density. The most common choice is the logistic
function θ = σ(x) = 11+e−x in which case we can write the density of the Bernoulli distribution as:
</p>
<p>p(b|x) = (1− σ(x))1−bσ(x)b (5.30)
</p>
<p>and now x can be any real number. A similar trick can be applied to the categorical function. In
this case we introduce K new parameters, x1, . . . , xK and re-define:
</p>
<p>θk =
exk∑K
c=1 e
</p>
<p>xc
</p>
<p>this operation, when applied to the entire parameter vector, will be written as
</p>
<p>θ = softmax(x) =
[
</p>
<p>ex1∑K
c=1 e
</p>
<p>xc
· · · exK∑K
</p>
<p>c=1 e
xc
</p>
<p>]T
and therefore
</p>
<p>p(y|x) = p(y|θ = softmax(x))
where the right-hand side is just the ordinary categorical distribution. While this is today the most
common way to re-parameterize the categorical distribution, the reader should be aware we can
alternatively choose to just use K − 1 parameters, x′1, . . . , x′K−1 and define:
</p>
<p>θk =
</p>
<p>
ex
′
k
</p>
<p>1+
∑K−1
c=1 e
</p>
<p>x′c
if k ≤ K − 1
</p>
<p>1
</p>
<p>1+
∑K−1
c=1 e
</p>
<p>x′c
if k = K.
</p>
<p>(5.31)
</p>
<p>5.4.4 Repeated events
</p>
<p>Suppose we flip the coin from before N times or, alternatively, we administer the treatment to
N patients and observe the outcome. In this case we have N binary (Bernoulli) events b1, . . . , bN ,
corresponding to the outcome of each event.
</p>
<p>When the events are based on the same process, it is reasonable to assume each event occurs with
a probability θ, but when we know θ the outcome of different coin flips or patients are independent.
In other words, the outcomes are conditionally independent given θ. If we then simply apply the
definition of conditional independence (eq. (5.25b)) the probability of an entire sequence becomes:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.4 The Bernoulli, categorical and binomial distributions 81
</p>
<p>p(b1, · · · , bN |θ) =
N∏
i=1
</p>
<p>p(bi|θ) =
N∏
i=1
</p>
<p>θbi(1− θ)1−bi = θ
∑N
i=1 bi(1− θ)N−
</p>
<p>∑N
i=1 bi
</p>
<p>= θm(1− θ)N−m, m = b1 + b2 + · · ·+ bN . (5.32)
</p>
<p>From this, we learn the important factor the probability of a particular sequence of outcomes
only depend on the length N and positive outcomes m. For convenience, we will refer to the sequence
of flips using the symbol b =
</p>
<p>[
b1 b2 . . . bN
</p>
<p>]
and write eq. (5.32) as p(b|θ) = θm(1− θ)N−m.
</p>
<p>5.4.5 A learning principle: Maximum likelihood
</p>
<p>Continuing the above example, the probability of the N flips b was:
</p>
<p>p(b|θ) = θm(1− θ)N−m. (5.33)
</p>
<p>When we vary θ, this probability changes. The exact way to think about this relationship is that
various values of θ makes the occurrence of the data more or less plausible.
</p>
<p>An idea is therefore to select θ as the value that maximizes the probability (plausibility) of the
data, and this principle is so often invoked the probability of the data given the parameters is called
the likelihood and denoted by the function L:
</p>
<p>L(θ) = p(b|θ).
</p>
<p>When we try to implement this idea, we often run into a practical issue, namely that probabilities
will often be very small. For instance, suppose θ = 0.9 and N = 1000. Then if m = 900, we have
</p>
<p>p(b|θ) ≈ 6.6 · 10−142.
</p>
<p>For this reason it is very common to work with logarithms of probabilities, called the log likelihood.
In our case the log likelihood is:
</p>
<p>logL(θ) = log p(b|θ) = m log θ + (N −m) log(1− θ)
</p>
<p>Maximizing the probability is the same as maximizing the likelihood, and learning θ by maximizing
the likelihood is known as the maximum likelihood principle.
</p>
<p>In fig. 6.11 we have illustrated the likelihood function L(θ) = p(b|θ) for different values of θ, N
and m. We see that as N increases, a smaller range of values of θ makes the data remotely possible.
The keen eyed reader will also observe the value of θ that maximizes the probability, called the
maximum likelihood estimate and referred to as θ∗, seems to be mN , i.e. the empirical frequency. We
can readily verify this is indeed the case by taking the derivative of the log likelihood (i.e., as the
logarithm is a monotonic function the optimum of the likelihood does not change when taking the
logarithm) and setting this derivative equal to zero:
</p>
<p>0 =
d logL(θ)
</p>
<p>dθ
=
m
</p>
<p>θ
− N −m
</p>
<p>1− θ , implies: θ
∗ =
</p>
<p>m
</p>
<p>N
.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>82 5 Discrete probabilities and information
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.02
</p>
<p>0.04
</p>
<p>0.06
</p>
<p>0.08
</p>
<p>0.1
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
10
</p>
<p>-30
</p>
<p>Fig. 5.5: Examples of the likelihood L(θ) = p(b|θ) from eq. (5.33) for different numbers of flips N
and different number of heads m. The top left figure corresponds to heads, the top-right to heads,
tails, bottom left to heads, tails, heads and bottom right to N = 100 flips where m = 51 came up
heads. Notice the dramatic change of scale on the y-axis.
</p>
<p>Summary 5.4.1: Common notation
</p>
<p>When Bayes’ theorem is used as a learning principle, for instance as in the Monty-Hall
example, it is common to give the different expressions names. Specifically, suppose x play the
role of data, y the role of a hypothesis, and we apply Bayes’ theorem to find the probability
of our hypothesis y given data x:
</p>
<p>p(y|x) = p(x|y)p(y)
p(x)
</p>
<p>It is then common to use the following names for the terms:
</p>
<p>Posterior =
Likelihood× Prior
</p>
<p>Evidence
.
</p>
<p>Even though these names may make it appear something more complicated is going on, each
of the terms are just familiar probability densities.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.5 Information TheoryF 83
</p>
<p>0 1 2 3 4
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0 1 2 3 4 5 6 7 8 9 10
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>0 10 20 30 40 50 60
</p>
<p>0
</p>
<p>0.02
</p>
<p>0.04
</p>
<p>0.06
</p>
<p>0.08
</p>
<p>0.1
</p>
<p>0.12
</p>
<p>Fig. 5.6: The binomial distribution p(m|N, θ) for N = 4, 10, 60 and θ = 0.7.
</p>
<p>5.4.6 The binomial distributionF
</p>
<p>The binomial distribution will play a minor role in this course, and a reader may choose to skip
this section. Briefly, suppose once more we have a sequence b1, . . . , bN of Bernoulli events. As we
have seen many times, their probability is
</p>
<p>p(b|θ) = θm(1− θ)N−m.
</p>
<p>However, suppose we wish to compute p(m|θ), namely the probability of observing m positive
outcomes in a sequence of N Bernoulli events that each occur with probability θ. This probability
can be computed using the sum rule:
</p>
<p>p(m|N, θ) = {Sum of the probability of all sequences of length N with m positive outcomes}
</p>
<p>=
</p>
<p>{
Number of sequences of length N
with m positive outcomes
</p>
<p>}
×
{
</p>
<p>Probability of a sequence
with m positive outcomes
</p>
<p>}
=
</p>
<p>{
Number of sequences of length N
with m positive outcomes
</p>
<p>}
× θm(1− θ)N−m (5.34)
</p>
<p>Computing the quantity in the bracket requires a combinatorial argument, but it can be shown to
be equal to
</p>
<p>(
N
m
</p>
<p>)
= N !m!(N−m)! where n! = n(n− 1)× · · · × 1. We therefore have:
</p>
<p>Binomial distribution: p(m|N, θ) =
(
N
</p>
<p>m
</p>
<p>)
θm(1− θ)N−m, (5.35)
</p>
<p>In fig. 5.6 we have shown the probability density function of m computed from eq. (5.35) when
θ = 0.7 and N = 4, 10 and 60.
</p>
<p>5.5 Information TheoryF
</p>
<p>Shannon’s theory of information attempts to describe the information content in a random vari-
able [Shannon, 1948]. While information theory is an important topic in machine learning, it will
play a minor role for the subjects we will discuss, and a reader may choose to simply use method 5.5.1
as a reference and skip the following justifications. With these warnings out of the way, the measures
which we will introduce are:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>84 5 Discrete probabilities and information
</p>
<p>Information How much information is contained in observing a single outcome xi of a random
variable?
</p>
<p>Entropy The complexity of the distribution of a a random variable, measured in bits
Mutual information How much information is shared between two random variables, measured
</p>
<p>in bits. Put differently, if we learn the state of one random variable, how many bits of information
does this tell us about the other
</p>
<p>Normalized mutual information Same as mutual information, but re-scaled by the information
content in the two random variables.
</p>
<p>5.5.1 Measuring information
</p>
<p>How do we measure information? First, we must recognize that information is a word that can take
many meanings, and information theory is a theory which explore one, particular, meaning of the
word.
</p>
<p>According to information theory, we consider the information contained in repeated, random
events. For instance, we can imagine a weather-station which each day send home a weather-report
consisting of one of the following three events:
</p>
<p>E1 : The weather is stormy.
</p>
<p>E2 : The weather is windy but not exceedingly so.
</p>
<p>E3 : The weather is fair.
</p>
<p>What we specifically wish to quantify is the amount of information the receivers of the weather-
report obtains when they learn for instance E3 occurred.
</p>
<p>Since the events are discrete, they happen with some probability, which we will write in the
usual way:
</p>
<p>P (E1) = p1, P (E2) = p2, P (E3) = p3.
</p>
<p>Let’s suppose there exists such a measure of information which we will write as
</p>
<p>I(E) : The information obtained by hearing E occurred.
</p>
<p>Obviously, this measure of information can’t take into account the specifics of what the event
was because that is a convention, however, it should take into account what probability the event
occurred with: There is little information in knowing about things that are very likely to happen,
but a lot of information in very infrequent events. For instance, if it is stormy nearly every day,
learning it is stormy again today does not tell us much, but on the other hand, if we learn something
surprising, like the weather was good, that contains a lot of information.
</p>
<p>Accordingly, our measure of information is a function of the probability of E:
</p>
<p>I(E) = I(P (E)) = I(p), where p = P (E).
</p>
<p>Let’s consider two important examples: Suppose we know an event will occur (P (E) = 1). The
information we obtain by hearing that E occurred is then zero, I(E) = I(1) = 0.
</p>
<p>Next, consider two events E1 and E2 which are independent, meaning:
</p>
<p>P (E1E2) = P (E1)P (E2) = p1p2.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.5 Information TheoryF 85
</p>
<p>However, since the events are independent, the information in knowing both events happened should
be the sum of the information of both events taken independently. That is, we conclude our measure
I should obey:
</p>
<p>I(p1p2) = I(p1) + I(p2),
</p>
<p>and in the particular case where p1 = p2 = p (for instance, E1 and E2 might be independent flips
of the same coin) then we have
</p>
<p>I(p2) = I(p) + I(p) = 2I(p).
</p>
<p>In general, if we consider n events we obtain:
</p>
<p>I(pn) = I(p · pn−1) = I(p) + I(pn−1) = nI(p). (5.36)
</p>
<p>To proceed we will use a small trick. Notice for any integer m ≥ 0 and probability p we can write
p = p
</p>
<p>m
m =
</p>
<p>(
p
</p>
<p>1
m
</p>
<p>)m
. Using eq. (5.36) we get:
</p>
<p>I(p) = I
((
p
</p>
<p>1
m
</p>
<p>)m)
= mI
</p>
<p>(
p
</p>
<p>1
m
</p>
<p>)
, (5.37)
</p>
<p>or more conveniently this can be written as I
(
p
</p>
<p>1
m
</p>
<p>)
= 1mI(p). Let’s then consider any rational
</p>
<p>number r = nm where n,m are integers. Combining eq. (5.37) and eq. (5.36) we obtain:
</p>
<p>I
(
p
n
m
</p>
<p>)
= I
</p>
<p>((
p
</p>
<p>1
m
</p>
<p>)n)
= nI
</p>
<p>(
p
</p>
<p>1
m
</p>
<p>)
=
</p>
<p>n
</p>
<p>m
I(p). (5.38)
</p>
<p>If we assume the measure of information is continuous, it therefore holds for general positive x that
I(px) = xI(p). If we differentiate according to x we get:
</p>
<p>I ′(px)px log p = I(p).
</p>
<p>Since this is true for any x it follows I ′(px) = A 1px for an unknown constant A. From this we
</p>
<p>conclude: I ′(z) = A 1z which implies
</p>
<p>I(z) = A log(z) +B
</p>
<p>where B is a constant. However since I(1) = 0 we can conclude B = 0. It is convenient to have
I(z) ≥ 0, and we therefore select A &lt; 0, however, any number will in principle do. A particular
convenient choice is A = − 1log 2 which ensures information is measured in bits3 We have now derived
that the information content of an event that occurs with probability p is:
</p>
<p>I(p) = − 1
log 2
</p>
<p>log p+ 0 = − log2(p) (5.39)
</p>
<p>where log2(p) is the base-2 logarithm. For instance, suppose you flip a single, unbiased coin. The
amount of information obtained is then I( 12 ) = − log2 12 = 1 bit and the amount of information in
N such coins is log2
</p>
<p>1
2N
</p>
<p>= n bits.
</p>
<p>3 Alternatively, if we choose A = −1 the information is said to be measured in nats.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>86 5 Discrete probabilities and information
</p>
<p>5.5.2 Entropy
</p>
<p>Suppose we consider a source of random events, for instance a die (the random events are which of
the six sides face upwards in a roll) or the next character in a newspaper article. In case of the die
there are 6 events, each occurring with probability p1, · · · , p6, and for the next letter in a sentence
there are 26 outcomes (letters in the alphabet) each occuring with probability p1, · · · p26. Such a
source is quantified by the average information content it produces which is simply the average of
the information
</p>
<p>H[p1, . . . , pn] =
</p>
<p>n∑
i=1
</p>
<p>piI(pi) = −
n∑
i=1
</p>
<p>pi log pi
</p>
<p>This quantity is known as the entropy and is a measure of the amount of uncertainty associated
with events produced from a given distribution4. Since we don’t want to write the probabilities
every time, let’s suppose the probabilities relates to a random quantity k = 1, · · · ,K and that we
write
</p>
<p>pk(1), pk(2), · · · , pk(K)
for the probability of each event k = 1, . . . ,K. We will then write H[pk] for the entropy of the
random variable k defined as
</p>
<p>H[pk] = −
K∑
k=1
</p>
<p>pk(k) = −
K∑
k=1
</p>
<p>pk(k) log pk(k)
</p>
<p>Example 5.5.1: Example 1: Entropy of a coin flip
</p>
<p>The entropy of a single (biased) coin c = 0, 1 is given by writing the probability of heads
and tails as pc(0), pc(1) = 1− pc(0). The entropy is then:
</p>
<p>H [pc] = −p log p− (1− p) log(1− p), where p = pc(0).
</p>
<p>This entropy is zero when p = 0 or p = 1 (i.e. we know the outcome of flipping the coin
beforehand) and maximal when p = 12 . In this case the entropy is H[pc] = log 2 or H[pc] =
log2 2 = 1 if we use the base-two logarithm. Intuitively, this is saying that a random, binary
event where we are completely uncertain about the outcome beforehand contains 1 bit of
information.
</p>
<p>This definition easily allows us to consider the entropy of multiple variables. Suppose we have a
density of two quantities k = 1, . . . ,K and m = 1, . . . ,M . We define their joint density as:
</p>
<p>pkm(k,m), for k = 1, . . . ,K and m = 1, . . . ,M
</p>
<p>We stress that pkm is just the regular old probability and the subscript km are only there to make
referencing easier later. For instance, the marginal density of k, m is as usual given by the sum rule:
</p>
<p>4 Two comments: Firstly, we are no longer using the base 2 logarithm but the regular logarithm, however,
as we saw in the previous section, this is only a matter of scale, and the natural logarithm is somewhat
easier to write. Secondly, if an event occurs with p = 0, we use the convention 0× log 0 = 0</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.5 Information TheoryF 87
</p>
<p>pk(k) =
</p>
<p>M∑
m=1
</p>
<p>pkm(k,m) and pm(m) =
</p>
<p>K∑
k=1
</p>
<p>pkm(k,m).
</p>
<p>In the case of two variables the entropy is simply:
</p>
<p>H[pkm] = −
K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>pkm(k,m) log pkm(k,m).
</p>
<p>The following two examples illustrates how the entropy can be calculated for distributions of two
variables
</p>
<p>Example 5.5.2: Example 2: Entropy of two variables
</p>
<p>Since it will be relevant later, let’s consider an example with two variables. Suppose M =
K = 2 and
</p>
<p>pkm(1, 1) = 0.4, pkm(1, 2) = 0.2, pkm(2, 1) = 0.1 and pkm(2, 2) = 0.3
</p>
<p>the entropy is then:
</p>
<p>H[pkm] = −
K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>pkm(k,m) log pkm(k,m)
</p>
<p>= −0.4 log 0.4− 0.2 log 0.2− 0.1 log 0.1− 0.3 log 0.3
≈ 1.28.
</p>
<p>For K outcomes, the entropy is largest when all events has the same probability, and in general
the entropy becomes lower when one particular outcome has high probability. This is simply saying
that the uncertainty in a random phenomenon is smaller when we know one outcome is very likely
to occur.
</p>
<p>5.5.3 Mutual information
</p>
<p>Recall two variables X and Y are independent if
</p>
<p>p(X = xi, Y = yi) = p(X = xi)p(Y = yi)
</p>
<p>and otherwise dependent. A way to view mutual information is as a way to quantify how dependent
two variables are; if the two variables are independent, the mutual information is 0, and otherwise
greater than zero. More specifically, Mutual information tell us how many bits of information we
learn about X if we know Y .
</p>
<p>Specifically, given a probability assignment pkm(k,m) the Mutual information is defined using
the entropy as:
</p>
<p>MI[pkm] = H[pk] +H[pm]−H[pkm]</p>
<p></p>
</div>, <div class="page"><p></p>
<p>88 5 Discrete probabilities and information
</p>
<p>A loose justification of this definition is that it computed the sum of information in the two quan-
tities, and then subtract a term which is low if the two variables are highly redundant. That is, the
mutual information becomes low if the two variables are highly highly redundant, and otherwise it
will be high. Note the mutual information can be re-written as:
</p>
<p>MI[pkm] =
</p>
<p>K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>pkm(k,m) log
pkm(k,m)
</p>
<p>pk(k)pm(m)
.
</p>
<p>To get some more intuition, we will consider two extreme cases. First, suppose k and m are com-
pletely unrelated such that pkm(k,m) = pk(k)pm(m). In this case:
</p>
<p>MI[pkm] =
</p>
<p>K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>pk(k)pm(m) log
pk(k)pm(m)
</p>
<p>pk(k)pm(m)
=
</p>
<p>K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>pk(k)pm(m) log 1 = 0
</p>
<p>This makes sense intuitively: If k and m are not informative about each other, the mutual infor-
mation (information shared between them) should be zero. On the other hand assume k actually
determines m, for instance that the two variables are the same quantity measured twice. In this
case pkm(k,m) = pk(k) = pm(m) and so:
</p>
<p>MI[pkm] =
</p>
<p>K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>pk(k) log
pk(k)
</p>
<p>pk(k)pm(m)
=
</p>
<p>M∑
m=1
</p>
<p>pk(k) log
1
</p>
<p>pm(m)
= H[pk], (5.40)
</p>
<p>which also makes sense from an intuitive standpoint: If k tells us what m is (for instance if they
are the same), then the mutual information is all the information in k or H[pk(k)].
</p>
<p>5.5.4 Normalized mutual information
</p>
<p>Sometimes, the mutual information is normalized to lie on a scale from 0 to 1 to make it easier to
interpret. While there are several ways of doing so we will here consider the method of Strehl and
Ghosh [2002] which is reminiscent of the definition of the correlation we saw earlier in eq. (4.7) in
chapter 4
</p>
<p>NMI[pkm] =
MI[pkm]√
</p>
<p>H[pk]
√
H[pm]
</p>
<p>. (5.41)
</p>
<p>We see now that if k determines m and visa-versa we obtain:
</p>
<p>NMI[pkm] =
MI[pkm]√
</p>
<p>H[pk]
√
H[pm]
</p>
<p>=
H[pm]
</p>
<p>H[pm]
= 1.
</p>
<p>Therefore, an NMI of 0 means the variables are independent, and 1 means they are maximally
dependent.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>5.5 Information TheoryF 89
</p>
<p>Method 5.5.1: Information theory
</p>
<p>The information contained in an event which occur with probability p is
</p>
<p>I(p) = − log(p)
</p>
<p>Next, consider two random quantities m and n, such that k can take values k = 1, . . . ,K
and m can take values m = 1, . . . ,M . We assume we know the joint distribution of n,m,
which is the K ×M matrix:
</p>
<p>pkm(k,m), for k = 1, . . . ,K and m = 1, . . . ,M
</p>
<p>Based on this matrix, we can define the marginal distributions as the K and M -dimensional
vectors:
</p>
<p>pk(k) =
</p>
<p>M∑
m=1
</p>
<p>pkm(k,m), pm(m) =
</p>
<p>K∑
k=1
</p>
<p>pkm(k,m)
</p>
<p>The Entropy in the 1 and 2d-case is then defines as:
</p>
<p>H[pk] = −
K∑
k=1
</p>
<p>pk(k) log pk(k). H[pkm] = −
K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>pkm(k,m) log pkm(k,m).
</p>
<p>In both cases, it measures the complexity of pk and pkm in bits. In addition, the mutual
information and normalized mutual information is defined as:
</p>
<p>MI[pkm] = H[pk] +H[pm]−H[pkm]
</p>
<p>NMI[pkm] =
MI[pkm]√
</p>
<p>H[pk]
√
H[pm]
</p>
<p>.
</p>
<p>When we use the natural logarithm (log(x)), The information, entropy, and mutual informa-
tion are all measured in nats. If we instead use the base-2 logarithm log2(x), or alternatively
simply divide each quantity by log(2), the quantities are measured in bits.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>6
</p>
<p>Densities and models
</p>
<p>So far we have considered the probability of binary events such as A, B, Ai and so on. When working
with machine-learning models, input is usually defined as continuous numbers and so we have to
work with the probability of continuous quantities. We do so by using probability densities defined
on continuous numbers. These may appear so different from probabilities they signify a departure
from the basic sum-and-product rules applied to binary events, however, we will here stress how
densities, and the rules relating to densities, follow from the basic rules and therefore do not signify
any new formalisms.
</p>
<p>We will begin by providing an intuitive introduction to this connection, and subsequently dis-
cuss the most commonly used continuous density, the multivariate normal distribution, and the
connection between continuous probabilities and learning. The end-result will be a general recipe
for building machine-learning models we will return to later in the course.
</p>
<p>6.1 Probability densities
</p>
<p>Let us consider a simple example. Suppose we denote by r the amount of daily rainfall in Denmark.
Clearly, r is random since the amount of rain varies for different days. However, the problem is that
it does not strictly speaking make sense to talk about the probability r takes a specific value. After
all, suppose we ask:
</p>
<p>What is the probability there will be r = 2.3mm of rain a given day?
</p>
<p>We could just as well have asked for r = 2.31 or r = 2.299mm, and so a little thought reveals the
probability there will be exactly r = 2.3mm of rain must be zero. The way we overcome this is to
rather ask
</p>
<p>What is the probability there will be between 2 and 3mm of rain?.
</p>
<p>This is a proper yes/no question that can be formulated by introducing a binary variable
</p>
<p>A[a,b] : There will be between a and b mm of rain, i.e. r ∈ [a, b],
</p>
<p>and then simply write P (A[2,3]) for the probability r ∈ [a, b]. Importantly, this probability is non-
zero, and of the usual kind encountered in the previous chapter. In fig. 6.2 (left) we have tabulated</p>
<p></p>
</div>, <div class="page"><p></p>
<p>92 6 Densities and models
</p>
<p>Rainfall in mm / day
</p>
<p>P
ro
b
a
b
il
it
y
</p>
<p>0-1mm 1-2mm 2-3mm 3-4mm 4-5mm 5-6mm 6-7mm 7-8mm 8-9mm 9-10mm
0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>0.35
</p>
<p>0.4
</p>
<p>P (A[5,10]) = 0.075
</p>
<p>P (A[2,3]) = 0.145
</p>
<p>P (A[0,1]) = 0.393
</p>
<p>Rainfall in mm / day
</p>
<p>P
ro
b
a
b
il
it
y
d
en
si
ty
</p>
<p>0 1 2 3 4 5 6 7 8 9 10
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 6.1: Left: The probability of rainfall per day illustrated as a histogram. Each bar denotes the
event that on a given day there are between 0 − 1mm of rain, 1 − 2mm of rain, 2 − 3mm of rain
etc. These well-defined binary events can be estimated from historical rainfall. records. Right: If we
introduce a function p(r), we can define the probability of the event A[a,b] (that there was between a
</p>
<p>and b mm of rainfall a given day) as p(A[a,b]) =
∫ b
a
p(r)dr. The three colored regions thus correspond
</p>
<p>to three events, and each area corresponds to their probability.
</p>
<p>different intervals of amounts of rainfall in Denmark and their respective probabilities. However,
keeping track of rainfall using such a histogram is not very practical. After all, suppose someone ask
for P (A[2.5,3.5])? We can perhaps make a qualified guess at this variable by eye-balling neighboring
bins in the histogram (a reasonable guess would be around 12%), however, we would like a convenient
and exact way to represent all such binary variables. This is exactly the task a probability density
function accomplishes. A probability density function is simply a function p that is non-negative
and integrates to one. Using this function, we can then represent the probability of a particular
event such as A[a,b] as the integral
</p>
<p>P (A[a,b]) =
</p>
<p>∫ b
a
</p>
<p>p(x)dx. (6.1)
</p>
<p>In fig. 6.1 (right) is shown the probability of the events A[2,3], A[0,1] and A[5,∞[ with their respective
probabilities corresponding to the area under the density function.
</p>
<p>6.1.1 Multiple continuous parameters
</p>
<p>As we have seen, probability densities are simply functions that are useful as bookkeeping devices
to make statements about continuous random variables, such as the amount of rainfall r. In this
section, we will use this connection to, by means of the sum-and-product rule for binary variables,
derive similar looking rules densities must therefore obey. We stress these new rules are not some
new postulate of probability theory, but merely a consequence of the binary sum-and-product rules.
</p>
<p>To do so, consider a very small interval of width dx, A[x,x+dx] (for instance dx = 0.1). Since p
will be nearly flat assuming dx is small enough then (see fig. 6.2)
</p>
<p>P (A[x,x+dx]) ≈ p(x)dx.
</p>
<p>In general, suppose we have two variables x, y. In direct generalization of the 1d case, the probability
that x, y both fall within some 2d subset D of R2 is then</p>
<p></p>
</div>, <div class="page"><p></p>
<p>6.1 Probability densities 93
</p>
<p>P (A[x,x+dx]) ≈ p(x)dx = 0.303dx
</p>
<p>Rainfall in mm / day
</p>
<p>P
ro
b
a
b
il
it
y
d
en
si
ty
</p>
<p>0 1 2 3 4 5 6 7 8 9 10
0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 6.2: Continuing further the rainfall example, rather than talking about the probability there
will be exactly x mm of rain, we can talk about the probability there will be between x and x+ dx
mm of rain. This can be approximated as p(A[x,x+dx]) ≈ p(x)dx which becomes more and more
exact when dx approaches 0.
</p>
<p>P ((x, y) ∈ D) =
∫
(x,y)∈D
</p>
<p>p(x, y)dxdy (6.2)
</p>
<p>However similar to the 1d case, we can consider the event x lies in the interval [x, x+ dx] and y in
the interval [y, y + dy], A[x,x+dx] and B[y,y+dy], see fig. 6.3 where this corresponds to the red area.
If we shut off our brain and apply the product rule:
</p>
<p>P (A[x,x+dx]B[y,y+dy]) = P (B[y,y+dy]|A[x,x+dx])P (A[x,x+dx]) (6.3)
</p>
<p>However using that dx, dy are both small we can again approximate this as
</p>
<p>P (A[x,x+dx]B[y,y+dy]) ≈ dxdyp(x, y), (6.4)
P (B[y,y+dy]|A[x,x+dx]) ≈ dyp(y|x), (6.5)
</p>
<p>P (A[x,x+dx]) ≈ dxp(x), (6.6)
</p>
<p>where p(y|x) is a new function of two parameters. If we plug these definitions into eq. (6.3) and
divide both sides by dxdy we obtain the more familiar form:
</p>
<p>p(x, y) = p(y|x)p(x).
</p>
<p>We say that p(x, y) is the joint density of x, y and p(y|x) is the density distribution of y given x.
In general, we can define the sum and product rules for continuous densities:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>94 6 Densities and models
</p>
<p>y + dy
y
</p>
<p>x+ dx
</p>
<p>x0
</p>
<p>1
0
</p>
<p>1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 6.3: Suppose we have a 2d density p(x, y). For a subset D ⊂ R2 we can define the probability
(x, y) lies in D as p(AD) =
</p>
<p>∫
(x,y)∈D p(x, y)dxdy. For small but non-zero values of dx and dy, and
</p>
<p>the case where D is the rectangle [x, x+ dx]× [y, y + dy] (the red area), the probability of D (the
volume indicated by the black lines) can be approximated as p(AD) ≈ p(x, y)dxdy
</p>
<p>The sum rule:
</p>
<p>∫
p(x|z)dx = 1, (6.7)
</p>
<p>The product rule: p(x, y|z) = p(y|x, z)p(x|z), (6.8)
</p>
<p>where again we will often omit z for simplicity. Since the machine-learning models we are interested
in use continuous variables we will in the coming sections and chapters mostly use these rules
as applied to densities. However, it is worth stressing this is not because these rules are more
fundamental or a departure from a simpler theory about binary probabilities: rather, they are
consequences of the simple sum-and-product rules introduced at the very beginning of this chapter
provided we choose to represent probabilities using densities.
</p>
<p>Notice, x, y, z in the above can also be vectors or discrete variables in which case we only have
to modify the integral in the sum rule to be either an integral over vectors or a sum over discrete</p>
<p></p>
</div>, <div class="page"><p></p>
<p>6.2 Expectations, mean and variance 95
</p>
<p>variables as we have already encountered. For completeness we also provide Bayes’ theorem for
continuous variables in summary box 6.1.1.
</p>
<p>Summary 6.1.1: Rules of probability, continuous version
</p>
<p>Consider three stochastic variables X,Y , and Z which are now considered to take continuous
values x, y, and z respectively. Note these are just numbers. We assume the joint density is
written as p(x, y, z) (in this case, a function of three variables). The sum and product rule
is:
</p>
<p>The sum rule:
</p>
<p>∫
p(x|z)dx = 1 (6.9a)
</p>
<p>The product rule: p(x, y|z) = p(x|y, z)p(y|z) (6.9b)
</p>
<p>As important special cases, we mention Bayes’ theorem and marginalization:
</p>
<p>p(y|x, z) = p(x|y, z)p(y|z)∫
p(x|y′, z)p(y′|z)dy′ , p(x|z) =
</p>
<p>∫
p(x|y, z)p(y|z)dy.
</p>
<p>Finally, note that:
</p>
<p>• These rules also hold for blocks of variables, for instance
∫∫∫
</p>
<p>p(x, y, z)dxdydz = 1 and
p(x, y, z, v) = p(x, y|z, v)p(z, v)
</p>
<p>• The variable z may be omitted, for instance p(x, y) = p(x|y)p(y)
• We are allowed to mix discrete and continuous variables, for instance: p(x, y, Z = zk) =
p(Z = zk, x|y)p(y) and
</p>
<p>∑∞
k=1 p(Z = zk, x) = p(x)
</p>
<p>• Independence/conditional independence is defined exactly as in the continuous case
</p>
<p>6.2 Expectations, mean and variance
</p>
<p>Expectations, means and variances work just like discrete probabilities, except we replace the sum
signs with integrals. Specifically, for any function f and random quantity x we have:
</p>
<p>E[f ] =
∫
f(x)p(x)dx (6.10)
</p>
<p>provided p is the density of x. Once more, mean and variance can be obtained by setting f(x) = x
and f(x) = (x− µ)2 where µ is the mean of x. In particular we write:
</p>
<p>mean: E[x] =
∫
xp(x)dx, variance: Var[x] =
</p>
<p>∫
(x− E[x])2p(x)dx (6.11)
</p>
<p>The rules governing expectations are the same in both cases. The reader is therefore invited to use
either eq. (6.11) or eq. (5.24) to verify the following useful identities:
</p>
<p>E[ax+ b] = aE[x] + b, Var[ax+ b] = a2 Var[x].
</p>
<p>where a, b are constants.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>96 6 Densities and models
</p>
<p>-2 0 2 4 6 8 10
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>-2 0 2 4 6 8 10
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>-2 0 2 4 6 8 10
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>Fig. 6.4: The density of the normal distribution N (x|µ, σ2) for µ = 4 and σ = 0.3, 1 and 2. Note
the density can be greater than 1.
</p>
<p>Multidimensional expectations
</p>
<p>Because it will be particularly relevant later, we will consider the more general case of a distribution
of several variables, say, x, y. In this case the concept of expectation generalizes:
</p>
<p>Ex,y[f(x, y)] =
∫∫
</p>
<p>f(x, y)p(x, y)dxdy
</p>
<p>notice we have introduced the subscript to indicate we take the expectation over x and y. An
important special case is when we consider the sum of several variables. In this case:
</p>
<p>Ex,y[x+ y] =
∫∫
</p>
<p>(x+ y)p(x, y)dxdy =
</p>
<p>∫∫
xp(x, y)dxdy +
</p>
<p>∫∫
yp(x, y)dxdy =
</p>
<p>∫
xp(x)dx+
</p>
<p>∫
yp(y)dy
</p>
<p>= Ex[x] + Ey[y] (6.12)
</p>
<p>More generally, if x1, . . . , xn are n random variables with joint density p, then for constants
a1, . . . , an we have:
</p>
<p>E
</p>
<p>[
n∑
i=1
</p>
<p>aixi
</p>
<p>]
=
</p>
<p>n∑
i=1
</p>
<p>aiE[xi] (6.13)
</p>
<p>Furthermore, if x1, . . . , xn are independent, that is, p(x1, . . . , xn) =
∏n
i=1 p(xi):
</p>
<p>Var
</p>
<p>[
n∑
i=1
</p>
<p>aixi
</p>
<p>]
=
</p>
<p>n∑
i=1
</p>
<p>a2i Var[xi]. (6.14)
</p>
<p>Both of these identities also hold in the discrete case.
</p>
<p>6.3 Examples of densities
</p>
<p>In this section, we will consider two examples of densities: The normal distribution (and it’s general-
ization, the multivariate normal distribution) which provides a convenient density forK-dimensional
vectors. Secondly, we will consider a simpler density, namely the Beta density which is defined on
the unit interval.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>6.3 Examples of densities 97
</p>
<p>x
</p>
<p>y
</p>
<p>P
ro
b
a
b
il
it
y
D
en
si
ty
</p>
<p>−3
−2
</p>
<p>−1
0
</p>
<p>1
2
</p>
<p>3
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>0.02
</p>
<p>0.04
</p>
<p>0.06
</p>
<p>0.08
</p>
<p>x
</p>
<p>y
</p>
<p>−2 0 2
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>Fig. 6.5: Example of the probability density function of a 2d multivariate normal distribution. In left
it is plotted as a function of x = [x y]T , i.e. p(x|µ,Σ), whereas on the right the same distribution
is shown as a contour plot.
</p>
<p>6.3.1 The normal and multivariate normal distribution
</p>
<p>In one dimension, the normal distribution with mean µ and variance σ2 has the density:
</p>
<p>N (x|µ, σ2) = 1√
2πσ2
</p>
<p>e−
(x−µ)2
</p>
<p>2σ2 .
</p>
<p>The symbol N is simply used for convenience; the normal density is nothing but a function which
depends on the three numbers x, µ and σ, and in more familiar notation we would write it as
p(x|µ, σ) = N (x|µ, σ2). Obviously, it follows from the sum rule eq. (6.9a):∫
</p>
<p>N (x|µ, σ2) = 1.
</p>
<p>The normal distribution has the familiar bell shaped curve seen in fig. 6.4. The parameters get their
name because if we computed the mean and variance using eq. (6.11):
</p>
<p>E[x] = µ, and Var[x] = σ2.
</p>
<p>The normal distribution can be generalized to the multivariate normal distribution which is a
distribution over d-dimensional vectors
</p>
<p>x =
[
x1 x2 · · · xd
</p>
<p>]T
,
</p>
<p>and is defined by the density:
</p>
<p>N (x|µ,Σ) = 1√
(2π)d|Σ|
</p>
<p>e−
1
2 (x−µ)
</p>
<p>TΣ−1(x−µ),</p>
<p></p>
</div>, <div class="page"><p></p>
<p>98 6 Densities and models
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 −1 0 1 2
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 −1 0 1 2
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 −1 0 1 2
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 −1 0 1 2
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>Fig. 6.6: Example of multivariate Gaussians when the covariance matrix is given as Σ1,Σ2,Σ3 and
Σ4 in eq. (6.15). When the covariance matrix is diagonal, the multivariate Gaussians are “cigars”
oriented along either of the two axis indicating that x1, x2 are independent (top row), however with
off-diagonal elements, the multivariate Gaussian indicate a dependence between x1, x2. Notice the
sign determines how they are slanted.
</p>
<p>where Σ is known as the covariance matrix which must be symmetric and positive definite and |Σ|
is the determinant of Σ and µ is known as the mean of the multivariate normal distribution1. In
fig. 6.5 is shown the multivariate normal distribution corresponding to
</p>
<p>Σ =
</p>
<p>[
1 0.8
</p>
<p>0.8 1
</p>
<p>]
and µ =
</p>
<p>[
0
0
</p>
<p>]
.
</p>
<p>Notice, the distribution for this choice of µ is centered on (0, 0); this is no accident. For a general
probability density p we can define the covariance matrix C as:
</p>
<p>1 The determinant quantifies the volume of the column-vectors of Σ. In 2d it corresponds to the cross-
</p>
<p>product, i.e.
</p>
<p>∣∣∣∣[a bc d
]∣∣∣∣ = ad− bc. In higher dimensions, the reader should consult a linear-algebra textbook
</p>
<p>or use a numerical library to compute the determinant, however, see also section 6.3.2.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>6.3 Examples of densities 99
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 −1 0 1 2
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 −1 0 1 2
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 −1 0 1 2
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>Fig. 6.7: Example of multivariate Gaussians when the covariance matrix is given as Σ1,Σ2 and Σ3
in eq. (6.16). Increasing the off-diagonal elements increases the covariance.
</p>
<p>Cij = cov(xi, xj) = Ex [(xi − E[xi])(xj − E[xj ])] =
∫
</p>
<p>( xi − E[xi])(xj − E[xj ])p(x)dx.
</p>
<p>For the special case of the multivariate normal distribution, it holds that
</p>
<p>µ = E[x] and Σ = C.
</p>
<p>This can allow us to get insight into how the multivariate normal distribution behaves for different
choice of covariance matrix. Different examples are illustrated in fig. 6.6 where in each instance the
</p>
<p>mean is chosen as µ =
[
0 0
]T
</p>
<p>and
</p>
<p>Σ1 =
</p>
<p>[
0.2 0
0 1
</p>
<p>]
, Σ3 =
</p>
<p>[
1 0
0 0.2
</p>
<p>]
, (6.15a)
</p>
<p>Σ2 =
</p>
<p>[
1 0.7
</p>
<p>0.7 1
</p>
<p>]
, Σ4 =
</p>
<p>[
1 −0.7
−0.7 1
</p>
<p>]
. (6.15b)
</p>
<p>Let us consider one final example where we vary the off-diagonal elements (see fig. 6.7) correspond-
ing to
</p>
<p>Σ1 =
</p>
<p>[
1 0
0 1
</p>
<p>]
Σ2 =
</p>
<p>[
1 0.45
</p>
<p>0.45 1
</p>
<p>]
Σ3 =
</p>
<p>[
1 0.9
</p>
<p>0.9 1
</p>
<p>]
. (6.16)
</p>
<p>Notice, the distribution becomes more slanted (skewed) when the off-diagonal elements increase.
</p>
<p>6.3.2 Diagonal covariance
</p>
<p>Consider the case where covariance matrix Σ is diagonal
</p>
<p>Σ =
</p>
<p>
σ21 0 . . . 0
0 σ22 0
...
</p>
<p>. . .
...
</p>
<p>0 0 . . . σ2d
</p>
<p> ,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>100 6 Densities and models
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>Fig. 6.8: Examples of the beta prior density of eq. (6.17) for different choices of α, β. These two
numbers control the mean and variance of the beta distribution, in particular notice the choice
α = β = 1 corresponding to the flat prior.
</p>
<p>In this case, the determinant is |Σ| = ∏di=1 σ2i and therefore:
p(x|µ,Σ) = 1√
</p>
<p>(2π)d|Σ|
e−
</p>
<p>1
2 (x−µ)
</p>
<p>TΣ−1(x−µ)
</p>
<p>=
1√
</p>
<p>(2π)d
∏d
i=1 σ
</p>
<p>2
i
</p>
<p>e−
∑d
i=1
</p>
<p>1
2 (xi−µi)σ
</p>
<p>−2
i (xi−µi)
</p>
<p>=
</p>
<p>d∏
i=1
</p>
<p>1√
2πσ2i
</p>
<p>e
− (xi−µi)
</p>
<p>2
</p>
<p>2σ2
i
</p>
<p>=
</p>
<p>d∏
i=1
</p>
<p>p(xi|µi, σ2i )
</p>
<p>Thus, in this case the multivariate normal distribution is just a product of univariate normal
distributions, i.e. the xi’s are independent.
</p>
<p>6.3.3 The Beta distribution
</p>
<p>The normal distribution gave us a distribution defined for all real numbers. For reasons that will be
apparent in a moment, we will be particular interested in quantities θ that are known to lie between
0 and 1. A particularly interesting family of distributions for such a quantity is the so-called Beta
distribution. This distribution depends on two parameters α, β &gt; 0 and has density2
</p>
<p>Beta density: Beta(θ|α, β) = Γ (α+ β)
Γ (α)Γ (β)
</p>
<p>θα−1(1− θ)β−1. (6.17)
</p>
<p>The two parameters α and β are related to the mean and variance as:
</p>
<p>Ep(θ|α,β)[θ] =
α
</p>
<p>α+ β
, Varp(θ|α,β)[θ] =
</p>
<p>αβ
</p>
<p>(α+ β)2(α+ β + 1)
. (6.18)
</p>
<p>2 In the definition, Γ (x) is the Gamma function. If x is an integer then Γ (x) = (x− 1)! and ! denotes the
factorial function, i.e., 4! = 4 · 3 · 2 · 1 = 24, and 0! = 1. For further details see https://en.wikipedia.
org/wiki/Gamma_function</p>
<p></p>
<div class="annotation"><a href="https://en.wikipedia.org/wiki/Gamma_function">https://en.wikipedia.org/wiki/Gamma_function</a></div>
<div class="annotation"><a href="https://en.wikipedia.org/wiki/Gamma_function">https://en.wikipedia.org/wiki/Gamma_function</a></div>
</div>, <div class="page"><p></p>
<p>6.3 Examples of densities 101
</p>
<p>0 0.5 1 1.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>0.2 0.4 0.6 0.8 1 1.2
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>0.5 0.6 0.7 0.8 0.9
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>Fig. 6.9: The distribution of ν = mN where m follows a binomial distribution p(m|N, θ) for N =
2, 10, 60 and θ = 0.7. The inserted red curve is the normal approximation.
</p>
<p>A reader should be warned these integrals are not easy to compute analytically. More insight in α
and β can be obtained by plotting realizations of the beta density for different values such as in
fig. 6.8. Notice in particular the choice α = β = 1, which exactly corresponds to a prior where no
value of θ is preferred. This is often called the uniform prior on the unit interval and is sometimes
written as U([0, 1]).
</p>
<p>6.3.4 The central limit theoremF
</p>
<p>The central limit theorem is a key theorem in statistics and provides both an explanation for why
the normal distribution is so common, but also a justification for why variance is reduced when
computing averages; as so many quantities in machine learning and statistics are averages, this
makes the central limit theorem a key theoretical concept.
</p>
<p>It is easier to introduce the central limit theorem by way of example. Once more, suppose we
flip N weighted coins and count the number of heads m. As we saw in the previous chapter, the
distribution of m is the Binomial distribution eq. (5.35)
</p>
<p>p(m|θ,N) =
(
N
</p>
<p>m
</p>
<p>)
θm(1− θ)N−m
</p>
<p>and in fig. 5.6 we have shown the probability density function of m when θ = 0.7 and N = 4, 10
and 60. If we look at these plots from arms length, they sort of look like the normal distributions
approximately centered at Nθ. In fact, this may give us an idea. Suppose we define the (random)
quantity
</p>
<p>ν ≡ m
N
</p>
<p>=
</p>
<p>N∑
i=1
</p>
<p>bi
N
</p>
<p>Since we are re-scaling by 1N , we should expect it to have mean θ. We can in fact compute both the
mean and variance analytically using first eq. (6.13) and eq. (6.14), and next eq. (5.27). Specifically
we get:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>102 6 Densities and models
</p>
<p>E[ν] = E
</p>
<p>[
n∑
i=1
</p>
<p>1
</p>
<p>N
bi
</p>
<p>]
=
</p>
<p>n∑
i=1
</p>
<p>1
</p>
<p>N
E [bi] =
</p>
<p>n∑
i=1
</p>
<p>1
</p>
<p>N
θ = θ (6.19a)
</p>
<p>Var[ν] = Var
</p>
<p>[
n∑
i=1
</p>
<p>1
</p>
<p>N
bi
</p>
<p>]
=
</p>
<p>n∑
i=1
</p>
<p>1
</p>
<p>N2
Var [bi] =
</p>
<p>n∑
i=1
</p>
<p>1
</p>
<p>N2
θ(1− θ) = θ(1− θ)
</p>
<p>N
(6.19b)
</p>
<p>This tell us two things. First, that we are right ν has mean θ, but also that the variance is inversely
proportional to 1N . We should therefore expect the normal density
</p>
<p>N
(
ν|µ = θ, σ2 = θ(1− θ)N−1
</p>
<p>)
=
</p>
<p>1√
2π θ(1−θ)N
</p>
<p>e
− (ν−θ)
</p>
<p>2
</p>
<p>2
θ(1−θ)
N =
</p>
<p>√
N
</p>
<p>2πθ(1− θ)e
−N(ν−θ)
</p>
<p>2
</p>
<p>2θ(1−θ)
</p>
<p>to at least provide an approximate match to the true density of ν. We have plotted both these
densities in fig. 6.9 for N = 4, 10, 60. While it should not be surprising the mean and variance
match (we just showed this to be the case in eq. (6.19)), it should be surprising that the exact shape
of the curves seem to match perfectly as N increases.
</p>
<p>We might suspect this has something to do with the Bernoulli distribution, but this is not the
case. To see this, let us turn to the silly die from section 5.2: Suppose we roll the silly die N times
and compute the mean value of the roll ν:
</p>
<p>ν =
</p>
<p>N∑
i=1
</p>
<p>xi
</p>
<p>For instance, let N = 3 and consider the following two example roll sequences:
</p>
<p>Roll sequence 1:
[
10 −1 4
</p>
<p>]
, mean of sequence 1: ν =
</p>
<p>13
</p>
<p>3
(6.20)
</p>
<p>Roll sequence 2:
[
4 2 1
</p>
<p>]
, mean of sequence 2: ν =
</p>
<p>7
</p>
<p>3
(6.21)
</p>
<p>The distribution of ν for general N , p(ν|N), is difficult to derive. For instance if N = 2 the chance of
getting ν = 10 (two 10’s) is p(ν = 10|N = 2) = p(ν = 10|N = 1)p(ν = 10|N = 1) = 162 , however it
is easy to simulate many realizations of the distribution, compute ν, and plot the estimated p(ν|N)
for different N . The result is shown in fig. 6.10 for N = 2, 10, 60 as the gray histogram.
</p>
<p>While the distribution is difficult to derive, a computation similar to eq. (6.19), along with the
mean/variance computed in example 5.2.1, show that
</p>
<p>E[ν] = {Mean of a single die} = 3, Var[ν] = {Variance of a single die}
N
</p>
<p>=
14
</p>
<p>N
</p>
<p>Both the true (simulated) density ν and the normal approximation N (ν|µ = E[ν], σ2 = Var[ν]) is
plotted in fig. 6.10. As we can clearly see in the figure, the normal approximation is very nearly
identical to the true distribution. This is the content of the central limit theorem:
</p>
<p>Consider N random variables X1, . . . , XN , each taking values x1, . . . , xN . If we then define z as
the mean:
</p>
<p>z =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>xi,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>6.4 Bayesian probabilities and machine learning 103
</p>
<p>-5 0 5 10
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0 2 4 6
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>1.5 2 2.5 3 3.5 4 4.5
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 6.10: The average of N rolls of the silly die for N = 2, 10, 60 and inserted normal approxima-
tions. Notice average of the rolls converge rapidly to the normal distribution.
</p>
<p>Then, as N increases, the distribution of z will be closer and closer to a normal distribution
N (z|µ, σ2) with mean/variance:
</p>
<p>µ = E[z] =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>E[xi], σ =
√
</p>
<p>Var[z] =
</p>
<p>√√√√ 1
N
</p>
<p>N∑
i=1
</p>
<p>Var[xi].
</p>
<p>It is this normal distribution, and in particular that the variance shrinks towards zero as N
increases, that guarantees averages converge in statistics to well-defined values. If the central limit
theorem did not hold there would be no reason to increase the number of patients in a clinical trial
to get a better idea about the average effect or increase the number of test examples in machine
learning to better judge the performance of a method, and it implies the normal distribution can
be expected to pop up in all kinds of circumstances because most quantities are (in one way or
another) representative of average effects.
</p>
<p>6.4 Bayesian probabilities and machine learning
</p>
<p>In this section, we will consider a very basic learning problem which nevertheless encapsulates how
probabilities are applied in general in machine learning. There are basically two steps when applying
probabilities in machine learning:
</p>
<p>• Write up a probability distribution for all relevant quantities of interest (data and parameters).
• Formulate the machine-learning task (prediction, classification, etc.) in terms of a probability
</p>
<p>which can be derived using the sum and product rule.
</p>
<p>We will illustrate this with a very simple inference problem. Suppose your friend shows you a coin
he bought at a flea market. The salesman informed him that it might be a magic coin (a magic coin
is a coin where one side comes up more often than 50%), but he wasn’t sure and in either case he
doesn’t know which side is supposed to come up more often.
</p>
<p>A simple learning problem could be to flip the coin a number of times and use the information
to figure out the chance it will come up heads in a new flip. As usual, let bi = 0 be the event the
coin comes up tails in flip i and bi = 1 heads. We once more represent the sequence heads, tails,
heads as
</p>
<p>b1 = 1, b2 = 0, b3 = 1,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>104 6 Densities and models
</p>
<p>which we will write as a vector b. Recall the by now well-known probability:
</p>
<p>p(b|θ) = θm(1− θ)N−m (6.22)
</p>
<p>Phrased in this way, what we are interested in is learning the chance the coin comes up heads θ
given the particular sequence of flips. In section 5.4.5 we saw one idea, namely to maximize the
likelihood function L(θ) = p(b|θ) and thereby obtain θ∗ = mN . A moments thought will reveal this
answer is quite plainly wrong. For instance, if we observe just N = 3 flips of a coin, nobody in their
right mind would rule out the possibility the coin was fair (i.e. θ = 12 ), but the only possible values
of θ∗ we could compute would be θ∗ are 0, 13 ,
</p>
<p>2
3 and 1. Obviously something has gone wrong!
</p>
<p>The problem is maximum likelihood is a principle, in the sense of a sometimes-good-idea, and
not something we have derived from first principles. The correct way to proceed is the only way to
proceed, namely to ask our rational robot from fig. 5.1 which only operates according to the rules
of probability. What we wish to learn is θ based on a sequence of coin flips b. Our belief in theta
given b is exactly p(θ|b), and simply plugging this into Bayes’ theorem we get:
</p>
<p>p(θ|b) = p(b|θ)p(θ)∫
p(b|θ′)p(θ′)dθ′ (6.23)
</p>
<p>Note we did not have to check or verify anything to use Bayes’ theorem: Rather, this is something
we can always do because the rules of probability are always true. Inspecting the above, we see
that in order to proceed, we must specify p(θ). Since we know just one distribution for a quantify
defined on the unit interval, the beta distribution, we will assume θ is beta distributed with (so far)
unknown parameters α, β &gt; 0
</p>
<p>p(θ) ≡ p(θ|α, β) = Γ (α+ β)
Γ (α)Γ (β)
</p>
<p>θα−1(1− θ)β−1 (6.24)
</p>
<p>We now have expressions for all terms on the right-hand side of eq. (6.23); simply plugging these
in and computing the integral we see:
</p>
<p>p(θ|b) = Beta(θ|a, b), a = α+m, b = β +N −m
= p(θ|b, α, β) (6.25)
</p>
<p>In the last line, we include α and β to signify the posterior depends on these two numbers (see also
eq. (6.29) for the full, analytical expression).
</p>
<p>In other words, when we use a Beta(·|α, β) prior for θ, the posterior density remains a Beta
distribution. This important property is known as conjugacy and is one of the motivations for using
a Beta prior. Furthermore, notice α and β plays roughly the same role as the number of flips that
come up heads m or tails N −m. In other words, we can interpret α and β in the prior as a number
of pseudo-counts where the specific case of the flat prior α = β = 1 corresponds to observing two
coin flips, one positive and one negative.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>6.4 Bayesian probabilities and machine learning 105
</p>
<p>Technical note 6.4.1: Deriving the posterior density of the coin
</p>
<p>Returning to our coin, to compute the posterior p(θ|b), we need to compute the numerator
and denominator of eq. (6.23). Beginning with the numerator, using the likelihood eq. (6.22)
and the Beta prior eq. (6.24) we obtain
</p>
<p>p(b|θ)p(θ) = p(b|θ)p(θ|α, β) (6.26)
</p>
<p>= θm(1− θ)N−m × Γ (α+ β)
Γ (α)Γ (β)
</p>
<p>θα−1(1− θ)β−1
</p>
<p>=
Γ (α+ β)
</p>
<p>Γ (α)Γ (β)
θm+α−1(1− θ)N−m+β−1 (6.27)
</p>
<p>The denominator is obtained by integrating this expression with respect to θ; the integral is
somewhat complicated analytically, but nevertheless based on well-known rules. We get:∫
</p>
<p>p(b|θ)p(θ)dθ =
∫ 1
0
</p>
<p>p(b|θ)p(θ|α, β)dθ = Γ (α+ β)
Γ (α)Γ (β)
</p>
<p>∫ 1
0
</p>
<p>θα+m−1(1− θ)β+N−m−1dθ
</p>
<p>=
Γ (α+ β)
</p>
<p>Γ (α)Γ (β)
</p>
<p>Γ (α+m)Γ (β +N −m)
Γ (α+ b+N)
</p>
<p>(6.28)
</p>
<p>Inserting eq. (6.28) and eq. (6.27) into eq. (6.23) we obtain:
</p>
<p>p(θ|b, α, β) =
Γ (α+β)
Γ (α)Γ (β)θ
</p>
<p>α+m−1(1− θ)β+N−m−1
Γ (α+β)
Γ (α)Γ (β)
</p>
<p>Γ (α+m)Γ (β+N−m)
Γ (α+β+N)
</p>
<p>=
Γ (α+ β +N)
</p>
<p>Γ (α+m)Γ (β +N −m)θ
α+m−1(1− θ)β+N−m−1. (6.29)
</p>
<p>All that remains is to note eq. (6.29) has the same form as the Beta density eq. (6.24), but
with new parameters a = α+m, b = β +N −m.
</p>
<p>6.4.1 Choosing the prior
</p>
<p>Continuing the example of the flat prior (α = β = 1), if we plug these values into either eq. (6.29)
or eq. (6.25) we obtain
</p>
<p>p(θ|b, α = β = 1) = (N + 1)!
m!(N −m)!θ
</p>
<p>m(1− θ)N−m. (6.30)
</p>
<p>In fig. 6.11 this figure is plotted for different sequences of flips, starting with just one flip that
came up heads N = 1,m = 1 and ending with N = 100 flips where m = 51 came up heads.
Several important aspects can be read off from this example. We observe that the distribution of θ
is peaked at around the expected value, i.e. mN . However when the number of observations increase
we become more and more certain that θ is near to this value. For instance in the N = 4,m = 3
we can compute the probability θ is in the interval [0.4, 0.6] as:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>106 6 Densities and models
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>Fig. 6.11: Examples of the posterior probability p(θ|b) from eq. (6.30) for different numbers of flips
N and different number of heads m. The top left figure corresponds to heads, the top-right to heads,
tails, bottom left to heads, tails, heads and bottom right to N = 100 flips where m = 51 came up
heads.
</p>
<p>P (θ is between 0.4 and 0.6|N = 4,m = 3) =
∫ 0.6
0.4
</p>
<p>p(θ|b) ≈ 0.25,
</p>
<p>whereas for N = 100,m = 51 this chance is more than 0.95!. This answer may not feel as satisfying
as a single number (“θ is really 0.5”), or the familiar confidence interval from classical statistics
(“with a confidence level of 95% θ is in the interval 0.5±θ0”), however, the Bayesian answer is more
general and later in chapter 10 we will see there exist a simple recipe for constructing the Bayesian
equivalent of the confidence interval, the credibility interval.
</p>
<p>6.5 Bayesian learning in general
</p>
<p>Let us turn to the general problem of learning parameters w from a dataset D = (X,y) consisting
of N observations in the usual format. First, the density we are interested in is
</p>
<p>p(w|D) = p(w|y,X) (6.31)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>6.5 Bayesian learning in general 107
</p>
<p>We will make the following two assumptions:
</p>
<p>p(y|X,w) =
N∏
i=1
</p>
<p>p(yi|xi,w) (6.32a)
</p>
<p>p(w|X) = p(w) (6.32b)
</p>
<p>What the first assumption tells us is that when we know the parameters w and xi, the other
observations are irrelevant in terms of predicting yi. The second assumption encode the idea X
alone does not tell us anything about w.
</p>
<p>At this point, the reader no doubt expect what will happen: Applying Bayes’ theorem to
eq. (6.31), and the then the two assumptions eq. (6.32a) and eq. (6.32b), we get:
</p>
<p>p(w|X,y) = p(y|X,w)p(w|X)
p(y|X)
</p>
<p>=
</p>
<p>∏N
i=1 p(yi|xi,w)p(w)
</p>
<p>p(y|X) (6.33)
</p>
<p>There are two general ways to proceed from here. In the coin-example, we proceeded by simply
computing the numerator and simplifying the expression. In that case, w was equal to θ, there was
no X, p(yi|xi,w) ≡ p(bi|θ) and p(w) = Beta(θ|α, β) was a Beta distribution.
</p>
<p>The second approach, which is simpler and the one we will consider in this book, is to maximize
the above expression with respect to w to find the most likely value of w. Maximizing eq. (6.33) is
equivalent to maximizing the logarithm, which can be written as:
</p>
<p>Maximize: logL(w) + log p(w), where L(w) =
N∑
i=1
</p>
<p>log p(yi|xi,w) (6.34)
</p>
<p>Sometimes, the prior term log p(w) (which does not depend on the data) will be ignored, and
sometimes not.
</p>
<p>It will often be more convenient to formulate the maximization problem as instead the corre-
sponding minimization problem of the cost-function obtained by multiplying eq. (6.34) with −1.
Furthermore, to easier compare the cost-function for models trained on different-size dataset, the
cost function is re-scaled by 1N so that it measures a cost-per-observation. In that case the mini-
mization problem becomes:
</p>
<p>Minimize: E(w) = − 1
N
</p>
<p>N∑
i=1
</p>
<p>log p(yi|xi,w). (6.35)
</p>
<p>Sometimes, we will add a regularization term to the right-hand side of this expression (see chap-
ter 14). This discussion has been somewhat abstract, but inspecting eq. (6.35), we see that one
way to do machine learning is to first come up with an expression for the probabilities p(yi|w,xi),
and then a numerical recipe for carrying out the minimization in eq. (6.35) to learn w. We will see
examples for how this can be implemented in chapter 8, chapter 14, and chapter 15. We summarize
this discussion in summary box 6.5.1.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>108 6 Densities and models
</p>
<p>Summary 6.5.1: Maximum likelihood framework
</p>
<p>Many machine-learning methods can be motivated within the following, general, framework.
Given data X,y, we select, based on expert knowledge, the likelihood density function:
</p>
<p>p(yi|w,xi). (6.36)
</p>
<p>Here, w are the parameters in the model. We then learn the weights w by letting them be
equal to the value w∗ found by either:
</p>
<p>Maximize: w∗ = arg max
w
</p>
<p>[
log p(w) +
</p>
<p>N∑
i=1
</p>
<p>log p(yi|xi,w)
]
</p>
<p>(6.37)
</p>
<p>Minimize: w∗ = arg min
w
</p>
<p>E(w) (6.38)
</p>
<p>where: E(w) = − 1
N
</p>
<p>N∑
i=1
</p>
<p>log p(yi|xi,w) + {Optional regularization term} .
</p>
<p>If we ignore either the prior or regularizations term (or they have the same analytically
form), these two formulations are equivalent, and can be derived using Bayes’ rule and the
maximum likelihood principle, see eq. (6.33) for further details.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>6.5 Bayesian learning in general 109
</p>
<p>Problems
</p>
<p>6.1. Fall 2014 question 8: A factory produces cars.
We consider three properties of the cars produced by the
factory and each property can only take two values:
</p>
<p>• The color which can be either red or blue.
• The weight which can be either heavy or light.
• The model which can be either 2-doors or 4-doors.
</p>
<p>There are thus 23 = 8 possible car types such as (red,
heavy,2-doors) or (blue,light,4-doors).
</p>
<p>Suppose you are given the following information
about cars produced from the factory:
</p>
<p>• The probability a car has four doors is 0.5
• The probability a car is heavy given it has four doors
</p>
<p>is 0.8
• The probability a car is heavy given it has two doors
</p>
<p>is 0.2
• The probability a car is heavy and red is 0.1
</p>
<p>Given the above information, what is the probability a
car is blue given it is heavy?
</p>
<p>A 0.2
B 0.5
C 0.8
D 0.9
E Don’t know.
</p>
<p>6.2. Spring 2013 question 17: In the study it was
found that
</p>
<p>• 88 pct. of the persons have normal semen.
• 12.5 pct. of the persons that have normal semen have
</p>
<p>had a childhood disease.
• 16.7 pct. of the persons that have abnormal semen
</p>
<p>have had a childhood disease.
</p>
<p>What is the probability that a person that has had a
childhood disease will have normal semen according to
the study?
</p>
<p>A 12.50 %
B 74.85 %
</p>
<p>C 84.59 %
D 88.00 %
E Don’t know.
</p>
<p>6.3. Fall 2013 question 15: Based on Haberman’s Sur-
vival Data found in Table 6.1 it is found:
</p>
<p>• 56 pct. of the subjects had positive axillary nodes
detected.
• 36 pct. of the subjects that had positive axillary
</p>
<p>nodes detected survived.
• 14 pct. of the subjects that did not have positive ax-
</p>
<p>illary nodes survived.
</p>
<p>What is the probability that a subject that has survived
would have positive axillary nodes according to the study
by Haberman?
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Young (&lt; 60 years), x1 = 0 or Age
Old (≥ 60 years), x1 = 1
</p>
<p>x2 Operated before, x2 = 0 or OpT
after 1960, x2 = 1
</p>
<p>x3 Positive axillary nodes detected PAN
No, x3 = 0 or Yes, x3 = 1
</p>
<p>y Lived after 5 years Surv
No, y = 0 or Yes, y = 1
</p>
<p>Table 6.1: A modified version of Haberman’s Survival
Data taken from http://archive.ics.uci.edu/ml/machine-
learning-databases/haberman/haberman.names. The at-
tributes x1-x3 denoting the age, operation time and can-
cer size as well as the output denoting survival after five
years are binary. The data contains a total of N = 306
observations.
</p>
<p>A 20.2 %
B 36.0 %
C 56.0%
D 76.6%
E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>7
</p>
<p>Data Visualization
</p>
<p>“The drawing shows me at one glance what might be spread over ten pages in a book.” wrote Ivan
S. Turgenev in 18621, thereby repeating the literary trope a picture is worth a thousand words. It
is worth reflecting on why this is so widely thought to be the case. One idea is that, simply put,
more of the brain is adapted to the direct processing of visual information than any other type of
sensory information. This in turn means we can distinguish between two senses a picture is worth a
thousand words: the first is it allows us to quickly comprehend and therefore learn new information
visually than from any other source and, the second, that it allows us to find and learn new patterns
in data; it is notable we often refer to this as seeing a new pattern.
</p>
<p>We will therefore decided to dedicate an entire chapter to visualizations, but split into two parts
reflecting these two senses of seeing: The first sections will treat to the classical use of visualizations,
namely communicating information to a reader. The remaining sections will re-visit the machine-
learning workflow of fig. 1.13, with a focus on the second use of using visualizations to see — namely
as a window into what machine-learning does. We will also use this as an oppertunity to discuss the
machine-learning workflow in slighly more details to provide a backdrop for the remaining sections
of this book.
</p>
<p>7.1 Basic plotting
</p>
<p>Visualization can be thought of as compressing a large quantity of information into a few visual
elements. This section will review some ways this can be accomplished roughly ordered according
to how much compression is desired.
</p>
<p>Visualization of a single attribute
</p>
<p>Consider each of the four attributes of the Fisher Iris dataset and suppose we wish to visualize
a single attribute. The histogram allows us to represent multiple observations in a limited space
while preserving nearly all the information. A histogram is constructed in two steps. The first step
is to divide the entire range of value of the variable into a series of intervals (referred to as bins),
</p>
<p>1 Quote taken from Fathers and Sons in http://www.phrases.org.uk/meanings/
a-picture-is-worth-a-thousand-words.html</p>
<p></p>
<div class="annotation"><a href="http://www.phrases.org.uk/meanings/a-picture-is-worth-a-thousand-words.html">http://www.phrases.org.uk/meanings/a-picture-is-worth-a-thousand-words.html</a></div>
<div class="annotation"><a href="http://www.phrases.org.uk/meanings/a-picture-is-worth-a-thousand-words.html">http://www.phrases.org.uk/meanings/a-picture-is-worth-a-thousand-words.html</a></div>
</div>, <div class="page"><p></p>
<p>112 7 Data Visualization
</p>
<p>0 2 4 6 8
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>0 2 4 6 8
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>0 2 4 6 8
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>0 2 4 6 8
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>30
</p>
<p>40
</p>
<p>Fig. 7.1: Histograms based on N = 16 bins of the four features in the Iris dataset.
</p>
<p>most often of equal length. We then count how many observations in the dataset fall within each
such bin and draw a rectangle where the base of the rectangle is the interval and the height is the
number of observations that fall into the interval. That is, the sum of the height of all rectangles
will be the number of observations. This procedure is also known as binning.
</p>
<p>In fig. 7.1 is shown the histograms of all four attributes of the Iris dataset. We see some of the
histograms look roughly symmetric and bell-shaped (this indicates the attribute is likely normally
distributed) whereas for instance the sepal width has two humps (it is multimodal). The advantage
of the histogram is that it tells us nearly all there is to know about a variable, the disadvantage is
that they take up quite a lot of space and that we have to select the number of bins manually and
too many or too few will create uninformative histograms. A more parsimonious representation of
the distribution of an attribute can be obtained with a boxplot. Boxplots of the four attributes of
the Fisher Iris data is shown in fig. 7.2 (left). Here, the middle red line corresponds to the median
(the p = 0.5 percentile), the upper and lower bounds, l75 and l25, of the blue box is the p = 0.75
and 0.25 percentile, the black lines are known as the whiskers and attempt to outline how wide the
distribution is. The upper/lower whiskers are defined as:
</p>
<p>upper whisker: min(l75 +
3
</p>
<p>2
(l75 − l25), vN ), (7.1)
</p>
<p>lower whisker: max(l25 −
3
</p>
<p>2
(l75 − l25), v1), (7.2)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>7.1 Basic plotting 113
</p>
<p>petal width petal length sepal width sepal length
0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>8
</p>
<p>petal width petal length sepal width sepal length
0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>Fig. 7.2: A boxplot (left pane) is a way to condense the information in a histogram into a stereo-
typical representation. An advantage of the boxplot is it allows us to read off relevant quantities
of the dataset, such as the medium value, however, compared to the histograms in fig. 7.1 we also
loose information such as the bimodality of the sepal width. Notice how these features affect the
symmetry of the boxplot. The right-most pane provides an example of how the same information
can be communicated with a simpler visual element, namely by simply plotting each observation
as a point and adding a bit of x, y jitter to make the points distinct.
</p>
<p>where vN denotes the value of the largest observation, and v1 the value of the smallest observation.
Observations that fall outside these bounds are marked as red crosses and they are said to be
outliers insofar as the boxplot is concerned. A similar effect to the boxplot and histogram can be
obtained with a bit of simple data-processing. In fig. 7.2 (right) we have simply plotted each value
of the attributes plus a bit of random noise applied to the x and y coordinate. This allows us to
distinguish each individual point and convey similar information as that found in the boxplot and
the histogram.
</p>
<p>Visualization of one-dimensional data
</p>
<p>Suppose we have to visualize a single 1d dataset, for instance, the sale of widgets produces by a
company over 12 months. The three most common ways to visualize this is shown in fig. 7.3 where
we have illustrated the line plot, a “dot” plot and a bar chart. Notice the bar chart start at 0 and
so should primarily be considered for variables which are ratio, i.e. 0 has some specific meaning.
The use of lines often help to “ground” the eye and provide guidance when there is correspondence
between observations whereas the dot and bar chart are easy to read and compare different values.
Also notice the bar chart and the other plots tend to guide the reader to different aspects of the
data.
</p>
<p>The first two plots would be useful for pointing out the variability of the data, whereas the bar
chart would be useful for pointing out the variability in absolute terms. Having in mind what we
want to communicate should always inform us about what graphical elements we choose and how
we decide to select or scale for instance the y-axis. In the bottom-right pane we have indicated the
chart we would likely prefer for this situation: We focus on the variability in the data and use a line
to indicate the months are connected while the large dots indicate the individual measurements and</p>
<p></p>
</div>, <div class="page"><p></p>
<p>114 7 Data Visualization
</p>
<p>Month
</p>
<p>W
id
g
et
s
so
ld
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12
</p>
<p>9500
</p>
<p>10000
</p>
<p>10500
</p>
<p>11000
</p>
<p>11500
</p>
<p>Month
</p>
<p>W
id
g
et
s
so
ld
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12
</p>
<p>9500
</p>
<p>10000
</p>
<p>10500
</p>
<p>11000
</p>
<p>11500
</p>
<p>Month
</p>
<p>W
id
g
et
s
so
ld
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12
</p>
<p>0
</p>
<p>5000
</p>
<p>10000
</p>
<p>15000
</p>
<p>Month
W
</p>
<p>id
g
et
s
so
ld
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12
</p>
<p>9500
</p>
<p>10000
</p>
<p>10500
</p>
<p>11000
</p>
<p>11500
</p>
<p>Fig. 7.3: Different attempts to visualize a simple 1D dataset corresponding to widget sale over
months. Top row is the line plot and the dot-plot. Bottom left is the bar plot (notice the y-axis
start at 0) and an attempt to combine the line and dot plot to better guide the readers eye.
</p>
<p>points out to the reader that we only have a month-by-month dataset with few observations. We
have also increased the line width slightly. In all four charts, we use grid lines to guide the reader’s
eye making it easier to compare the first months to the last.
</p>
<p>Several one-dimensional series
</p>
<p>Suppose we tests four different models on eight datasets and for each model we obtain a performance
rating. We believe our method (Method 1) is the better. How do we best communicate this? Of
course, we should include a table in the report. However, suppose we want to include a visualization
of this data in for instance a presentation or in the main pages of the report and wish to include
the table as an appendix. In fig. 7.4 are four attempts to visualize this dataset. Notice the three
methods we have seen so far are all fairly difficult to read. The lines cross many times for the
line plots, the dot plots too seem difficult to compare and the bar chart has an almost psychedelic
effect. One strategy to fix this is to sort the datasets in descending order. In this way, connecting
the datasets with lines makes the (relative) performance easier to read. It is now fairly apparent the
yellow method seems to have some benefits, especially for the medium-difficult problems whereas
the blue method seems to perform worse. We have also sorted the methods such that the first (best!)
method is first in the legend and in addition (try to zoom in) we make sure to plot the graphs such
that the first method are on top of the others. This is a rather subtle effect but it does make a
noticeable difference in terms of what the reader is focused on.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>7.1 Basic plotting 115
</p>
<p>Dataset
</p>
<p>A
cc
u
ra
cy
</p>
<p>1 2 3 4 5 6 7 8
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>Dataset
</p>
<p>A
cc
u
ra
cy
</p>
<p>1 2 3 4 5 6 7 8
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>Dataset
</p>
<p>A
cc
u
ra
cy
</p>
<p>1 2 3 4 5 6 7 8
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Method 1
</p>
<p>Method 2
</p>
<p>Method 3
</p>
<p>Method 4
</p>
<p>Dataset
</p>
<p>A
cc
u
ra
cy
</p>
<p>1 2 3 4 5 6 7 8
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>Fig. 7.4: Illustration of four 1D datasets corresponding to the performance of four machine-learning
methods on eight datasets. In the bottom right pane we have tried to sort the datasets and use
lines to connect related datasets. Which are easier to read?
</p>
<p>setosa
</p>
<p>versicolor
</p>
<p>virginica
</p>
<p>petal width
</p>
<p>p
et
a
l
le
n
g
th
</p>
<p>4 5 6 7 8
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>4
</p>
<p>4.5
</p>
<p>5
</p>
<p>Fig. 7.5: Scatter plot of two attributes of the
Fisher Iris data. Colors are used to visualize the
three classes.
</p>
<p>This is an example where arranging the data
is important for easier communication. In addi-
tion to what we have already done, one could
try to select a color or line scheme for the other
method that made them stand out less. For in-
stance by marking them all in graytone, which
would further emphasize that we were compar-
ing our method against three others.
</p>
<p>Visualizing higher-dimensional data
</p>
<p>Likely, the best and certainly the simplest way
to visualize 2D data is the scatter plot. In
fig. 7.5 we have plotted two coordinates of the
the Fisher Iris data against each other and used
colors to denote the classes. A 2D plot quickly
provides an overview of how spread out the data
is and in this case, it immediately tells us that
determining if a flower is setosa (as opposed to the two other types) is a trivial problem whereas the
other two classes, insofar as these two features are concerned, are more difficult to discern. When</p>
<p></p>
</div>, <div class="page"><p></p>
<p>116 7 Data Visualization
</p>
<p>5 6 7
</p>
<p>petal width
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>s
e
</p>
<p>p
a
</p>
<p>l 
le
</p>
<p>n
g
</p>
<p>th
</p>
<p>2 3 4
</p>
<p>petal length
</p>
<p>2 4 6
</p>
<p>sepal width
</p>
<p>1 2
</p>
<p>sepal length
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>s
e
</p>
<p>p
a
</p>
<p>l 
w
</p>
<p>id
th
</p>
<p>3
</p>
<p>4
</p>
<p>p
e
</p>
<p>ta
l 
le
</p>
<p>n
g
</p>
<p>th
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>8
</p>
<p>p
e
</p>
<p>ta
l 
w
</p>
<p>id
th
</p>
<p>setosa versicolor virginica
</p>
<p>Fig. 7.6: A matrix plot in which the four attributes are plotted pairwise against each other and
colors are used to indicate class labels
</p>
<p>making 2D scatter plots, be aware of the scaling of the axes; if the units of the axes are the same
(length) then it may be sensible to ensure they have the same scale.
</p>
<p>A difficulty in the 2D scatter plot is that we only see two dimensions at the same time. This can
(to some extend) be overcome by plotting all dimensions against each other in pairs constituting
what is known as a matrix plot, see fig. 7.6. An advantage of this type of plot is that we no longer
have to select two particular dimensions; a disadvantage is that this is only possible to display for
a limited number of attributes. What we perhaps learn from this plot is that sepal width and sepal
length may be two features useful for distinguishing versicolor and virginica, which may leave us
even more optimistic in terms of the classification problem. However, one cannot conclude that
because no two features in and by themselves can be used to separate two classes then the problem</p>
<p></p>
</div>, <div class="page"><p></p>
<p>7.1 Basic plotting 117
</p>
<p>1
</p>
<p>8
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>6
</p>
<p>5
</p>
<p>6
</p>
<p>8
</p>
<p>4 7
</p>
<p>6
</p>
<p>5
2 petal width petal length sepal width sepal length
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>setosa versicolor virginica
</p>
<p>Fig. 7.7: Higher-dimensional objects are difficult to visualize meaningfully. To the left is shown a
3D scatter plot where three attributes are plotted against each other. 3D plots are best when one
can interactively move the camera around since their 2D projection onto paper necessarily ruins
much of the information that can be extracted. For more dimensions some creativity is required,
for instance changing a 4D point to a line intersecting points given by coordinate number and the
corresponding value as shown to the right.
</p>
<p>is impossible to solve: Firstly, this tells us nothing about how three features can perform, and
secondly it only tells us about certain (axis-oriented) projections onto a 2D plane.
</p>
<p>Higher-dimensional observations
</p>
<p>To go beyond 2d requires either changing the visual element or accept some distortion. for instance
one can attempt a 3D scatter plot of the data where we consider three features together as shown
in fig. 7.7. The problem with 3D plots is that they have to be projected onto a 2D screen or paper
which ruins most of the benefits of the 3D plot. Another type of technique for high-dimensional
data is to represent each multi-dimensional observation by a more complex visual element than a
point. One such example is the coordinate plot illustrated in fig. 7.7 (right) where an observation
is represented by a line passing through the 4 points with coordinates (coordinate × value). As
a rule, one should consider using selection or projection of the data set before including plots of
high-dimensional observations.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>118 7 Data Visualization
</p>
<p>The question
</p>
<p>What do we want the reader to learn
Align this point with text, conclusion
A single question/point per figure
</p>
<p>The data
</p>
<p>What data is sufficient to answer the question
Exclude everything else
Select transformations, scaling, etc.
</p>
<p>The visual element
</p>
<p>As simple as possible
Let it be stand-alone (labels, titles, captions)
Use colors, markers, etc. insofar they add value
</p>
<p>Fig. 7.8: A visualizations is first and foremost about providing a truthful summary of the data, but
the clarity of a visualization can be greatly increased by carefully considering the following three
points: (i) what question are we trying to answer? i.e. what conclusion do we wish to convey to the
reader using the figure (ii) what is the minimal amount of data which will make that conclusion clear
and how should it be pre-processed (if necessary) (iii) what visual element, colors, arrangement,
etc. is more suitable.
</p>
<p>7.2 What sets apart a good plot?
</p>
<p>Having introduced the basic plots, a natural question to ask is when to use which plot type. Beyond
the basic requirement our visualizations should provide a truthful summary of the data there is no
single optimal answer to this question, however, there are useful guidelines2.
</p>
<p>A useful analogy is to consider technical writing. Suppose we are writing a section in a report.
What are the relevant questions to keep in mind? Arguably, the first, and most important, question
is what the point of the section actually is: What particular question are we hoping to answer? If
we are unsure about what point we are trying to convey, the text will only confuse the reader, and
we should be better of discarding the section entirely.
</p>
<p>When we know which point we wish to convey the next element is how the section should
address the question: Should we use examples? An abstract definition first and then illustrations?
Draw on other parts of the text? Perhaps begin by explaining the reader why he or she should care?
</p>
<p>After we have narrowed in on which question we want to answer, and how (in the broad picture)
this will be accomplished, we get to the low-level issue of putting our thoughts into well-structured
and readable English sentences. While this is arguably the least important aspect of writing3, it is
certainly important to ensure the text is enjoyable or, as a bare minimum, readable.
</p>
<p>For visualizations we can imagine a similar thought-process illustrated in fig. 7.8
</p>
<p>The question: The most important aspect of a visualization is what question the graphical ele-
ment will pose and answer. A figure should attempt to convey one (or a very few) interesting
facts to the reader and nothing more. It should be aligned with the main conclusions of the
text and be interesting enough to warrant the space.
</p>
<p>2 The guidelines illustrated here are adopted from http://junkcharts.typepad.com/junk_charts/
3 Or so we hope</p>
<p></p>
<div class="annotation"><a href="http://junkcharts.typepad.com/junk_charts/">http://junkcharts.typepad.com/junk_charts/</a></div>
</div>, <div class="page"><p></p>
<p>7.3 Visualizing the machine-learning workflowF 119
</p>
<p>The data: Next we should determine what data is useful to answer the question and possibly what
transformations should be applied to the data to (say) reduce noise, change scale, etc. The rule
is to go with the bare minimum of data transformations.
</p>
<p>The visuals: Lastly, what visual element (i.e. the type of plot) should be used to represent the
data and answer the question. Preference should be given to simplicity. Consider how to make
important visual feature stand out, use correct labels to guide the reader, etc.
</p>
<p>There are many answers as to how these steps4. However, the main point is that some thought is
given to the process of making visualizations. For instance, no person would hand in a report written
in a font that was unreadable. However, it is common to see plots where axis or labels are quite
literally impossible to read due to pixelization and poor font size choices. Errors of this kind, as
well as the related mistake of omitting labels to axis where their labeling is non-obvious, are easily
avoided with a minimum of afterthought.
</p>
<p>As a final recommendation, consider in the future to occasionally ask yourself if a particular
graphical element in a book or slide show works well (or not!) and ask yourself why and if any of
the ideas are worth copying.
</p>
<p>7.3 Visualizing the machine-learning workflowF
</p>
<p>In this section, we will focus our attention on how visualizations can help us at the various stages
of the machine-learning workflow we first encountered in fig. 1.13. Our hope is to convince the
reader that visualizations are useful when working with a practical machine-learning problem as
encountered later during the course and, therefore, building visualizations for our own benefit
should become a natural part of our machine-learning toolkit and workflow.
</p>
<p>As this is more of a general suggestion than a single, practical method we have chosen to make
the point early and at once rather than scattering it throughout the text. However, this means the
suggestions presented in this section will not in and by themselves be very helpful at this exact
moment, and that throughout the section we will, for illustrative purpose, refer to machine-learning
methods only introduced later. We emphasize a first-time reader is not supposed to understand the
methods fully at this point, and a reader should focus their reading on the general point we try to
convey rather than the specifics. Notice that this section (including subsections) is marked by a F
</p>
<p>indicating it is not necessary to understand the main text.
</p>
<p>7.3.1 Visualizations to understand loss
</p>
<p>To meaningfully distinguish between any two methods you need a well-defined goal. Therefore, begin
by figuring out a quantifiable way to express one model is preferable to another. Sometimes this
is trivial, but sometimes it is much harder: Suppose you are designing a method to automatically
re-stock a supermarket. Is the goal to ensure on average there are always n packs of ground beef
on the shelves, that on average no more than m packages of ground beef are discarded as waste, or
that no more than p percent of the customers are unable to buy the product they came for?
</p>
<p>A more concrete example is given in fig. 7.9 which illustrates how different choices of loss shifts
emphasis between outlier errors vs. typical error. The figure shows a simple prediction problem
</p>
<p>4 An interested reader can consult the work of Edward Tufte (Tufte et al. [1990], see also https://www.
edwardtufte.com/tufte/) or the ACCENT principles [Burn, 1993]</p>
<p></p>
<div class="annotation"><a href="https://www.edwardtufte.com/tufte/">https://www.edwardtufte.com/tufte/</a></div>
<div class="annotation"><a href="https://www.edwardtufte.com/tufte/">https://www.edwardtufte.com/tufte/</a></div>
</div>, <div class="page"><p></p>
<p>120 7 Data Visualization
</p>
<p>4 5 6 7 8 9 10 11
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>12
</p>
<p>14
</p>
<p>4 5 6 7 8 9 10 11
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>12
</p>
<p>14
</p>
<p>10 20 30 40
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>Fig. 7.9: Example of how the performance metric has a qualitative impact on the choice of model.
For the 2D dataset shown in the left-most pane, we consider a simple model (a line) but using
two performance metrics: One is the standard L2 error typically used in regression, the other is
the L1 error defined as the sum-of-distances between the points and the line. The middle figure
illustrates the best linear models using these two methods and the circles the relative contribution
of each point to the total error (the histogram in the right-hand pane are the same values sorted
in ascending order). The optimal model change qualitatively between a slightly positive/negative
trend because the L2 error is much more affected by outliers.
</p>
<p>where the interpolating line is selected so as to minimize the sum-of-distances between the line and
the observations, however, we try different measures of distance as defined by the Lp norm (see
eq. (4.17) in chapter 4).
</p>
<p>The figure illustrates the standard L2 error (square loss) and the L1 error (the sum-of-distances
to prediction) and how this results in qualitatively different predictions.
</p>
<p>A temptation is to take a wait-and-see attitude where several losses are computed, however this
encourage working in an ad-hoc manner where one is unable to reliably track progress. Alternatively,
it may seem impossible to find such a number, in which case one should wonder if the problem is
too poorly specified.
</p>
<p>Having a single, well-defined loss does not mean you cannot change your loss if it turns out to be
poorly selected, or you should not compute alternative metrics as you go along. Doing so provides
important information about what your method does and perhaps does wrong.
</p>
<p>7.3.2 Use visualizations to understand mistakes
</p>
<p>A simple but versatile idea is to visualize the outcome of the machine learning method. Suppose
we are trying to distinguish between cars and cities. A thing that can always be done is to plot
a meaningful number of members of these two classes to get an idea about what the images look
like. Even more useful, once a method is set up plot the images that are wrongly classified. This is
illustrated5 in fig. 7.10 where we have plotted 6 members of each class here assumed to be wrongly
classified: we can immediately notice some interesting facts, such as the “cars” class contains images
of the interior of cars, things that does not look like cars at all (lower, right), wrongly labeled images
(lower, left or upper, right) and a motorbike. Similarly the city images contains things that do not
look very city like; from this we can conclude (i) we should not expect our method to be 100%
accurate (ii) perhaps it is more useful to measure performance by looking at the top-5 predicted
labels as the labels are somewhat ambiguous.
</p>
<p>5 Images obtained from https://www.pexels.com</p>
<p></p>
<div class="annotation"><a href="https://www.pexels.com">https://www.pexels.com</a></div>
</div>, <div class="page"><p></p>
<p>7.3 Visualizing the machine-learning workflowF 121
</p>
<p>Images from “car” class mislabelled by method Images from “city” class mislabelled by method
</p>
<p>Fig. 7.10: A simple way to visualize the result of a method is to picture wrongly classified obser-
vations. For instance suppose the above 12 images show (wrongly) classified instances of the cars
and city class; this would tell us there are issues with the labelling (the motorcycle and truck),
as well as hint as a possible methodological problem of assigning a single label to an image. One
should perhaps also wonder if the method has a problem with very dark images; notice this could
be further illuminated with other visualizations.
</p>
<p>7.3.3 Visualization to debug methods
</p>
<p>Suppose your method does not work as well as you expect. Two recommended approaches is to
attempt to use a simpler version of the method, for instance by reducing the number of neurons
or layers in a neural network, or applying the method on a simplified version of the data set.
Assuming this seems to improve the performance, one should suspect something is going wrong in
the training of the method. Once more, we stress visualizations are the single most useful way to
understand what might go wrong. Simply put, extracting information about what the method does
internally should be the go-to technique, and doing so in a visual format is often the easiest and
most re-useable way. An example is shown in fig. 7.11; we assume an artificial neural network (see
chapter 15) is applied to a dataset and performs poorly. Neural networks often consist of thousands
(or millions) of weights which are hard to visualize, but in the figure we try to plot the output of
each layer as a histogram. We see that for the first layer, the histogram show a variety of activation
of the neurons indicating they maintain some information about the input. In the third and 10th
layer things look a bit askew: Neurons are now either fully on or off (-3 and 3) and we should wonder
why. Is this natural? could it be this is arbitrary? where does the number 3 come from? Finally in
layer 10 we see neurons all have nearly no activation, and once more we should wonder if this is
what we expect.
</p>
<p>More experience with neural networks will tell us a histogram such as layer 3 can be expected if
the layer weights are initialized to have a too high value, and layer 10 if they have a too low initial
value, how the true benefit of such a visualization is it allows us to learn such experience: it can
readily be re-used when the network is applied to a simpler dataset where it is perhaps known to
work well, and then we can see, even if highly approximately, what a good network is supposed to
look like. if all layers look more or less as the layer 1 histogram, this should inform us something
is going wrong, and we can begin to prod and poke the network to see what that is. Simply put,
when things go wrong, attempt to figure out the problem by writing code to see what goes wrong
rather than sit in the armchair and figure out what might be wrong.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>122 7 Data Visualization
</p>
<p>-1 0 1 2
</p>
<p>0
</p>
<p>50
</p>
<p>100
</p>
<p>150
</p>
<p>200
</p>
<p>-3 -2 -1 0 1 2 3
</p>
<p>0
</p>
<p>200
</p>
<p>400
</p>
<p>600
</p>
<p>800
</p>
<p>-3 -2 -1 0 1 2 3
</p>
<p>0
</p>
<p>200
</p>
<p>400
</p>
<p>600
</p>
<p>800
</p>
<p>Fig. 7.11: Even models with millions of parameters can be meaningfully visualized with some cre-
ativity and with the right visual element. This example illustrate the activity of the neurons in
three layers of a neural network. In the first layer, we see a broad distribution of activity indicating
the neurons are probably exhibiting varying degrees of activity for the input as we would hope. The
center and right-most pane show behaviour that should make us slightly suspicious: in the center
pane, neurons are either fully on or off (obviously, we should ask ourselves if the number 3 is what
we expect), and in the right-most pane nearly all neurons have no activity. Notice these histograms
are not supposed to be a gold-standard for visualizing networks, but an illustration of what can be
done with simple means.
</p>
<p>7.3.4 Use visualization for an overview
</p>
<p>Visualizations can often provide an immediate overview of what to expect in terms of performance.
When first encountering a machine-learning problem, we suggest a reader try to estimate two
quantities:
</p>
<p>The baseline performance
</p>
<p>The baseline performance (os simply baseline) refers to the performance of a naive method. A well-
chosen baseline should be quick to compute, simple and foolproof; think of it as what you would
do if you had 10 minutes to solve the problem. Examples could be:
</p>
<p>Classification Classify everything as belonging to the class with the most elements
Regression Output the mean of the sample (i.e. make a constant prediction)
Density estimation Return a uniform density, i.e. all outcomes are equally likely
Outlier detection Mark everything (or nothing) as outliers
</p>
<p>These examples are very naive and refers to the situation where we simply ignore X in our learning
problem and only focus on y. When working with more elaborate methods, for instance an artificial
neural network (see chapter 15), it is common to use a simple linear model (see chapter 8) as baseline.
Regardless of ones choice, the point of the baseline is twofold: First, to be able to recognize when a
method is not learning anything or very little. This may seem silly, but it is not at all uncommon
to encounter situations where a neural network is trained for hundreds of hours on giant datasets
and still only outperform a linear classifier with a few percent.
</p>
<p>The target, or ceiling, performance
</p>
<p>The baseline provides a lower bound of performance and the target (or ceiling) performance refers
to an upper bound. Such an upper bound may arise for different reasons depending on the situation:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>7.3 Visualizing the machine-learning workflowF 123
</p>
<p>Error: 0.10 Error: 0.14 Error: 0.17 Error: 0.09 Error: 0.15 Error: 0.07 Error: 0.19 Error: 0.13 Error: 0.16 Error: 0.18
</p>
<p>Error: 0.10 Error: 0.14 Error: 0.11 Error: 0.09 Error: 0.11 Error: 0.05 Error: 0.12 Error: 0.13 Error: 0.09 Error: 0.13
</p>
<p>Error: 0.14 Error: 0.14 Error: 0.19 Error: 0.10 Error: 0.17 Error: 0.07 Error: 0.22 Error: 0.21 Error: 0.18 Error: 0.21
</p>
<p>Error: 0.17 Error: 0.11 Error: 0.19 Error: 0.07 Error: 0.17 Error: 0.07 Error: 0.05 Error: 0.23 Error: 0.19 Error: 0.25
</p>
<p>Error: 0.09 Error: 0.09 Error: 0.10 Error: 0.07 Error: 0.09 Error: 0.05 Error: 0.10 Error: 0.09 Error: 0.08 Error: 0.08
</p>
<p>Error: 0.15 Error: 0.11 Error: 0.17 Error: 0.17 Error: 0.09 Error: 0.07 Error: 0.12 Error: 0.15 Error: 0.13 Error: 0.18
</p>
<p>Error: 0.07 Error: 0.05 Error: 0.07 Error: 0.07 Error: 0.05 Error: 0.07 Error: 0.04 Error: 0.07 Error: 0.06 Error: 0.07
</p>
<p>Error: 0.19 Error: 0.12 Error: 0.22 Error: 0.05 Error: 0.10 Error: 0.12 Error: 0.04 Error: 0.21 Error: 0.18 Error: 0.20
</p>
<p>Error: 0.13 Error: 0.13 Error: 0.21 Error: 0.23 Error: 0.09 Error: 0.15 Error: 0.07 Error: 0.21 Error: 0.19 Error: 0.23
</p>
<p>Error: 0.16 Error: 0.09 Error: 0.18 Error: 0.19 Error: 0.08 Error: 0.13 Error: 0.06 Error: 0.18 Error: 0.19 Error: 0.21
</p>
<p>Error: 0.18 Error: 0.13 Error: 0.21 Error: 0.25 Error: 0.08 Error: 0.18 Error: 0.07 Error: 0.20 Error: 0.23 Error: 0.21
</p>
<p>Fig. 7.12: Scatterplot of M = 11 attributes considered pairwise and colored according to class labels.
The inserted yellow line indicate the decicion boundary of a simple classification method, logistic
regression, and the corresponding errors are shown as inserts.
</p>
<p>For instance, it may refer to how well a human performs at the task, or how well we have to perform
the task for our machine-learning method to be useful, or the performance of a comparable method
from the literature. Alternatively, it may reflect some inherent noise in the problem, for instance
we cannot expect to predict a persons test score with higher accuracy than the inherent noise in
the test (such noise arise due to the limited number of questions or possibly arbitrariness of the
scoring), or predict the exact number of goals in a highly random game like soccer, or if 10% of the
observations in a dataset are mislabeled we should not expect to be more than 90% accurate6.
</p>
<p>6 A technical point: If the observations are mislabeled in a predictable manner, for instance all wolves are
mislabeled as dogs, we can learn to accurately predict the wrong label if that is all we have access to.
What we refer to here is the true class.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>124 7 Data Visualization
</p>
<p>Error: 0.02 Error: 0.03 Error: 0.02
</p>
<p>Error: 0.02 Error: 0.26 Error: 0.26
</p>
<p>Error: 0.03 Error: 0.26 Error: 0.24
</p>
<p>Error: 0.02 Error: 0.26 Error: 0.24
</p>
<p>Fig. 7.13: Continuing the example of fig. 7.12, the figure indicates the pairwise plots of a dataset
projected onto K = 4 PCA components. Note the first two components easily seperates the classes
and give rise to a very low error, whereas the other attributes appears to be fairly uninforma-
tive. A plot such as this may reveal important information about how relatively easy/difficult the
classification problem is.
</p>
<p>The point of the baseline and ceiling performance is to get an intuitive feeling for what our,
specific, performance means as well as how easy the problem is. Suppose we accurately predict if
an image contains either a dog or a cat 91% of the time, but 90% of the dataset is comprised of
dogs; then the performance improvement by using machine learning is about 1%, probably within
chance levels and we should not be too happy. On the other hand, suppose the dataset contains
100 classes, each with the same number of elements, then the naive baseline level is about 1% and
a performance of 15% mean the method is doing something (we will return to the point of class
imbalance in chapter 16).
</p>
<p>Example:
</p>
<p>To make this concrete we will consider a 12-feature classification problem7 where the goal is the
predict the wine type based on 12 different features.
</p>
<p>In fig. 7.12 we have made a scatter plot of each pair of feature and colored the two classes. If we
inspect the plots we can see several things: (i) some of the features (for instance feature 8 and 3)
pairwise allow good separation (ii) others (such as feature 11 and 4) does not (the two point clouds
are on top of each other). The lesson we can immediately draw from this illustration is the problem
is feasible and relatively easy; it is more or less a matter of finding the right pair of features.
</p>
<p>7 The Wine dataset was collected by Cortez et al. [2009] and obtained from http://archive.ics.uci.
edu/ml/datasets/Wine+Quality (note dataset has been processed to remove outliers).</p>
<p></p>
<div class="annotation"><a href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">http://archive.ics.uci.edu/ml/datasets/Wine+Quality</a></div>
<div class="annotation"><a href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">http://archive.ics.uci.edu/ml/datasets/Wine+Quality</a></div>
</div>, <div class="page"><p></p>
<p>7.3 Visualizing the machine-learning workflowF 125
</p>
<p>20 40 60 80 100
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>2 4 6 8 10 12
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>0 500 1000 1500
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>Fig. 7.14: Illustration of the relative difficulty of the classification task in fig. 7.12. Left: Dotted
lines indicate baseline model as well as a model trained on all features. The blue and red curves
indicates the classification error of each of the 55 plots in fig. 7.12; as shown these ranges from worse
than random guessing to nearly as good as using all the data. A similar plot for the PCA-projected
version of the same dataset is shown in fig. 7.13. The last figure illustrates how the method improves
(in terms of test error) as more data is used to train a linear model using all the features.
</p>
<p>On the other hand, suppose we only had access to the pair of features shown in the bottom-right
corner of fig. 7.12. In this case, we can see the two classes are so intermixed, and nothing suggests
they can be separated with any sort of rule, that we should at the very least conclude the problem
is very difficult and throwing one advanced method at the problem after another is unlikely to do
us any sgood.
</p>
<p>Obviously, for high dimensional data, eye-balling becomes infeasible and dimensionality reduc-
tion methods may be of use. In fig. 7.13 we have illustrated the data projected onto the first 4
principal components and again we clearly see the first two principal components would easily solve
the problem to a high accuracy.
</p>
<p>7.3.5 Illustration of baseline and ceiling performance
</p>
<p>The simplest baseline is obtained by classifying everything as belonging to the largest class. In
the wine example, such a baseline obtains an error of just 25%8; however the degree to which the
classes are separated in for instance the first subplot of fig. 7.13 suggests we can do a lot better.
For illustrative purpose we have visualized a simple, linear classifier (specifically, logistic regression,
which we will introduce in chapter 8) where everything on one side of the line is classified as
belonging to one class and everything on the other. The error of the classifier is shown for each
pair of coordinates and we see the best pair of PCA components obtain an error of just 2% on the
training set.
</p>
<p>The error of all linear classifiers using pairs of features are shown in fig. 7.14 (left and middle
figures). These plot show the baseline error 25% as well as the error obtained by training a model
on all features (about 2.5%). Visually inspecting the randomness in the data in fig. 7.12 an error
of 2.5% is probably quite close to the best we can hope to do on this dataset.
</p>
<p>Between these two quantities we have illustrated the performance of a linear classifier trained
on each pair of features on both a training and test set; we note the performance of these range
from abysmal (worse than the trivial baseline) to nearly as good as when using all features. For the
</p>
<p>8 Because the largest class contains 75% of the observations</p>
<p></p>
</div>, <div class="page"><p></p>
<p>126 7 Data Visualization
</p>
<p>0 50 100 150 200
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>0 50 100 150 200 250 300
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>0 200 400 600 800 1000
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>Fig. 7.15: While it is important to quantify model performance using a single number, visualizing
learning curves can often reveal important facts about our method including how much we should
trust these numbers as providing an absolute truth. The examples indicate seperate runs of two
method, and the performance of the methods would normally be computed after learning (at T =
200, 300, 1000 respectively) and compared using a standard test. Such a test would reveal method
1 perform better than 2 in the first two panes (p = 0.012 and p = 0.048) (T = 200, 300), however
inspecting the learning curve reveal such a conclusion would be highly spurious due to the non-
normal behaviour of method 2 (the jumps) as well as the error is obviously not stationary. In
T = 1000 we in fact see that method 2 seems to perform better than methdo 1 (p = 0.0071).
</p>
<p>PCA projections (middle) this is even more pronounced. What we learn from this example is the
wine dataset represents a fairly simple problem and a linear classifier should (and will) do well.
</p>
<p>7.3.6 Visualizing learning curves
</p>
<p>We previously argued it is important to quantify the performance of a given method as a single
number, however it is worth emphasizing that a single number can often hide important information
about the method which visualizations can make apparent.
</p>
<p>Example 1:
</p>
<p>Continuing our wine-example from the previous section, in fig. 7.14 (right pane) we have shown
how the error (on a training and test set) of a linear classifier depend on the number of training
observations. What we see is an important, general, feature of a well-functioning machine-learning
method: When using very few training observations, the method can fit the training data perfectly,
but does not learn to generalize to the test set (because there is insufficient data to learn the
true underlying statistical features of the problem; we say the method overfits). When more data
becomes available, the error on the training set actually increases slightly (because the training set
becomes more diverse and eventually exceeds the flexibility of a linear model), however the test
error (which, recall, is an estimate of the generalization error which is the quantity we are interested
in) drops. We will return to how such learning curves are interpreted in section 10.4.
</p>
<p>Example 2:
</p>
<p>A plot may provide insights which are not apparent from a statistical test, both because it is often
intuitively easier to judge a plot than a p-value, but also more importantly, that the distribution
of the error of our method may (and often, will) violate assumptions of the statistical test in more</p>
<p></p>
</div>, <div class="page"><p></p>
<p>7.3 Visualizing the machine-learning workflowF 127
</p>
<p>or less pronounced ways, thereby significantly weakening (or completely invalidating) the utility of
the statistical test.
</p>
<p>An example of this is shown in fig. 7.15. This curve illustrates the performance (here: accuracy,
higher is better) of two methods as a function of training time (it is assumed we are using a model
that benefits from more training time; a large neural network is an example of such a method). A
statistical test will show that at both time T = 200 and T = 300 method 1 outperforms method
2, however if we inspect the learning curves we see method 2 behaves differently than method 2; it
makes discrete “jumps”. This kind of behaviour is in fact quite common for methods which operate
on a discrete internal representation or in a discrete environment. At any rate, if we look at these
curves we should quickly draw the conclusions that
</p>
<p>• The error of method 2 is not normally distributed meaning our statistical test is only suggestive
• The need to evaluate method 2 longer and be aware performance of the two methods may shift
</p>
<p>depending on initial conditions
• there are good reasons to think tweaking of method 2, such as initial conditions or other learning
</p>
<p>parameters, can dramatically improve it’s relative performance.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>128 7 Data Visualization
</p>
<p>Problems
</p>
<p>7.1. Fall 2012 question 2: We will only use the at-
tributes x1–x10 as well as the output y in our model-
ing of the data. The attributes x1–x10 are standardized
(i.e., the mean has been subtracted each attribute and
the attributes divided by their standard deviations). In
Figure 7.16 a boxplot of the standardized data is given.
Which of the following statements is correct?
</p>
<p>Fig. 7.16: Boxplots of the 10 attributes x1–x10 where the
data has been standardized.
</p>
<p>A The value of the 50th and 75th percentiles of the
attribute DB coincides.
</p>
<p>B Even though the distribution of AlA and AsA may
have a similar shape this does not imply that the two
attributes are correlated.
</p>
<p>C The attribute TB is likely to be normal distributed.
D The attribute GDR has a clear outlier that should
</p>
<p>be removed.
E Don’t know.
</p>
<p>7.2. Fall 2014 question 2: A 1-dimensional dataset is
composed of N = 60 observations; exactly 40 of these
observations take the value 1, 10 take the value 2 and
the remaining 10 observations take the value 3. Which of
the four boxplots in fig. 7.17 is a boxplot of the dataset?
</p>
<p>DCBA
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>Fig. 7.17: Boxplots
</p>
<p>A Boxplot A
B Boxplot B
C Boxplot C
D Boxplot D
E Don’t know.
</p>
<p>7.3. Spring 2014 question 1: We will consider a
dataset on wholesale taken from http://archive.ics.
uci.edu/ml/datasets/Wholesale+customers. The
data set includes 440 customers. The customers are
from Lisbon and Oporto in Portugal as well as one
additional region here denoted Other. The data provides
the costumers’ annual expenditures in monetary units of
fresh products (FRESH), milk products (MILK), grocery
products (GROCERY), frozen products (FROZEN), de-
tergents and paper products (PAPER), and delicatessen
products (DELI). The attributes of the data and their
abbreviations are also given in Table 7.1.
</p>
<p>In Figure 7.18 is given a boxplot of the six input at-
tributes of the data. Which one of the following state-
ments is correct?</p>
<p></p>
<div class="annotation"><a href="http://archive.ics.uci.edu/ml/datasets/Wholesale+customers">http://archive.ics.uci.edu/ml/datasets/Wholesale+customers</a></div>
<div class="annotation"><a href="http://archive.ics.uci.edu/ml/datasets/Wholesale+customers">http://archive.ics.uci.edu/ml/datasets/Wholesale+customers</a></div>
</div>, <div class="page"><p></p>
<p>7.3 Visualizing the machine-learning workflowF 129
</p>
<p>Fig. 7.18: Boxplot of the six input attributes x1–x6 of
the wholesale data.
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Fresh products FRESH
x2 Milk products MILK
x3 Grocery products GROCERY
x4 Frozen products FROZEN
x5 Detergents and paper products PAPER
x6 Delicatessen products DELI
</p>
<p>y Region REGION
</p>
<p>Table 7.1: The six input attributes x1–x6 denoting the
annual consumption in monetary units of customers as
well as the output y denoting which of the three re-
gions; Lisbon, Oporto, and one additional region denoted
Other, the customers came from in the wholesale cus-
tomer data.
</p>
<p>A The boxplot contains prominent outliers that must
be removed.
</p>
<p>B All the attributes appear to be normal distributed.
C If we do not standardize the data (i.e., for each at-
</p>
<p>tribute subtract the mean and divide by the standard
deviation) a PCA would give equal importance to all
the attributes.
</p>
<p>D The mean and median values are not likely to be very
close to each other for any of the attributes.
</p>
<p>E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>Part II
</p>
<p>Supervised learning</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>8
</p>
<p>Introduction to classification and regression
</p>
<p>We will now turn our attention to supervised learning. In supervised learning we are given a training
set comprised of N observations, x1, . . . ,xN and N targets y1, . . . , yN and we wish to come up with
a way to predict y from x:
</p>
<p>y = f(x,w) + ε, (8.1)
</p>
<p>where w is a vector of tunable parameters and ε represents a noise term. Learning then consists
of selecting the parameters w based on the training data X,y. If y is a continuous parameter, for
instance the price of a stock, we will say that the model (denoted M) is a regression model. On
the other hand if y is discrete, i.e. y = 1, 2, . . . , C as in the MNIST example we encountered in
chapter 1, we will say M is a classification model. In this chapter, we will discuss the linear and
logistic regression models for regression and classification, starting by first explaining what f(x,w)
looks like and then show how probabilities can be used to treat the noise term ε.
</p>
<p>The history of linear regression can be traced back to mathematicians Adrien-Marie Legendre
and Carl Friedrich Gauss who independently in 1805 and 1809 applied linear regression models to
astronomical observations of the orbit of planets [Legendre, 1805, Gauß, 1809]. Meanwhile logistic
regression, which we will consider in a later section, has it’s origin with the discovery of the logistic
function by Pierre François Verhulst and Adolphe Quételetin in 1838 [Garnier and Quételet, 1838]
where it was originally applied to growth curves of populations.
</p>
<p>8.1 Linear models
</p>
<p>Despite having different goals, linear and logistic regression are closely related by virtue of using
a linear transformation of the input features which will be our natural starting point. Recall in a
linear model, the output y in eq. (8.1) is modelled as a linear combination of the input features:
</p>
<p>f(x,w) = w0 + w1x1 + · · ·+ wMxM , and x =
</p>
<p>
x1
x2
...
xM
</p>
<p> . (8.2)
The reason this is known as a linear model is that it is a linear function of the input features x. To</p>
<p></p>
</div>, <div class="page"><p></p>
<p>134 8 Introduction to classification and regression
</p>
<p>y = f(x,w)
</p>
<p>x
</p>
<p>y
</p>
<p>−1 −0.5 0 0.5 1 1.5 2
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>x1
x2
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>Fig. 8.1: The linear regression models prediction is a linear combination of the features f(x,w) =
w0 + w1x1 + · · · + wMxM . This allows for lines (left pane), y = w0 + w1x, planes (right pane)
y = w0 + w1x1 + w2x2 and in general hyperplanes.
</p>
<p>consider a very simple example, consider the linear model with w0 = 1, w1 = −1 shown in fig. 8.1
as the blue line
</p>
<p>y = f(x,w) = 1− x.
This naturally also extends to multiple input features. In the right-hand pane of fig. 8.1 is shown
the two-dimensional regression example with w0 = 0, w1 = 1, w2 =
</p>
<p>1
2 .
</p>
<p>y = f(x,w) = 0 + x1 +
1
</p>
<p>2
x2 = x1 +
</p>
<p>1
</p>
<p>2
x2.
</p>
<p>More generally, we can consider a feature transformation of x such that
</p>
<p>y = f(x,w) = w0 +
</p>
<p>M−1∑
j=1
</p>
<p>wjφj(x)
</p>
<p>where φ1(x), . . . , φM−1(x) are M − 1 basis functions. If we define
</p>
<p>φ(x) =
</p>
<p>
1
</p>
<p>φ1(x)
...
</p>
<p>φM−1(x)
</p>
<p>
(that is, the first basis function is just a constant) we can write the linear regression model more
compactly as simply
</p>
<p>y = f(x,w) = φ(x)Tw and w =
</p>
<p>
w0
w1
...
</p>
<p>wM−1
</p>
<p> . (8.3)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>8.1 Linear models 135
</p>
<p>y = f(x,w)
</p>
<p>x
</p>
<p>y
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>y = f(x,w)
</p>
<p>x
</p>
<p>y
</p>
<p>−10 −5 0 5 10
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>Fig. 8.2: Applying a non-linear transformation to the input x allows much more complicated curves
to be fitted by the linear regression model. In the left-hand pane is shown a polynomial y =
w0+w1x+w2x
</p>
<p>2+w3x
3 and in the right-hand pane a sinusoidal model y = w0+w1 cos(x)+w2 sin(4x).
</p>
<p>The use of non-linear basis functions allow the linear regression model to model non-linear features.
In fig. 8.2 is shown two such examples, the first is a 3rd degree polynomial corresponding to
</p>
<p>φ(x) =
[
1 x x2 x3
</p>
<p>]T
and w =
</p>
<p>[
1 −1 −1 2
</p>
<p>]T
. (8.4)
</p>
<p>The second example corresponds to two trigonometric functions suitable for a periodic signal
</p>
<p>φ(x) =
[
1 cos(x) sin(4x)
</p>
<p>]T
and w =
</p>
<p>[
1 −2 1
</p>
<p>]T
. (8.5)
</p>
<p>Since the transformation by a basis function does not change the linearity in w the discussion in
this chapter will be independent on the choice of basis functions. In practical terms, applying a basis
functions to a dataset X to obtain the transformed dataset X̃ is equivalent to applying feature
transformations such as the ones we encountered in chapter 2
</p>
<p>x̃i = φ(x), and X̃ =
</p>
<p>
φ(x1)
</p>
<p>T
</p>
<p>φ(x2)
T
</p>
<p>...
φ(xN )
</p>
<p>T
</p>
<p>
and we can write the prediction of observation i as yi = x̃
</p>
<p>T
i w.
</p>
<p>8.1.1 Training the linear regression model
</p>
<p>To learn the parameters of the linear regression model, we will follow the general procedure outlined
in section 6.5, in particular eq. (6.38). The first step of which is to come up with an expression for
p(y|x,w) (see eq. (6.36)).
</p>
<p>To this end, note the linear regression model eq. (8.3) is an ideal relationship between the input
x and the target y. An actual observation will be noisy which we will capture with a noise parameter
ε:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>136 8 Introduction to classification and regression
</p>
<p>y = f(x,w) + ε = x̃&gt;w + ε
</p>
<p>Since we don’t know what ε is, we have to model it with our only tool for handling unknown
quantities: probabilities. Therefore, assume that for each observation ε follows a normal distribution
with mean 0 and variance σ2. Using this assumption, the the probability density of y is then a normal
distribution centered around x̃&gt;w:
</p>
<p>p(y|x,w, σ) = N (y − x̃&gt;w|µ = 0, σ2)
</p>
<p>=
1√
2πσ
</p>
<p>e−
(y−x̃&gt;w)2
</p>
<p>2σ2 (8.6)
</p>
<p>Our objective (see eq. (6.31)) is to find w as the value which is most plausible given the data, i.e.
as maximizing p(w|X,y). Using Bayes’ theorem this can be written as
</p>
<p>p(w|X,y) = p(y|X,w)p(w)
p(y|X) . (8.7)
</p>
<p>As we saw earlier (see eq. (6.37)), this is equivalent to selecting w∗ as the value which maximizes
</p>
<p>w∗ = arg max
w
</p>
<p>[
log p(w) +
</p>
<p>N∑
i=1
</p>
<p>log p(yi|xi,w)
]
</p>
<p>(8.8)
</p>
<p>= arg max
w
</p>
<p>[
log p(w)− N
</p>
<p>2
log(2πσ2)− 1
</p>
<p>2σ2
</p>
<p>N∑
i=1
</p>
<p>(
yi − x̃&gt;i w
</p>
<p>)2]
. (8.9)
</p>
<p>We will now assume the prior of w is flat, p(w) = 1, which is also known as the uniform or improper
prior1, and furthermore we will choose to formulate the problem as a minimization problem by
dropping constant terms and re-scaling the above expression. The problem of finding w∗ is therefore
equivalent to minimizing the sum-of-squares error function (compare to eq. (6.38)):
</p>
<p>w∗ = arg max
w
</p>
<p>p(w|X,y) = arg min
w
</p>
<p>E(w)
</p>
<p>where E(w) =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>(
yi − x̃&gt;i w
</p>
<p>)2
. (8.10)
</p>
<p>As we know from analysis, this can be accomplished by setting the derivative of E equal to zero
and solving for w. If we differentiate eq. (8.10) with respect to a weight wj we obtain:
</p>
<p>∂
</p>
<p>∂wj
E(w) =
</p>
<p>2
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>(yi − x̃Ti w)X̃ij =
2
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>yiX̃ij −
2
</p>
<p>N
</p>
<p>[
N∑
i=1
</p>
<p>Xijx̃
T
i
</p>
<p>]
w (8.11)
</p>
<p>The gradient is therefore
</p>
<p>1 Notice the choice p(w) = 1 strictly speaking does not make sense since the density is no longer normalized:∫
p(w)dw =∞. The prior can however be understood as saying that no particular value of w is preferred
</p>
<p>over another or more formally it can be understood as using the prior p(w) = N (0, Iδ) throughout the
derivation and then taking the limit δ →∞.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>8.1 Linear models 137
</p>
<p>x
</p>
<p>y
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x1
</p>
<p>x2
</p>
<p>y
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−0.4
</p>
<p>−0.2
</p>
<p>0
</p>
<p>Fig. 8.3: Examples of two datasets for which we will apply linear regression. In the left-hand pane
is a 1-d dataset comprised of x, in the right-hand side a 2d dataset comprised of red dots lying on
a curved plane.
</p>
<p>∇E(w) =
</p>
<p>
∂E(w)
∂w1
</p>
<p>...
∂E(w)
∂wM
</p>
<p> = 2N X̃Ty − 2N (X̃T X̃)w. (8.12)
Setting the gradient equal to zero and solving we obtain
</p>
<p>w∗ = (X̃
T
X̃)−1X̃
</p>
<p>T
y = (X̃
</p>
<p>T
X̃)\X̃Ty. (8.13)
</p>
<p>Thus, training the linear regression model can be accomplished using one line of code in many com-
puting environments. Since the linear regression model is so basic and important we will illustrate
it in two scenarios in the following.
</p>
<p>Example 1: Linear regression applied to a 1d dataset
</p>
<p>Consider the 1d dataset shown in the left-pane of fig. 8.3. Suppose we wish to fit two models to
the dataset, one corresponding to plain linear regression and the other to feature transforming the
data to correspond to a second-degree polynomial.
</p>
<p>For the first order polynomial linear regression case, this is accomplished by applying the (iden-
tity) feature transformation:
</p>
<p>X̃(1) =
</p>
<p>
1 x1
1 x2
...
</p>
<p>...
1 xN
</p>
<p> (8.14)
then we compute w(1) = (X̃
</p>
<p>T
</p>
<p>(1)X̃(1))\X̃
T
</p>
<p>(1)y and and the red curve for an arbitrary point x can
</p>
<p>be predicted as y = f(x,w(1)) =
[
1 x
]
w(1). In the right-hand pane of fig. 8.4 we illustrate the
</p>
<p>second-degree polynomial linear regression corresponding to the feature transformation:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>138 8 Introduction to classification and regression
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 8.4: Examples of applying the linear regression model to the dataset shown in the left-hand pane
of fig. 8.3. The two panes respectively show a basic linear regression model y = X̃(1)w(1) without
feature transformations, and linear regression model with feature transformation by adding the
feature x2 to produce a second-polynomial curve, y = X̃(2)w(2). See text for details.
</p>
<p>X̃(2) =
</p>
<p>
1 x1 x
</p>
<p>2
1
</p>
<p>1 x2 x
2
2
</p>
<p>...
...
</p>
<p>1 xN x
2
N
</p>
<p> (8.15)
</p>
<p>then we compute w(2) = (X̃
T
</p>
<p>(2)X̃(2))\X̃
T
</p>
<p>(2)y and the red curve for an arbitrary point x can be
</p>
<p>predicted as y = f(x,w(2)) =
[
1 x x2
</p>
<p>]
w(2).
</p>
<p>Example 2: Linear regression applied to a 2d dataset
</p>
<p>In the second example, we will consider the 2d dataset shown in the right-hand pane of fig. 8.3.
We will again consider two models, one corresponding to plain linear regression and the other to
feature transforming the data to correspond to a second order Taylor expansion.
</p>
<p>In the left-hand pane of fig. 8.5 we illustrate the second-degree polynomial linear regression
corresponding to the (trivial) feature transformation:
</p>
<p>X̃(1) =
</p>
<p>
1 X11 X12
1 X22 X22
...
</p>
<p>...
...
</p>
<p>1 XN1 XN2
</p>
<p> , (8.16)
</p>
<p>then we compute w(1) = (X̃
T
</p>
<p>(1)X̃(1))\X̃
T
</p>
<p>(1)y (notice w is three-dimensional) and for an arbitrary
</p>
<p>point x we can predict y = f(x,w(1)) =
[
1 x1 x2
</p>
<p>]
w(1). This is used to generate the plane shown
</p>
<p>in the left-hand pane of fig. 8.5.
In the second example, we will attempt to fit a second-order expansion to the same dataset.
</p>
<p>This is accomplished by the feature transformation:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>8.2 Logistic Regression 139
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−0.4
</p>
<p>−0.2
</p>
<p>0
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−0.4
</p>
<p>−0.2
</p>
<p>0
</p>
<p>Fig. 8.5: Examples of applying the linear regression model to the dataset shown in the righ-hand
pane of fig. 8.3. The left-hand pane shows the basic linear regression model y = X̃(1)w, and in the
right-hand pane we make a feature transformation to include second order terms corresponding to
y = X̃(2)w. See text for details.
</p>
<p>X̃(2) =
</p>
<p>
1 X11 X12 X
</p>
<p>2
11 X
</p>
<p>2
12 X11X12
</p>
<p>1 X21 X22 X
2
21 X
</p>
<p>2
22 X21X22
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>...
...
</p>
<p>1 XN1 XN2 X
2
N1 X
</p>
<p>2
N2 XN1XN2
</p>
<p> (8.17)
</p>
<p>Again, learningw (which is now six-dimensional!) can be accomplished asw(2) = (X̃
T
</p>
<p>(2)X̃(2))\X̃
T
</p>
<p>(2)y
</p>
<p>and predictions, as shown in the right-hand pane of fig. 8.5, for a new point x =
[
x1 x2
</p>
<p>]T
can be
</p>
<p>made as
y = f(x,w(2)) =
</p>
<p>[
1 x1 x2 x
</p>
<p>2
1 x
</p>
<p>2
2 x1x2
</p>
<p>]
w(2).
</p>
<p>In the later case the found value of w(2) is
</p>
<p>w(2) =
[
−0.5 0.5 0.5 −0.25 −0.25 −0.125
</p>
<p>]T
,
</p>
<p>which is exactly (at this precision at least) equal to the value of w used to generate the data.
</p>
<p>8.2 Logistic Regression
</p>
<p>The goal of classification and regression may seem very different, however, it turns out linear
regression can easily be extended to classification by the use of probabilities. Consider a binary
classification task where y = 0 corresponds to the negative class and y = 1 to the positive class. We
will now proceed exactly as we did in the case of linear regression by first finding an expression for
</p>
<p>p(y|x,w) (8.18)
</p>
<p>and apply the maximum likelihood framework from section 6.5.
Since y is binary, our immediate idea should be to model it’s density eq. (8.18) as a Bernoulli
</p>
<p>variable. However, as the output of a linear model is a general continuous number, and the parameter</p>
<p></p>
</div>, <div class="page"><p></p>
<p>140 8 Introduction to classification and regression
</p>
<p>z
σ
(z
)
</p>
<p>−10 −5 0 5 10
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 8.6: The logistic sigmoid σ(z) = (1 + e−z)−1. Notice as z → −∞ then σ(z) → 0 and when
z →∞ then σ(z)→ 1.
</p>
<p>θ of the Bernoulli distribution belongs to the unit interval [0, 1], we re-parameterize the Bernoulli
distribution using the sigmoid function. Recall that according to eq. (5.30) from section 5.4.3 the
Bernoulli density could be written as:
</p>
<p>p(b|z) = Bernouilli(b|θ = σ(z)) = σ(z)b(1− σ(z))1−b, σ(z) = 1
1 + e−z
</p>
<p>. (8.19)
</p>
<p>Where the function σ(z) is the logistic sigmoid and is shown in fig. 8.6. The way we will proceed is
to define z as being equal to the output of a standard linear model x̃&gt;w. Specifically, define:
</p>
<p>ŷi = σ(x̃
&gt;
i w)
</p>
<p>The probability density of a given observation yi is then:
</p>
<p>p(yi|xi,w) = Bernouilli(yi|ŷi) = ŷyii (1− ŷi)1−yi . (8.20)
</p>
<p>If we then proceed exactly as in the case of the linear regression model, by using eq. (8.20) and
eq. (6.38), we see we should select the parameters w∗ as the solution of the optimization problem:
</p>
<p>w∗ = arg min
w
</p>
<p>E(w)
</p>
<p>where: E(w) = − 1
N
</p>
<p>log
</p>
<p>[
N∏
i=1
</p>
<p>p(yi|xi,w)
]
</p>
<p>= − 1
N
</p>
<p>N∑
i=1
</p>
<p>[yi log ŷi + (1− yi) log(1− ŷi)] , ŷi = σ
[
x̃&gt;i w
</p>
<p>]
, (8.21)
</p>
<p>However, at this point our discussion has to departs from linear regression: there is no closed-
form analytical solution for the minimum of the error function. How we find w∗ in practice will
be discussed later in the chapter for neural networks where we will consider a general method
for minimizing error functions, however, until then simply rely on the commands build into your
computing environment for solving logistic regression problems.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>8.2 Logistic Regression 141
</p>
<p>Negative, yi = 0
</p>
<p>Positive, yi = 1
</p>
<p>X̃w
</p>
<p>x
</p>
<p>ŷ
=
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>−2 0 2 4 6 8 10
</p>
<p>−10
</p>
<p>0
</p>
<p>10
</p>
<p>20
</p>
<p>Negative, yi = 0
</p>
<p>Positive, yi = 1
</p>
<p>ŷ
</p>
<p>x
</p>
<p>ŷ
=
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>−2 0 2 4 6 8 10
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 8.7: A one-dimensional logistic regression example. The left-hand pane shows the intermediate
(linear) output Xw and the right-hand pane the true logistic-regression decision boundary ŷ =
σ(Xw).
</p>
<p>As mentioned, all the tricks of feature-transforming X also “work” for logistic regression and
we will therefore only consider two simple examples where X has not had feature-transformations
applied to it asides being pre-fixed with 1s as is required for any regression problem. In fig. 8.7
is shown a basic logistic regression example for a simple 1-dimensional dataset. The procedure is
exactly similar to linear regression. We first define:
</p>
<p>X̃(1) =
</p>
<p>
1 x1
1 x2
...
</p>
<p>...
1 xN
</p>
<p> (8.22)
Then, the left-hand pane shown the intermediate linear regression value z = X̃(1)w as the black
</p>
<p>line, and the right-hand pane the predicted probabilities ŷ = σ(X̃(1)w) are plotted as a black
line. As expected for such a simple problem the logistic regression learns how to separate the two
classes. For completeness, fig. 8.8 illustrates a 2d example, where the two classes are fitted with a
logistic regression model and the decision surface ŷ is shown. In the example, the class-membership
probabilities are computed as:
</p>
<p>ŷi = σ(x̃
&gt;
i w) = σ
</p>
<p>([
1 Xi1 X21
</p>
<p>]
w
)
.
</p>
<p>Notice the decision boundary is linear and quite steep. Logistic regression will have linear decision
boundaries unless we apply (non-linear) feature transformations to our dataset.
</p>
<p>8.2.1 The confusion matrix
</p>
<p>While the error function E(w) could be used to evaluate a logistic regression model it is important to
keep in mind that the error function measures the probability of the data, however, at the end of the
day, we are probably more interested in how often the logistic regression model classifies correctly
and how often it classifies wrongly. To this end, we must turn the output of the logistic regression</p>
<p></p>
</div>, <div class="page"><p></p>
<p>142 8 Introduction to classification and regression
</p>
<p>y = 1
</p>
<p>y = 0
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 −1 0 1 2 3
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>y = 1
</p>
<p>y = 0
</p>
<p>x1x2
</p>
<p>ŷ
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 8.8: 2d logistic regression example. The dataset shown in the left-hand pane is fitted with a
logistic regression model and the class-membership prediction ŷ is shown in the right-hand pane.
</p>
<p>(which is a probability) into a class label. This can be accomplished by simply thresholding at 0.5
(we will return to how the threshold should be selected in chapter 10) and predict that observation
i belongs to the positive class if ŷi &gt;
</p>
<p>1
2 and the negative class if ŷi ≤ 12 .
</p>
<p>There are now four different combinations of what class an observation actually belongs to and
what it is predicted to belong to by the model. They are called:
</p>
<p>True Positives, TP: Number of observations which are in fact positive yi = 1 which the classifier
correctly labels as positive ŷi &gt;
</p>
<p>1
2
</p>
<p>False Positives, FP: Number of observations which are in fact negative yi = 0 which the classi-
fier incorrectly labels as positive ŷi &gt;
</p>
<p>1
2
</p>
<p>False Negatives, FN: Number of observations which are in fact positive yi = 1 which the clas-
sifier incorrectly labels as negative ŷi &lt;
</p>
<p>1
2
</p>
<p>True Negatives, TN: Number of observations which are in fact negative yi = 0 which the
classifier correctly labels as negative ŷi &lt;
</p>
<p>1
2
</p>
<p>These are illustrated in fig. 8.9. In the left-hand pane is shown a binary classification problem of
N = 10 observations where the colors indicate the predictions made by a logistic regression model
(blue corresponds to the positive class and red to the negative). In the right-hand pane the true
positives, false negatives, etc. are collected in what is known as the confusion matrix, where the
inserted ticks on colored background indicate which observations counts towards which numbers.
As mentioned we will return to the meaning of these numbers in more detail in chapter 10, however,
for now we will simply focus on how often the classifier is right which is known as the accuracy (or
equivalently how often the classifier is wrong which is known as the error rate):
</p>
<p>Accuracy =
TP + TN
</p>
<p>N
, Error rate =
</p>
<p>FP + FN
</p>
<p>N
= 1−Accuracy.
</p>
<p>For instance the accuracy of the logistic regression model in fig. 8.9 is 5+2N =
7
10 .</p>
<p></p>
</div>, <div class="page"><p></p>
<p>8.3 The general linear modelF 143
</p>
<p>Positive class
</p>
<p>Negative class
</p>
<p>0 0.2 0.4 0.6
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>TP = 5
</p>
<p>TN = 2
</p>
<p>FN = 1
</p>
<p>FP = 2
</p>
<p>N = 10
</p>
<p>Actually
Positive
</p>
<p>Actually
Negative
</p>
<p>Predicted
Positive Negative
</p>
<p>Predicted
</p>
<p>(False Positive) (True Negative)
</p>
<p>(False Negative)(True Positive)N+ = 6
</p>
<p>N− = 4
</p>
<p>Fig. 8.9: (Left:) A small N = 10 observation binary classification problem. The colors indicate the
prediction made by a logistic regression classifier obtained by thresholding ŷi at
</p>
<p>1
2 . (Right:) The
</p>
<p>confusion matrix of the classifier in the left-hand pane. The inserts (ticks on background) indicate
which observations counts towards which numbers in the confusion matrix.
</p>
<p>8.3 The general linear modelF
</p>
<p>The overall form of the cost-function in the linear and logistic regression models can be generalized
into what is known as the general linear model. Since this is the form of these models which is
mostly commonly encountered in a computing environment, we will briefly discuss it here. Basically,
it decompose the model into two parts: a link function g and a cost function d. It then assumes the
output of the model is:
</p>
<p>y = f(x,w) = g(x̃&gt;w)
</p>
<p>and that the cost function can be written as:
</p>
<p>E(w) =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>d(yi, g(x̃
&gt;
i w)) (8.23)
</p>
<p>Parameters are found in the usual way by minimizing E(w). For a summary, see method 8.3.1.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>144 8 Introduction to classification and regression
</p>
<p>Method 8.3.1: The general linear model
</p>
<p>Given a dataset consisting of N pairs (xi, yi), we first define x̃i as a feature-transformation
of xi, in the simplest form obtained by pre-fixing xi with a constant intercept term:
</p>
<p>x̃i =
[
1 x&gt;i
</p>
<p>]&gt;
.
</p>
<p>The predicted output ŷ, cost function E and learned parameters w∗ in a general linear model
(GLM) are then defined as:
</p>
<p>ŷ = g(x̃&gt;w)
</p>
<p>E(w) =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>d(yi, g(x̃
&gt;
i w))
</p>
<p>w∗ = arg min
w
</p>
<p>E(w).
</p>
<p>The linear regression model can be recovered by defining
</p>
<p>g(x̃&gt;w) = x̃&gt;w and d(y, ŷ) = (y − ŷ)2
</p>
<p>and the logistic regression model as:
</p>
<p>g(x̃&gt;w) = σ(x̃&gt;w) and d(y, ŷ) = −y log ŷ − (1− y) log(1− ŷ).</p>
<p></p>
</div>, <div class="page"><p></p>
<p>8.3 The general linear modelF 145
</p>
<p>Problems
</p>
<p>8.1. Spring 2013 question 12: We fit a linear regres-
sion model to the PM10 data shown in Table 8.1. The in-
put attributes are standardized (i.e., we have subtracted
the mean of each input attribute, x1–x7, and divided by
their standard deviations) whereas the output logPM10
is kept in its original format. We obtain the following
model:
</p>
<p>f(x) = 3.27 + 0.36x1 − 0.01x2 − 0.19x3
+ 0.01x4 + 0.05x7.
</p>
<p>Which one of the following statements about the model
is incorrect?
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Logarithm of number logCAR
of cars per hour
</p>
<p>x2 Temperature 2 meter TEMP
above ground (degree Celsius)
</p>
<p>x3 Wind speed (meters/second) WIND
x4 Temperature difference between TEMPDIF
</p>
<p>25 and 2 meters (degree Celsius)
x5 Wind direction WINDDIR
</p>
<p>(degrees between 0 and 360)
x6 Whole hour of the day HOUR
x7 Day number from DAY
</p>
<p>October 1. 2001
</p>
<p>y Logarithm of PM10 logPM10
concentration
</p>
<p>Table 8.1: The attributes of the PM10 data. The out-
put is given by the hourly values of the logarithm of the
concentration of PM10 particles (logPM10).
</p>
<p>A According to the model WINDDIR and HOUR are
not relevant for predicting the pollution level.
</p>
<p>B According to the model fewer cars and more wind
will result in lower pollution levels.
</p>
<p>C According to the model it seems that pollution is
decreasing over time.
</p>
<p>D According to the model higher temperatures will re-
sult in lower pollution levels.
</p>
<p>E Don’t know.
</p>
<p>8.2. Spring 2014 question 5: We consider the Whole-
sale dataset shown in Table 8.2 and wish to predict
whether a consumer is from Lisbon (y=0) or Oporto
(y=1) by discarding observations from the Other region
included in the wholesale data. After discarding the ob-
servations pertaining to the Other region we standardize
the attributes x1–x6 (i.e., for each attribute subtract the
mean and divide by the standard deviation) and fit a lo-
gistic regression model. We obtain the following model
for the prediction of the origin of the consumer:
</p>
<p>f(x1, x2, x3, x4, x5, x6) = logit(−0.51− 0.11x1 − 0.36x2
+0.44x3 + 0.39x4 + 0.09x5 − 0.28x6),
</p>
<p>where logit(w) = 11+exp(−w) is the logit function. Which
</p>
<p>one of the following statements about the model is cor-
rect?
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Fresh products FRESH
x2 Milk products MILK
x3 Grocery products GROCERY
x4 Frozen products FROZEN
x5 Detergents and paper products PAPER
x6 Delicatessen products DELI
</p>
<p>y Region REGION
</p>
<p>Table 8.2: The six input attributes x1–x6 denoting the
annual consumption in monetary units of customers as
well as the output y denoting which of the three re-
gions; Lisbon, Oporto, and one additional region denoted
Other, the customers came from in the wholesale cus-
tomer data.
</p>
<p>A According to the model it seems that people in Lis-
bon buy more FRESH products, MILK products and
DELI products than people in Oporto.
</p>
<p>B According to the model if a costumer after the stan-
dardization has x1 = x2 = x3 = x4 = x5 = x6 = 0
the customer is more likely to come from Oporto than
Lisbon.
</p>
<p>C The logit function will return the probability a per-
son is from Lisbon.
</p>
<p>D From the model it can be seen that FRESH and PA-
PER are unimportant and should be removed in or-
der to avoid overfitting.
</p>
<p>E Don’t know.
</p>
<p>8.3. textbfFall 2013 question 6: We consider the Galapa-
pos dataset shown in Table 8.3 fit with a linear regression
model which predicts the area of an island x3 based on
the remaining attributes, i.e. x1, x2, x4, x5, x6, x7. We ob-
tain the following model for the prediction of an islands
area (x3) using the raw untransformed attributes that
are plotted in Figure 8.10:
</p>
<p>f(x1, x2, x4, x5, x6, x7) = 63.4 + 4.3x1 − 34.7x2
+ 3.0x4 − 7.2x5 − 1.4x6 − 0.5x7
</p>
<p>Which one of the following statements about the model
is correct?</p>
<p></p>
</div>, <div class="page"><p></p>
<p>146 8 Introduction to classification and regression
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Number of plant species Plants
x2 Number of endemic plant species E-Plants
x3 Area of island (in km
</p>
<p>2) Area
x4 Max. elevation above sea-level (in m) Elev
x5 Distance to nearest island (in km) DistNI
x6 Distance to Santa Cruz Island (in km) StCruz
x7 Area of adjacent island (in km
</p>
<p>2) AreaNI
</p>
<p>Table 8.3: The seven attributes of the data on a selection
of 29 of the Galápagos islands.
</p>
<p>Fig. 8.10: Boxplot of the seven input attributes of the
Galápagos data.
</p>
<p>A According to the model AreaNI is irrelevant for pre-
dicting the Area of islands.
</p>
<p>B According to the model it seems that the closer the
neighboring island is the larger area the island has.
</p>
<p>C According to the model endemic plants is the most
important predictor of island area.
</p>
<p>D According to the model an island that is highly ele-
vated and close to Santa Cruz Island will in general
be predicted to be relatively small.
</p>
<p>E Don’t know.
</p>
<p>8.4. Fall 2014 question 22:
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>x
2
</p>
<p>x1
</p>
<p>Class 1
</p>
<p>Class 0
</p>
<p>x
2
</p>
<p>x1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 8.11: top pane: Observed data. Bottom pane: Esti-
mated probability of belonging to class 1 according to
the logistic regression classifier.
</p>
<p>Recall the logistic function is defined as
</p>
<p>logistic(z) =
1
</p>
<p>1 + e−z
.
</p>
<p>Suppose logistic regression classifier is trained on obser-
vations from two classes as shown in the top pane of
fig. 8.11 and the trained classifier produces an estimate
of the probability of belonging to class 1 as shown in the
bottom pane. If the classifier takes the following form
</p>
<p>f(x1, x2) = logistic(w0 + w1x1 + w2x2 + w3x1x2)
</p>
<p>what are the values of w0, w1, w2, w3?
</p>
<p>A w0 = 2, w1 = w2 = 0, w3 = 10.
B w0 = −2, w1 = w2 = 0, w3 = −10.
C w0 = −2, w1 = w2 = 1, w3 = 10.
D w0 = 2, w1 = w2 = 1, w3 = −10.
E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>9
</p>
<p>Tree-based methods
</p>
<p>In this section, we will consider tree-based methods for classification or regression. The goal of tree-
based methods is the same as above: In classification, we wish to predict a discrete output label yi
for observation i based on features xi, and in regression, we wish to predict a continuous-valued yi.
However it is accomplished quite differently than linear or logistic regression, in that we consider
the value of yi as being determined by asking a series of questions organized as a tree about xi
and then, based on the answers, assign yi a constant value. Decision trees were originally developed
by Earl B. Hunt (and coauthors) in 1966 in his Concept Learning System where the construction
of the sequence of questions was intended to model human concept acquisition [Hunt et al., 1966].
However, today decision trees have grown to be an important yet simply supervised learning model.
In the next sections, we will introduce regression and classification trees as well as discuss how they
can be learned using Hunt’s algorithm.
</p>
<p>Table 9.1: Animals dataset. A dataset of N = 15 observations and M = 4 binary features where
the goal is to predict if the animal is a Mammal or not.
</p>
<p>Name x1: Cold Blooded x2: Has Legs x3: Lay Eggs x4:Has Fur y: Mammal
</p>
<p>Snake yes - yes - -
Starfish yes - yes - -
Bluebird - yes yes - -
Blackbird - yes yes - -
Earthworm yes - yes - -
Chameleon yes - yes - -
Ant yes - yes - -
Jellyfish yes - yes - -
Snail yes - yes - -
Sea Urchin yes - yes - -
Dolphin - - - - yes
Rat - yes - yes yes
Dog - yes - yes yes
Monkey - yes - yes yes
Lion - yes - yes yes</p>
<p></p>
</div>, <div class="page"><p></p>
<p>148 9 Tree-based methods
</p>
<p>Algorithm 2: Hunt’s algorithm for decision trees
</p>
<p>Require: Initial tree T only containing the root node
Require: Dr : Dataset associated with the current branch. Initially just the full dataset
</p>
<p>if The stop criterion is met then
Add a leaf node to the tree which assigns every observation to the most prevalent class in Dr
</p>
<p>else
Try a number of different splits on Dr. For each split, compute the purity gain and select the split
Dr = {Dv1 , . . . , DvK} with the highest purity gain
Recursively call the method on Dv1 , . . . , DvK
</p>
<p>end if
</p>
<p>9.1 Classification trees
</p>
<p>Consider the dataset in table 9.1 comprised of N = 15 animals. For each animal, we have recorded
M = 4 features (cold blooded, has legs, lay eggs, and has fur) and we wish to build a classifier which
determines y, if the animal is a mammal or not. Of course this corresponds to our usual situation
where we are given a matrix X and a vector y, however, for the moment we will limit ourselves to
the case where X is binary and consider the general case later.
</p>
<p>The decision tree can then be constructed using what is known as Hunt’s algorithm and is
outlined in fig. 9.1. We first place all 15 animals at the root of the tree (top left pane) and consider
a binary yes/no question (we will discuss how these questions are chosen later) for instance “cold
blooded?”. This question partitions the 15 animals into two new groups (top right pane); since one
group (the cold blooded animals) are all non-mammals they are classified as such in a leaf node
and we say this node is pure. The other group consists of a mixture of animals and so we ask a new
question: “lay eggs?” in the bottom-right pane. This partitions the animals into two new groups
and since they only contain mammals or non-mammals (i.e. they are pure) the method terminates.
</p>
<p>The general procedure, Hunt’s algorithm for decision-tree induction, is a simple recursive ap-
plication of the same yes/no questioning procedure we just illustrated on the animal dataset. In
general we will consider splits which are not just binary but multi-way. In fig. 9.2 we consider 5
example splits for different attribute types. For binary variables, we are limited to simple yes/no
split (however there is one such potential split for each attribute type!) and for discrete or contin-
uous values we can potentially consider splits into K branches. We then assume we at every step
in the procedure has access to many potential splits and select the best split based on the purity
gain that we will discuss shortly. The method terminated for the animal dataset when a branch
contained only one type of animal, however, in general we will stop when a general stop criterion
is met. The full method can be found in algorithm 2.
</p>
<p>9.1.1 Impurity measures and purity gains
</p>
<p>So how do we determine when one question is better than another? In fig. 9.3 we have outlined two
potential root-node questions. In the left pane we ask if the animals have legs, and in the right pane
we ask if it has fur. There are generally two components to a good question: Firstly, how balanced
the question is. If the question is very specific, then one branch will only contain very few animals
and the question will therefore not be very informative. If we consider the left-pane of fig. 9.3, and
we denote the root of the tree by r and the two branches by v1 and v2, the left-most branch of the</p>
<p></p>
</div>, <div class="page"><p></p>
<p>9.1 Classification trees 149
</p>
<p>Starfish
</p>
<p>Bluebird
</p>
<p>Blackbird
</p>
<p>Earthworm
</p>
<p>Chameleon
</p>
<p>SnakeAnt
</p>
<p>Jellyfish
</p>
<p>Snail
</p>
<p>Sea-urchin
</p>
<p>Dolphin
</p>
<p>Rat Dog
</p>
<p>Monkey
</p>
<p>Lion
</p>
<p>Bluebird
</p>
<p>BlackbirdDolphin
</p>
<p>Rat
Dog
</p>
<p>Monkey
</p>
<p>Lion
</p>
<p>Starfish
</p>
<p>Earthworm
</p>
<p>Chameleon Snake
</p>
<p>Ant
</p>
<p>Jellyfish
</p>
<p>Snail Sea-urchin
</p>
<p>Bluebird
</p>
<p>Blackbird
</p>
<p>Dolphin
</p>
<p>RatDog
</p>
<p>Monkey Lion
</p>
<p>YesNo
</p>
<p>Non-Mammals
</p>
<p>Starfish
</p>
<p>Earthworm
</p>
<p>Chameleon Snake
</p>
<p>Ant
</p>
<p>Jellyfish
</p>
<p>Snail Sea-urchin
</p>
<p>Starfish
</p>
<p>Earthworm
</p>
<p>Chameleon Snake
</p>
<p>Ant
</p>
<p>Jellyfish
</p>
<p>Snail Sea-urchin
</p>
<p>Bluebird
</p>
<p>Blackbird
</p>
<p>Dolphin
</p>
<p>Rat
</p>
<p>Dog
</p>
<p>Monkey Lion
</p>
<p>Fig. 9.1: Construction of a decision tree using Hunt’s algorithm to classify whether an animal is a
mammal or not. Initially, all observations are assigned to the root (top left pane) and we consider
a question. The question divides the animals into two sets, if a particular set is pure, i.e. only
contains mammals (or non-mammals) the method terminates (top right), else we recursively apply
the method (bottom left). The method terminates in the bottom-right pane because all leaf branches
are pure. In the general method, we consider many question at each branch, and select the best
according to it’s purity gain .
</p>
<p>tree, v1, contains N(v1) = 7 animals whereas the right-most branch contains N(v2) = 8 animals.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>150 9 Tree-based methods
</p>
<p>ContiniousDiscreteBinary
</p>
<p>Shirt SizeHas Legs Age
</p>
<p>{XS,S}
</p>
<p>{M}
{L,XL,XXL}
</p>
<p>{XS,S,M} {L,XL,XXL}Yes
No
</p>
<p>&gt; 30≤ 30
</p>
<p>&lt; 10
</p>
<p>[10; 20[
[20; 35]
</p>
<p>&gt; 35
</p>
<p>Fig. 9.2: Different types of attributes allow different splits. Binary attributes only allow binary
(yes/no) splits, whereas discrete and continuous attributes allow either binary splits or many-way
splits.
</p>
<p>Starfish
</p>
<p>Bluebird Blackbird
</p>
<p>Earthworm Chameleon
</p>
<p>Snake
</p>
<p>Ant
</p>
<p>Jellyfish Snail Sea-urchin
</p>
<p>Dolphin
</p>
<p>Rat
</p>
<p>Dog
</p>
<p>MonkeyLion
</p>
<p>Starfish
</p>
<p>Earthworm
</p>
<p>Snake Jellyfish
</p>
<p>Snail
</p>
<p>Sea-urchin
</p>
<p>Dolphin
</p>
<p>Bluebird
</p>
<p>Blackbird
</p>
<p>Chameleon
</p>
<p>Ant Rat
</p>
<p>Dog
</p>
<p>Monkey
</p>
<p>Lion
</p>
<p>Starfish
</p>
<p>Bluebird
</p>
<p>Blackbird
</p>
<p>Earthworm
</p>
<p>Chameleon
</p>
<p>Snake
</p>
<p>Ant
</p>
<p>Jellyfish
</p>
<p>Snail
</p>
<p>Sea-urchinDolphin
</p>
<p>Rat
</p>
<p>Dog
</p>
<p>Monkey
</p>
<p>Lion
</p>
<p>N(r) = 15
</p>
<p>p(M |r) = 2
3
</p>
<p>N(r) = 15
</p>
<p>p(M |r) = 2
3
</p>
<p>N(v1) = 11
</p>
<p>p(M |v1) = 111
</p>
<p>N(v2) = 4
</p>
<p>p(M |v2) = 1
</p>
<p>N(v2) = 8
</p>
<p>p(M |v2) = 12
N(v1) = 7
</p>
<p>p(M |v1) = 17
</p>
<p>Has Legs?
</p>
<p>Fig. 9.3: Two different potential splits (left and right pane). The splits divide the animals at the root
r into different groups corresponding to the left v1 and right v2 branch. When choosing between
two splits, we are interested in how balanced they are (i.e. if N(v1) ≈ N(v2)) and to what extend
it is pure i.e. only contain one class as measured with the within-class probabilities p(M |v1) and
p(M |v2). From these quantities we can compute the purity gain ∆.
</p>
<p>The question is thus fairly balanced compared to the right-most branch where we have N(v1) = 11
and N(v2) = 4.
</p>
<p>On the other hand, we also want the split into different groups of animals to be as pure as
possible, that is to contain (preferably) only one kind of animal. In the left-pane of fig. 9.3 the left-
branch v1 contains only one mammal (i.e. the probability the animal is a mammal in this branch is</p>
<p></p>
</div>, <div class="page"><p></p>
<p>9.1 Classification trees 151
</p>
<p>p(M |v1) = 17 ) and in the right-branch p(M |v2) = 12 . However, the fur-split in the right hand pane
is much more pure since the probability of a mammal in the left-branch is p(M |v1) = 111 and in the
right-pane p(M |v2) = 1.
</p>
<p>A measure of how good a question is should therefore consider both how balanced the question
is and how pure the resulting classes are. This can be accomplished by first quantifying how impure
the classes are at the root and at the K potential branches using an impurity measure. We will
write this as I(r) for the impurity at the root and I(v1), I(v2), . . . , I(vK) for the impurity at the
branches. These impurities for each of the branches are weighted by the fraction of observations
at the branch and combined to compute the overall impurity after the split. By contrasting the
impurity before the split to the overall impurity after the split we obtain the purity gain ∆ for the
question, which is given by the formula
</p>
<p>∆ = I(r)−
K∑
k=1
</p>
<p>N(vk)
</p>
<p>N(r)
I(vk). (9.1)
</p>
<p>A high purity gain indicates that the impurity of the individual splits, I(v1), . . . , I(vK) is low, i.e.
the classes have become more pure relative to the root impurity I(r). The weighting by the fraction
N(vk)
N(r) is used to make the measure focus on the larger (important) groups in the split. This only
</p>
<p>requires us to specify the impurity. The impurity function I(v) only depends on the (relative) size
of the classes in the given branch v, i.e. the probabilities p(c|v). If in general we consider there are
C classes, we have C such probabilities in each branch p(c = 1|v), . . . , p(c = C|v) (in the animals
example we had two classes corresponding to C = 2 and p(M |v), p(not M |v)). The following three
are popular choices for the impurity function I(v); entropy, Gini and classification error:
</p>
<p>Entropy(v) = −
C∑
c=1
</p>
<p>p(c|v) log2 p(c|v), (9.2)
</p>
<p>Gini(v) = 1−
C∑
c=1
</p>
<p>p(c|v)2, (9.3)
</p>
<p>ClassError(v) = 1−max
c
p(c|v). (9.4)
</p>
<p>Here log2 p(c|v) is the base-2 logarithm. Suppose we consider the example in the right pane of
fig. 9.3 and we use the ClassError impurity measure we obtain:
</p>
<p>I(r) = 1− 2
3
</p>
<p>=
1
</p>
<p>3
, I(v1) = 1−
</p>
<p>10
</p>
<p>11
=
</p>
<p>1
</p>
<p>11
and I(v2) = 1− 1 = 0. (9.5)
</p>
<p>We can then compute the purity gain as
</p>
<p>∆ = I(r)− 11
15
I(v1)−
</p>
<p>4
</p>
<p>15
I(v2) =
</p>
<p>1
</p>
<p>3
− 1
</p>
<p>15
=
</p>
<p>4
</p>
<p>15
. (9.6)
</p>
<p>As mentioned, the purity gain can easily be computed for many types of splits and having
multiple classes. In fig. 9.4 we consider a three-way split (K = 3) for N(r) = 12 objects (the colored
balls) corresponding to a total of C = 4 true classes. In the example, the impurity gain would be
</p>
<p>∆ = I(r)− 5
12
I(v1)−
</p>
<p>1
</p>
<p>4
I(v2)−
</p>
<p>1
</p>
<p>3
I(v3). (9.7)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>152 9 Tree-based methods
</p>
<p>v3
</p>
<p>v1 v2
</p>
<p>N(r) = 12
</p>
<p>N(v3) = 4
</p>
<p>N(v2) = 3
</p>
<p>N(v1) = 5 p(Y |v3) = p(B|v3) = 14
p(R|v3) = 12 p(G|v3) = 0
</p>
<p>p(B|v2) = 23 p(R|v2) =
1
3
</p>
<p>p(Y |v2) = p(G|v2) = 0
</p>
<p>p(Y |v1) = 35 , p(B|v1) = 0
</p>
<p>p(G|v1) = p(R|v1) = 15
</p>
<p>p(R|r) = p(Y |r) = 1
3
</p>
<p>p(G|r) = 1
12
</p>
<p>p(B|r) = 1
4
</p>
<p>r
</p>
<p>Fig. 9.4: A multi-way split where N = 12 observations belonging to C = 4 classes are split in a
K = 3 way split. The classes are indicated by the colors. See text for details on how the purity gain
∆ can be computed from the given numbers.
</p>
<p>In practice, we are of course primarily interested in applying classification trees to the case
where X contains general continuous features. The splits most often considered are binary, two-
way splits obtained by considering each of the M features of X and then attempting different
possible split-values:
</p>
<p>xm &lt; x
∗ (9.8)
</p>
<p>where x∗ is the split-value varied over the range of the observed data points. This gives a very large
number of potential splits, each being axis aligned. This is illustrated for a 2D dataset in fig. 9.5. We
start by considering all binary splits x1 &lt; x
</p>
<p>∗ and x2 &lt; y
∗ where x∗ and y∗ are varied. The split with
</p>
<p>the highest purity gain is selected and indicated by the colors in the top-left pane. The method
is applied recursively for each of the two new splits. Again all axis-aligned splits are considered
and two splits are selected giving a tree with four leaf nodes (top-right pane). This procedure is
continued recursively in the bottom row and the method terminates when it encounters pure classes.
</p>
<p>9.1.2 Controlling tree complexity
</p>
<p>Hunt’s algorithm terminates if it encounters pure splits, i.e. the current set of observations in a leaf
only contains one class. However, it is often a good idea to terminate the method earlier. Consider
for instance fig. 9.5 bottom-right pane where in order to place all observations in a pure leaf node
the method creates some rather odd-looking boxes. In general, there are two strategies for ensuring
this does not happen.
</p>
<p>Early stopping
</p>
<p>The simplest way to control the complexity of the tree is to stop Hunt’s algorithm before it encoun-
ters pure splits. There are several criteria for stopping:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>9.1 Classification trees 153
</p>
<p>Fig. 9.5: Construction of a decision tree using Hunt’s algorithm. We consider binary K = 2 splits
where we as candidate splits consider if x1 (and x2) is less than or greater than a sequence of
split-values. In the top-left pane we have selected an initial split based on the value of the x-axis.
Hunt’s algorithm is then applied recursively (top right pane) to produce two additional splits, then
it is applied recursively on the non-pure groups (bottom left) and after several steps produces the
final split in the bottom-right pane. Notice the final configuration likely overfits the data.
</p>
<p>• Stop splitting when a branch contains less than a specific number of observations.
• Stop splitting if a certain depth of the tree is reached.
• Stop splitting if purity gain ∆ for the best split is below a certain value.
</p>
<p>This of course leaves open the question of how we should select for instance the minimum number
of observations in a branch. For now simply assume it is selected manually - in chapter 10 we will
consider how cross-validation can be used to solve this problem.
</p>
<p>Pruning*
</p>
<p>Early stopping is simple to implement, but comes with an important disadvantage known as the
horizon effect. Simply put, since early stopping stops growing the tree at some point, we can’t
know if continuing growing the tree by just one node beyond that point would have resulted in a
significant reduction in error.
</p>
<p>Pruning tries to get around this problem by first growing a full tree with no (or very little)
early stopping and then afterwards select which branches in the tree should be replaced by a single
leaf (i.e. should be pruned). How the pruned subtrees are selected differ from pruning strategy to
pruning strategy but a simple strategy is cost complexity pruning [Breiman et al., 1984].
</p>
<p>In cost complexity pruning, we construct a series of trees T0, T1, · · · , Tm where T0 is the initial
(full) tree produced by Hunt’s algorithm and Tm is a tree only consisting of the root. Each tree Ti</p>
<p></p>
</div>, <div class="page"><p></p>
<p>154 9 Tree-based methods
</p>
<p>Algorithm 3: Cost complexity pruning of decision trees
</p>
<p>Require: Initial full tree T0 produced by Hunt’s algorithm
Require: T0, T1, · · · , Tm, a sequence of increasingly more pruned trees
</p>
<p>for i = 1, 2, · · · and Ti is not only the root do
for each subtree t of Ti−1 do
</p>
<p>Compute the cost-complexity error corresponding to collapsing tree t:
Ct =
</p>
<p>E(Prune(T,t))−E(T )
|T |−|Prune(T,t)|
</p>
<p>end for
Let t be the subtree of Ti−1 which minimizes Ct
Set Ti = Prune(T,t)
</p>
<p>end for
</p>
<p>x
</p>
<p>y
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>Fig. 9.6: A simple 1D example regression data set containing N = 100 observations.
</p>
<p>is constructed from Ti−1 by first trying to collapse each subtree t of Ti−1 into a single node and
for the collapsed tree compute the cost-complexity tradeoff which measures the relative increase in
error per removed node; the intuition being that the removal of many nodes should be favored over
the removal of a single node. Once the cost-complexity has been computed for each internal branch
the branch with the lowest cost-complexity is collapsed producing Ti. Algorithmically the method is
shown in algorithm 3. Once the sequence T0, · · · , Tm has been produced, the tree Ti with the lowest
generalization (i.e. test) error is selected. How to estimate the generalization error is discussed in
chapter 10 as two-layer cross-validation.
</p>
<p>9.2 Regression trees
</p>
<p>In regression, yi for an observation i is no longer discrete but continuous. The decision tree
method may however very easily be altered to accommodate for this change. Suppose we consider
the animal example again, but this time we wish to predict the animals mean life span. We can ask
exactly the same questions, but then in order to predict the mean life span yi for a given branch,
say for instance the right-most branch in the right-most pane of fig. 9.3, we would simply predict
the mean value of the animals in that branch:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>9.2 Regression trees 155
</p>
<p>Algorithm 4: Hunt’s algorithm for regression trees
</p>
<p>Require: Initial tree T only containing the root node
Require: Dr : Dataset associated with the current branch. Initially just the full dataset
</p>
<p>if The stop criterion is met then
Add a leaf node to the tree which assigns every observation the mean value of the nodes in Dr:
y(r) = 1
</p>
<p>N(r)
</p>
<p>∑
i∈r yi
</p>
<p>else
Try a number of different splits on Dr. For each split, compute the purity gain using the
sum-of-squares impurity measure and select the split Dr = {Dv1 , . . . , DvK} with the highest purity
gain
Recursively call the method on Dv1 , . . . , DvK
</p>
<p>end if
</p>
<p>Predicted y-value in v2 : y(v2) =
yRat + yLion + yDog + yMonkey
</p>
<p>4
(9.9)
</p>
<p>=
</p>
<p>∑
i∈v2 yi
</p>
<p>N(v2)
(9.10)
</p>
<p>To evaluate the goodness of a new split, we can now simply compute the average sum-of-squares
error between the observed yi’s and the mean value y(v2). This can be done by simply introducing
a new impurity measure:
</p>
<p>I(v) =
1
</p>
<p>N(v)
</p>
<p>∑
i∈v
</p>
<p>(yi − y(v))2 where y(v) =
1
</p>
<p>N(v)
</p>
<p>∑
i∈v
</p>
<p>yi, (9.11)
</p>
<p>and then simply use Hunt’s algorithm as already introduced where the stopping criteria may for
instance be that the purity gain (or the impurity) falls below a certain value. The algorithm is very
similar to algorithm 2 but for completeness it is listed in algorithm 4.
</p>
<p>We will consider this method applied to the simple 1-d regression problem in fig. 9.6, the result
can be seen in fig. 9.7. We again consider recursive, binary splits. In the first iteration of the
algorithm, all observations are assigned to the same branch and we simply predict the mean value
of all observations (top left pane). Then, the optimal split is selected as the split which increases
the purity gain the most and we split the x-values once at the dotted vertical line to produce two
predicted y-values (right pane). This procedure is applied recursively to give two splits at the next
level (bottom left) and finally four splits (bottom right). As can be seen, this method very quickly
allows for a flexible but piece-wise constant prediction of y.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>156 9 Tree-based methods
</p>
<p>x
</p>
<p>y
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>x
</p>
<p>y
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>x
</p>
<p>y
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>x
</p>
<p>y
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>Fig. 9.7: Application of the regression tree method to the 1d example. First, all observations are
assigned a constant y-value (top left pane). Then we consider various splits (along the x-axis) and
select the one with the highest purity gain computed using the sum-of-squares impurity measure
(top right). The method is applied recursively on each section until a stopping criterion is met
(bottom row).</p>
<p></p>
</div>, <div class="page"><p></p>
<p>9.2 Regression trees 157
</p>
<p>Problems
</p>
<p>9.1. Fall 2013 question 14: We consider a dataset on
survival of breast cancer taken from http://archive.
ics.uci.edu/ml/machine-learning-databases/
</p>
<p>haberman/haberman.names. The data has been bina-
rized as outlined in Table 9.2. The dataset contain a
total of 306 observations. In the dataset 81 survived af-
ter 5 years (i.e., y = 1) whereas 225 died (i.e., y = 0).
We would like to build a decision tree and consider using
positive axillary nodes detected (PAN) as an attribute
condition at the root of the tree. We thereby split ac-
cording to whether positive axillary nodes were detected
and find:
</p>
<p>• For the 170 subjects that had positive axillary nodes
detected 62 survived.
• For the 136 subjects that did not have positive axil-
</p>
<p>lary nodes 19 survived.
</p>
<p>What is the gain, ∆, of splitting according to whether
a subject had positive axillary nodes (PAN) using
the Gini as impurity measure I(t), (i.e., I(t) = 1 −∑C−1
i=0 p(i|t)2)?
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Young (&lt; 60 years), x1 = 0 or Age
Old (≥ 60 years), x1 = 1
</p>
<p>x2 Operated before, x2 = 0 or OpT
after 1960, x2 = 1
</p>
<p>x3 Positive axillary nodes detected PAN
No, x3 = 0 or Yes, x3 = 1
</p>
<p>y Lived after 5 years Surv
No, y = 0 or Yes, y = 1
</p>
<p>Table 9.2: A modified version of Haberman’s Survival
Data taken from http://archive.ics.uci.edu/ml/machine-
learning-databases/haberman/haberman.names. The at-
tributes x1-x3 denoting the age, operation time and can-
cer size as well as the output denoting survival after five
years are binary. The data contains a total of N = 306
observations.
</p>
<p>A -0.025
B 0
C 0.025
D 0.036
E Don’t know.
</p>
<p>9.2. Fall 2013 question 13: We will use the decision
tree given in Figure 9.9 to attempt to solve the classi-
fication problems given to the right of Figure 9.9 corre-
sponding to the classification problem also considered in
Figure 9.8. Which one of the following choices for the two
decisions A and B in the decision tree would be the most
well suited to separate the two classes?
</p>
<p>Fig. 9.8: The decision boundaries given in white and gray
of four different classifiers used to separate red crosses
from black circles.
</p>
<p>Fig. 9.9: A decision tree with two decisions denoted A
and B that if given the right decisions can be used to per-
fectly separate the red crosses from black circles given in
the classification problem to the right that was also con-
sidered in Figure 9.8.
</p>
<p>A A=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.25
</p>
<p>]
‖∞ &lt; 0.25
</p>
<p>B=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.5
</p>
<p>]
‖2 &lt; 0.25
</p>
<p>B A=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.25
</p>
<p>]
‖1 &lt; 0.25
</p>
<p>B=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.5
</p>
<p>]
‖2 &lt; 0.25
</p>
<p>C A=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.25
</p>
<p>]
‖∞ &lt; 0.25
</p>
<p>B=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.5
</p>
<p>]
‖1 &lt; 0.25</p>
<p></p>
<div class="annotation"><a href="http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.names">http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.names</a></div>
<div class="annotation"><a href="http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.names">http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.names</a></div>
<div class="annotation"><a href="http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.names">http://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.names</a></div>
</div>, <div class="page"><p></p>
<p>158 9 Tree-based methods
</p>
<p>D A=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.25
</p>
<p>]
‖2 &lt; 0.5
</p>
<p>B=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.5
</p>
<p>]
‖∞ &lt; 0.5
</p>
<p>E Don’t know.
</p>
<p>9.3. Fall 2014 question 9: Consider a one-dimensional
data set of features X and 3-class responses y shown in
table 9.3; there are thus N = 7 observations.
</p>
<p>X 1 3 1 2 1 4 2
</p>
<p>y 2 2 2 0 0 1 0
</p>
<p>Table 9.3: Table of data and responses.
</p>
<p>Suppose a decision tree is used to classify y on X.
Consider an attempted split at X = x1 &gt; 2.5. What is
the impurity gain ∆ of this split for the data set if the
classification error is used as impurity measure?
</p>
<p>A ∆ = 0.123
B ∆ = 0.143
C ∆ = 0.239
D ∆ = 0.428
E Don’t know.
</p>
<p>9.4. Fall 2014 question 21:
</p>
<p>
</p>
<p>
</p>
<p>Class 1
</p>
<p>Class 0
</p>
<p>x
2
</p>
<p>x1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.8
</p>
<p>−0.6
</p>
<p>−0.4
</p>
<p>−0.2
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 9.10: Two-class classification problem
</p>
<p>Class 1 Class 0 Class 0Class 1
</p>
<p>A
</p>
<p>B C
</p>
<p>(true)
</p>
<p>(true)(true) (false)(false)
</p>
<p>(false)
</p>
<p>Fig. 9.11: Decision tree with 3 nodes A, B and C
</p>
<p>Suppose we wish to solve the two-class classification
problem in fig. 9.10 using a classification tree of the
form fig. 9.11. What rules, acting on the coordinates
x = (x1, x2), should be assigned to the three internal
nodes A,B and C for the tree to give rise to the indi-
cated decision boundary?
</p>
<p>A A : ‖x‖2 ≥ 12 , B :
∥∥∥∥x− [−11
</p>
<p>]∥∥∥∥
∞
&gt; 1
</p>
<p>C :
</p>
<p>∥∥∥∥x− [ 1−1
]∥∥∥∥
</p>
<p>1
</p>
<p>&gt; 2
</p>
<p>B A : ‖x‖2 ≥ 12 , B :
∥∥∥∥x− [ 1−1
</p>
<p>]∥∥∥∥
1
</p>
<p>&gt; 2
</p>
<p>C :
</p>
<p>∥∥∥∥x− [−11
]∥∥∥∥
∞
&gt; 1
</p>
<p>C A :
</p>
<p>∥∥∥∥x− [ 1−1
]∥∥∥∥
</p>
<p>1
</p>
<p>&gt; 2, B :
</p>
<p>∥∥∥∥x− [−11
]∥∥∥∥
∞
&gt; 1
</p>
<p>C : ‖x‖2 ≥ 12
D A :
</p>
<p>∥∥∥∥x− [ 1−1
]∥∥∥∥
</p>
<p>1
</p>
<p>&gt; 2, B : ‖x‖2 ≥ 12
</p>
<p>C :
</p>
<p>∥∥∥∥x− [−11
]∥∥∥∥
∞
&gt; 1
</p>
<p>E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10
</p>
<p>Overfitting and performance evaluation
</p>
<p>For some practitioners of machine learning the most interesting aspect is devising new and exciting
algorithms and words such as “evaluation” or “testing” is likely to be treated as an afterthought.
However, testing and quantifying the performance of machine-learning methods is possibly the most
important aspect of data modelling.
</p>
<p>Suppose we are in a situation where we has S different models M1, . . . ,MS that each tries to
solve a particular supervised learning problem. Without an objective way of comparing the models
we will not know which to choose. Sure, we might feel we should select modelMS which is the most
complicated of the models, but that is not an objective justification. Worse yet, if we are working
in a company, it will be impossible to quantify if progress is being made at solving the problem or
if there is any benefits for the company to have a machine learning department at all.
</p>
<p>Seen in this way quantification (and comparison) of model performance is something a machine-
learning practitioner should be obsessively preoccupied with. In this chapter, we will discuss common
issues with model performance evaluation and provide the industry standard, cross-validation, for
estimating the generalization error which allow us to evaluate a given models performance and
thereby select between different models. The chapter will finish with a discussion of how the gener-
alization error provides more qualitative information about the goodness of a given model.
</p>
<p>10.1 Cross-validation
</p>
<p>The principal way of comparing and validating models is by cross-validation. In this chapter, we
will introduce the reasoning behind cross-validation and discuss important applications of cross-
validation. We will use the simple linear regression model as a running example.
</p>
<p>10.1.1 A simple example, linear regression
</p>
<p>To provide a concrete example, consider the simple regression problem in fig. 10.1 where the goal is
to predict y from x and we have access to 9 data points collected in a training data set Dtrain. The
data consists of noisy observations of the black curve and and we wish to fit a regression model to
the data. We assume we have access to three different models</p>
<p></p>
</div>, <div class="page"><p></p>
<p>160 10 Overfitting and performance evaluation
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 10.1: A small dataset of nine observations generated from the true curve shown in the black line.
The three red lines are three different linear regression models M1,M2,M3 fitted to the dataset.
Clearly the most complicated model M3 fits the dataset best, however, the second model M2 is
better suited to account for the true black curve.
</p>
<p>Training error
</p>
<p>Model Mk
</p>
<p>T
ra
in
in
g
er
ro
r
</p>
<p>1 2 3
</p>
<p>0
</p>
<p>0.002
</p>
<p>0.004
</p>
<p>0.006
</p>
<p>0.008
</p>
<p>0.01
</p>
<p>Fig. 10.2: Training error for each of the three models M1,M2,M3 computed on the training data
set. The most complicated model has the lowest training error.
</p>
<p>M1 = {1’st order polynomial, i.e. }fM1(x,w) = w0 + w1x.
M2 = {2’nd order polynomial, i.e. }fM2(x,w) = w0 + w1x+ w2x2.
M3 = {6’th order polynomial, i.e. }fM3(x,w) = w0 + w1x+ w2x2 + w3x3 + w4x4 + w5x5 + w6x6.
</p>
<p>The red line indicates the fitted polynomials. For each model we quantify how well the model fits
the training data by the training error which for model Ms is
</p>
<p>EtrainMs =
1
</p>
<p>N train
</p>
<p>∑
i∈Dtrain
</p>
<p>(yi − fMs(xi,w))2.
</p>
<p>Here fMs is the model Ms fitted to the training data and N train = |Dtrain| is the number of
observations in the training data set. The training error of each of these three models is shown in
fig. 10.2. Notice the “most correct” model, M2, fits the data better than the model M1, however,
both models fits the data far worse than the complicated model M3.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.1 Cross-validation 161
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 10.3: The example from before with a test dataset of three new points. If the models are
evaluated in terms of how they predict the new points M2 is preferred.
</p>
<p>Nevertheless, we are not interested in a model like M3 as it clearly will not generalize well to
new data. We say model M3 is overfitting the data. Thus, we can’t tell the models apart by how
well they fit the training data and in fact this can give us an entirely misleading picture of the
models performance due to overfitting. This is such an important principle it is worth framing:
</p>
<p>Never, ever should you estimate how well a model performs by its predictions on data it was
trained upon.
</p>
<p>However, let’s assume we obtain access to some new data, the test data Dtest, indicated by
the green squares in fig. 10.3. Testing the models on this new test-dataset gives us the ability to
estimate how well the models generalize to new data. We can define the test error as
</p>
<p>EtestMs =
1
</p>
<p>N test
</p>
<p>∑
i∈Dtest
</p>
<p>(yi − fMs(xi,w))2.
</p>
<p>Notice, fMs is the model that was fitted to the training data. The test error of each of these three
models is shown in fig. 10.4. Notice, the test error allows us to select the correct model, i.e. the
model that can be expected to generalize better to new data.
</p>
<p>The problem is that as a rule nobody is going to turn up and give us a test dataset when we
need it. The basic idea in cross-validation is to overcome this problem by taking our existing fixed
data set D and manually divide it into a training set, Dtrain, and a testing data set, Dtest, and then
use these two to select the appropriate model.
</p>
<p>10.1.2 The basic setup for cross-validation
</p>
<p>The basic setup for cross-validation is as follows: We consider a supervised learning problem with
a data set D = (X,y). It is important to keep in mind the dataset is finite and this is all the
data we have. As in the regression example, we consider different models for solving the problem,
M1,M2, . . . ,MS and for each of these models we have access to a loss function L(yi, ŷi) which
quantifies the error of predicting ŷi when the true value is yi. In the regression example the loss
function was the least square error L(yi, ŷi) = ‖yi − ŷi‖2, but in general the loss function will be
defined according to the specific modeling purpose.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>162 10 Overfitting and performance evaluation
</p>
<p>Test error
</p>
<p>Training error
</p>
<p>Model index Mk
</p>
<p>E
r
r
o
r
</p>
<p>1 2 3
</p>
<p>0
</p>
<p>0.005
</p>
<p>0.01
</p>
<p>0.015
</p>
<p>Fig. 10.4: Test error for each of the three modelsM1,M2,M3 computed on the test data set. The
test error correctly singles out model M2 as the better model.
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 10.5: The basic regression problem for model M2 and three random test sets. Since the test
sets are random, the test error too will vary depending on the particulars of the test set. The
generalization error overcomes this by averaging over all test sets.
</p>
<p>Training and test error
</p>
<p>If the data is divided into a training set and a test set Dtrain and Dtest and the model M is fitted
on the training set to provide the prediction rule fM then we define the training and test errors as:
</p>
<p>EtrainM =
1
</p>
<p>N train
</p>
<p>∑
i∈Dtrain
</p>
<p>L(yi, fM(xi)), (10.1)
</p>
<p>EtestM =
1
</p>
<p>N test
</p>
<p>∑
i∈Dtest
</p>
<p>L(yi, fM(xi)). (10.2)
</p>
<p>These definitions are similar except fM is fitted on the training set in both cases.
</p>
<p>Generalization error
</p>
<p>A problem with the test error is that it depends on the specific test set. Since we have to construct
the test set ourselves, this makes the test error slightly random. This is illustrated in fig. 10.5 for the</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.1 Cross-validation 163
</p>
<p>Generalization error
</p>
<p>Test errors
</p>
<p>Model Mk
</p>
<p>E
r
r
o
r
</p>
<p>1 2 3
</p>
<p>0
</p>
<p>0.002
</p>
<p>0.004
</p>
<p>0.006
</p>
<p>0.008
</p>
<p>0.01
</p>
<p>0.012
</p>
<p>0.014
</p>
<p>Fig. 10.6: Continuing the example, for the three models different test sets gives different test error
as indicated by the black dots. The generalization error is simply the average over all possible test
sets according to their probability of occurring.
</p>
<p>test error for three different (random) test sets. To alleviate this problem we introduce a new, third
error namely the generalization error. The generalization error is an idealized quantity indicating
how well our model performs on average assuming we had an infinite amount of data to test it
on, i.e. the average of the test errors as illustrated in fig. 10.6. The generalization error is what we
truly wish to estimate and the best model is the model with the lowest generalization error. If we
assume the test observations (xi,yi) come from a distribution p(x,y) then the generalization error
is defined as follows:
</p>
<p>• Train the model M on the full dataset available D to give a prediction rule fM.
• The generalization error of model M is1
</p>
<p>EgenM = E(x,y) [L(y,fM(x))] (10.3)
</p>
<p>=
</p>
<p>∫
L(y,fM(x))p(x,y)dxdy. (10.4)
</p>
<p>The generalization error is the fairest estimate of how well our model can perform because it assumes
we train our model on all data we have available and then computes the average loss on all future
data.
</p>
<p>1 Another popular definition is to consider the training set random as well which we will later call the
averaged generalization error. This is however notationally more cumbersome and leads to the same
definition of the cross-validation algorithms.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>164 10 Overfitting and performance evaluation
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 10.7: Cross-validation applied to the model M2 using 3-fold cross-validation. Errors are esti-
mated from the test data points and is indicated by the gray bars. Averaging all errors produce the
estimate of the generalization error ÊgenM2
</p>
<p>10.1.3 Cross-validation for quantifying generalization
</p>
<p>The obvious problem with the generalization error is that we cannot compute it since we don’t
know the true distribution of the data. Cross-validation, is thus a framework to estimate a model’s
generalization error typically based on one of the following three approaches:
</p>
<p>Hold-out method
</p>
<p>In the hold-out method, the full dataset D is split into a train and a testing set
</p>
<p>D = Dtrain ∪ Dtest.
</p>
<p>Then, we train a model on Dtrain and compute the test error EtestM using the test data set Dtest and
formula eq. (10.2) and simply use the approximation:
</p>
<p>EgenM ≈ EtestM .
</p>
<p>Why does this work? The test error is different in two ways from the generalization error. Firstly,
we only train the model on a subset of the data Dtrain and not the full data set D and secondly
we do not compute the true expectation but only the empirical average based on Dtest. However,
if the training data set is large, we can expect (or rather, hope!) there will be little difference in
using Dtrain instead of D and secondly, if we have a lot of test data in Dtest, and each element in
the test data set is drawn from the true distribution p(x,y), we can expect the empirical average in
eq. (10.2) to be quite close to the true average for the generalization error eq. (10.4). By recognizing
these limitations we can provide two alternative methods which generally does better but are also
computationally more demanding:
</p>
<p>K-fold cross-validation
</p>
<p>Ideally, we want each data point to be used in the test set, and one way to accomplish this is with
K-fold cross-validation. In K-fold cross-validation the full data set is split into K pieces
</p>
<p>D = D1 ∪ D2 ∪ · · · ∪ DK ,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.1 Cross-validation 165
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
0 0.5 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 10.8: Cross-validation applied to model-selection for the linear regression models. Each row
corresponds to one of the three models, and each column to the estimate of the test error on that
particular fold.
</p>
<p>each containing NK observations. We then produce K splits into training and test sets by, for each k,
treating Dk as the test set and the other K − 1 pieces as the training set. Computing the test error
on each of these K splits gives K estimates of the error EtestM,1, . . . , E
</p>
<p>test
M,K and we now approximate:
</p>
<p>EgenM ≈
K∑
k=1
</p>
<p>N testk
N
</p>
<p>EtestM,k,
</p>
<p>i.e., as the weighted average of the test errors, weighted by the number of test observations in each
fold N testk relative to the total number of observations used for testing N . Since each data point is
used once in the test set this method is generally more precise than the hold-out method, however,
it requires K times more training and testing of models than the hold-out method.
</p>
<p>Leave-one-out cross-validation
</p>
<p>The final method is based on the intuition that we ideally want the training set Dtrain to be as
close to D as possible. This can be accomplished by using K-fold cross-validation with K = N ,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>166 10 Overfitting and performance evaluation
</p>
<p>the total number of observations in the full data set. In this way, we train N models and each
model is trained on the full data set except a single observation and then tested on that single
observation. The benefit of this method is that it uses as much data as possible for training such
that each trained model is less prone to overfitting than when larger parts of the data are taking
out for testing at a time which is especially a concern when having very limited data. However, the
drawback is that it requires N models to be trained which can be very wasteful. While leave-one-out
cross-validation gives an almost unbiased estimate of the generalization error, in some cases it can
have a larger variance compared with, say, 10-fold cross-validation. Overall, it is recommended to
use 10-fold cross-validation [Kohavi, 1995].
</p>
<p>In the following we will denote by ÊgenM an estimate of the generalization error E
gen
M computed
</p>
<p>by any of the three above techniques. An illustration where 3-fold cross-validation is applied to the
linear-regression example is given in fig. 10.7. Each figure corresponds to a fold and in each fold the
test error is computed as the average of the error on the three datapoints that are left out. Notice,
all nine data-points are part of the test set exactly once.
</p>
<p>10.1.4 Cross-validation for model selection
</p>
<p>We will accept that the generalization error eq. (10.4) is the optimal way to measure the performance
of a model, and that cross-validation using any of the three techniques (hold-out, K-fold or leave-
one-out) is a faithful estimate of the generalization error. Then, an obvious way to select between
S models M1, . . . ,MS is to estimate the generalization error of each model using cross-validation
and select the model with the lowest cross-validation error. To summarize:
</p>
<p>• For each model, compute the estimate of the generalization error ÊgenM1 , . . . , Ê
gen
MS using cross-
</p>
<p>validation.
• Select the optimal model Ms∗ as that with the lowest error:
</p>
<p>s∗ = arg min
s
</p>
<p>ÊgenMs
</p>
<p>There is nothing more to cross-validation for model selection than this! This technique is most often
used in conjunction with K-fold cross-validation. In this case it is strongly recommended that the
same data splits (i.e. choices of D1, . . . ,DK) is used for all models. Since the resulting method is so
important it is provided as an explicit algorithm in algorithm 5.
</p>
<p>An illustration of 3-fold cross-validation for model selection in the linear-regression example is
given in fig. 10.8. Each of the columns corresponds to a fold and each of the rows to a model. In
fig. 10.9 the estimated generalization and training errors (averaged over the cross-validation folds)
is plotted. As can be seen the training error drops for the more complicated model, however, the
cross-validation estimate of the generalization error allows us to select the right model M2.
</p>
<p>10.1.5 Two-layer cross-validation
</p>
<p>Let’s turn to the following situation: We wish to select the optimal modelMs∗ out of S models and
estimate the generalization error for this optimal model Ms∗ . A tempting way to accomplish this
is to apply K-fold cross-validation to estimate the generalization error for each model Ms using
cross-validation and select the model with the lowest generalization error and then use the estimate
of the generalization error as an estimate of how well the model performs. This is illustrated in</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.1 Cross-validation 167
</p>
<p>Algorithm 5: K-fold cross-validation for model selection
</p>
<p>Require: K, the number of folds in the cross-validation loop
Require: M1, . . . ,MS . The S different models to select between
Ensure: Ms∗ the optimal model suggested by cross-validation
</p>
<p>for k = 1, . . . ,K splits do
Let Dtraink , Dtestk the k’th split of D
for s = 1, . . . , S models do
</p>
<p>Train model Ms on the data Dtraink
Let EtestMs,k be the test error of the model Ms when it is tested on D
</p>
<p>test
k
</p>
<p>end for
end for
For each s compute: ÊgenMs =
</p>
<p>∑K
k=1
</p>
<p>Ntestk
N
</p>
<p>EtestMs,k
Select the optimal model: s∗ = arg mins Ê
</p>
<p>gen
Ms
</p>
<p>Ms∗ is now the optimal model suggested by cross-validation
</p>
<p>Training error
</p>
<p>Estimated generalization error
</p>
<p>Model index Ms
</p>
<p>E
r
r
o
r
</p>
<p>1 2 3
</p>
<p>0
</p>
<p>0.005
</p>
<p>0.01
</p>
<p>0.015
</p>
<p>0.02
</p>
<p>0.025
</p>
<p>0.03
</p>
<p>Fig. 10.9: Training error and the cross-validation estimate of the generalization error. The estimate
of the generalization error is simply the averages of the errors in fig. 10.8 over all 3 folds as dictated by
the cross-validation method for model selection. The model with the lowest estimated generalization
error is M2 even though it does not have the lowest training error.
</p>
<p>fig. 10.10 where we consider 14 different models and for each model the true generalization error is
indicated as the black line and the estimated generalization error as the small red dots. The selected
model is the model with the lowest (estimated) generalization error indicated with the red circle.
The estimates of the generalization error is imprecise due to the randomness in the test set which
is why they are not all on the black line.
</p>
<p>There is however a problem with this approach. Suppose we had access to additional test sets
and use these to estimate the generalization error (indicated in the 3 other panels of fig. 10.10).</p>
<p></p>
</div>, <div class="page"><p></p>
<p>168 10 Overfitting and performance evaluation
</p>
<p>True generalization error
</p>
<p>Estimated generalization error
</p>
<p>Model selected by cross-validation
</p>
<p>Model Ms
</p>
<p>E
r
r
o
r
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>Estimated generalization error
</p>
<p>Model selected by cross-validation
</p>
<p>Generalization error on new test set
</p>
<p>Model Ms
</p>
<p>E
r
r
o
r
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>Model Ms
</p>
<p>E
r
r
o
r
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>Model Ms
</p>
<p>E
r
r
o
r
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>Fig. 10.10: Top right: The true generalization error of 14 models is indicated by the black line and
estimates of the generalization error (computed using cross-validation) are indicated by red dots.
Standard cross-validation then selects the model with the lowest (estimated) cross-validation error
(indicated by red circle). However, this estimate of the generalization error is not in general a fair
estimate of how the model will generalize to future data because it is selected as a minimum. In
subplots 2-4 is shown the same procedure and as seen the estimated generalization error is too
optimistic (below the black line) in all instances. A better estimate can be obtained by using a
completely new test set, blue dots, which provides a fairer estimate of the generalization error for
the selected values. This leads to two-layer cross-validation.
</p>
<p>These too are estimates of the true generalization error (black line), but they are independent of
the red dots. However, since we always select the red dot with the lowest error, and the blue dots
are random, we will in general be too optimistic with respect to our estimate of the generalization
error. After all, there are many roughly equally good models to choose from, so when we select the
best of these we will due to the randomness often do exceedingly well. In the figure, this is seen as
the selected red point being far lower than the true generalization error in all instances. Obviously,
this is cheating! To understand exactly what goes wrong we need to take a step back. By including
the step where we select the optimal modelM∗ = Ms∗ based on the data we have actually changed</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.1 Cross-validation 169
</p>
<p>Algorithm 6: Two-level cross-validation
</p>
<p>Require: K1,K2, folds in outer, and inner cross-validation loop respectively
Require: M1, . . . ,MS : The S different models to cross-validate
Ensure: Êgen, the estimate of the generalization error
</p>
<p>for i = 1, . . . ,K1 do
Outer cross-validation loop. First make the outer split into K1 folds
Let Dpari , D
</p>
<p>test
i be the i’th split of D
</p>
<p>for j = 1, . . . ,K2 do
Inner cross-validation loop. Use cross-validation to select optimal model
Let Dtrainj , Dvalj be the j’th split of Dpari
for s = 1, . . . , S do
</p>
<p>Train Ms on Dtrainj
Let EvalMs,j be the validation error of the model Ms when it is tested on D
</p>
<p>val
j
</p>
<p>end for
end for
</p>
<p>For each s compute: Êgens =
∑K2
j=1
</p>
<p>|Dvalj |
|Dpari |
</p>
<p>EvalMs,j
</p>
<p>Select the optimal model M∗ =Ms∗ where s∗ = arg mins Êgens
Train M∗ on Dpari
Let Etesti be the test error of the model M∗ when it is tested on Dtesti
</p>
<p>end for
Compute the estimate of the generalization error: Êgen =
</p>
<p>∑K1
i=1
</p>
<p>|Dtesti |
N
</p>
<p>Etesti
</p>
<p>the underlying model being tested. The model the above method produces, M∗, is now composed
of two things:
</p>
<p>• Use K2-fold cross-validation to estimates Êgens .
• Select M∗ as the optimal model Ms∗ where s∗ = arg mins Êgens .
</p>
<p>Thus, estimating the generalization error requires estimating the generalization error of the model
obtained through this two-step procedure. Fortunately, we know how to estimate the generalization
error of a model: Cross-validation. Since the method now makes use of two nested cross-validation
procedures, one in selecting Ms∗ as above and one for estimating performance, the resulting pro-
cedure is known as two-layer cross-validation. The method can be sketched as follows:
</p>
<p>• For i = 1, . . . ,K1 cross-validation iterations, split the data D into a training set Dpari and a test
set Dtesti
• For each iteration, find the optimal value s∗ using K2-fold cross-validation on Dpari . (In the jth
</p>
<p>inner fold Dpari is split into a training set Dtrainj and a test set (called a validation set) Dvalj ).
• Train the model M∗ using the selected model structure Ms∗ trained on the full outer fold
</p>
<p>training set Dpari
• Let EtestM∗,i be the test error of M∗ computed on the i’th test set Dtesti
• Estimate the generalization error as Êgen = ∑K1i=1 |Dtesti |N EtestM∗,i
</p>
<p>Again, since this method is so important it is worth providing it in pseudo code as algorithm 6.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>170 10 Overfitting and performance evaluation
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>x
</p>
<p>f
(x
,w
</p>
<p>)
</p>
<p>0 0.5 1
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>Fig. 10.11: A regularized linear regression model is fitted to the dataset of nine observations and six
test observations. The different plots correspond to adding more “junk” attributes, i.e. attributes
where the values are just random. As more random attributes are added, the model better fit the
training set but does worse on the test set.
</p>
<p>10.2 Sequential feature selection
</p>
<p>Consider a dataset where observations xi correspond to patients and we wish to predict a patient’s
survival time after an operation yi using linear regression. Suppose for each patient we observe
three attributes namely: x1: Age, x2: The room number of the patient and x3: Length of hospital
stay. Clearly, only the first and last attribute is relevant to our purpose, so rather than considering
</p>
<p>the attribute x =
[
x1 x2 x3
</p>
<p>]T
we could just as well consider the smaller dataset: x =
</p>
<p>[
x1 x3
</p>
<p>]T
.
</p>
<p>So does it matter that we include the room number x2 in our dataset? Well in general irrelevant
attributes matter for three reasons:
</p>
<p>• If the number of attributes (in particular the irrelevant ones) is large compared to the total
number of observations our model performance will degrade.
</p>
<p>• Storing and manipulating irrelevant attributes takes space and makes our models slower.
• A hospital will often wish to know which attributes are important and which are irrelevant. A
</p>
<p>model with many irrelevant attributes will not tell them that directly.
</p>
<p>Let’s examine the first claim first. Suppose we have the simple, linear regression problem which can
be fitted well with a second-order polynomial. That is, optimally we should consider:
</p>
<p>x =
[
x1 x
</p>
<p>2
1
</p>
<p>]
(10.5)
</p>
<p>However, we now add “junk” attributes to the dataset and considers
</p>
<p>x =
[
x1 x
</p>
<p>2
1 x3 x4 . . . xS+2
</p>
<p>]
where S is the number of junk attributes added to the dataset. Thus S = 0 will correspond to
eq. (10.5) and S = 3 will correspond to adding 3 junk attributes. The junk attributes are simply
generated as random numbers in the unit interval.
</p>
<p>Examples of the predictions on training and test set for S = 0, 3, 6 added junk attributes can
be seen in fig. 10.11. As can be seen, when more junk attributes are added, the model will begin to
overfit the training set. This is easily seen when plotting the training error against the test error as
is done in fig. 10.12 for S = 0, . . . , 7 and the three specific values shown in fig. 10.11 are indicated
by the circles. In a way, we already know how to solve this problem: Each selection of which features</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.2 Sequential feature selection 171
</p>
<p>Training Error
</p>
<p>Test Error
</p>
<p>Junk attributes S
</p>
<p>E
r
r
o
r
</p>
<p>0 1 2 3 4 5 6 7
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>Fig. 10.12: The training and test error for different number of junk attributes in the example of
fig. 10.11. The circles indicate the particular values shown in fig. 10.11.
</p>
<p>to use corresponds to a particular model, so in for instance the hospital example we can consider
all eight possible models
</p>
<p>M123 =
[
x1 x2 x3
</p>
<p>]
M12 =
</p>
<p>[
x1 x2
</p>
<p>]
M13 =
</p>
<p>[
x1 x3
</p>
<p>]
M23 =
</p>
<p>[
x2 x3
</p>
<p>]
M1 =
</p>
<p>[
x1
]
</p>
<p>M2 =
[
x2
]
</p>
<p>M3 =
[
x3
]
</p>
<p>M· =
[
•
]
,
</p>
<p>and select the optimal model by the use of cross-validation for model selection as already described.
In many ways this is the best we can do from a theoretical perspective, however, the problem is that
this procedure quickly becomes very costly. In general, if we have M attributes to choose between,
we must select between 2M models, thus if M = 20 then this results in having to cross-validate
more than a million different models. Clearly this won’t do!
</p>
<p>Sequential feature selection overcomes this problem by not considering all possible models but
only a subset. Sequential feature selecting comes in two variation, forward and backward selection,
but they are very similar.
</p>
<p>10.2.1 Forward Selection
</p>
<p>In forward selection, we first consider a model with no features
</p>
<p>M· =
[
•
]
</p>
<p>That is, it predicts yi as just being constant. Then it considers the models obtained by adding each
attribute to the existing (empty) set of selected attributes thereby testing all the models:
</p>
<p>M• =
[
•
]
, M1 =
</p>
<p>[
x1
]
, M2 =
</p>
<p>[
x2
]
, . . . ,MK =
</p>
<p>[
xM
]
.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>172 10 Overfitting and performance evaluation
</p>
<p>Model Êgen
</p>
<p>M• 0.91
M1 0.86
M2 0.92
M3 0.88
M4 0.83
M12 0.78
M13 0.62
M14 0.78
M23 0.74
M24 0.72
M34 0.76
M123 0.64
M124 0.68
M134 0.73
M234 0.78
M1234 0.79
</p>
<p>Table 10.1: The estimated generalization error Êgen as estimated by cross-validation for models
trained on different subsets of the features x1, x2, x3 and x4.
</p>
<p>Each of these M + 1 models are evaluated by cross-validation for model selection and the optimal
model, say Mi, is selected. If M• is selected the process terminates. Else, this procedure is now
repeated by evaluating the M models corresponding to
</p>
<p>Mi =
[
xi
]
, M1i =
</p>
<p>[
x1 xi
</p>
<p>]
, . . .Mi−1,i =
</p>
<p>[
xi−1 xi
</p>
<p>]
. . .MiM =
</p>
<p>[
xi xM
</p>
<p>]
Again, if Mi is the optimal model the process terminates, else an optimal model (say model Mij)
is selected and then all M − 1 models corresponding to Mij and the M − 2 models obtained by
adding all other attributes than xi, xj to the set evaluated by cross-validation. If it is found that for
instance Mij is the optimal model, the process terminates, else it continues possibly terminating
with the full model: M12...M .
</p>
<p>Example of forward selection
</p>
<p>Let’s illustrate this procedure with a concrete example. Suppose we have a dataset of M = 4
attributes giving 16 possible models with generalization errors (as estimated by cross-validation)
shown in table 10.1. Forward selection now proceeds as follows
</p>
<p>• Start with model M• with an error of 0.91.
• Compare models M• and M1,M2,M3,M4.
• Optimal model is M4 with error of 0.83.
• Compare models M4 and M14,M24,M34.
• Optimal model is M24 with error of 0.72.
• Compare models M24 and M124,M234.
• Optimal model is M124 with error of 0.68.
• Compare models M124 and M1234.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.3 Cross validation of time-series dataF 173
</p>
<p>• Since M124 has lowest error, forward selection terminates and select features 1, 2, 4.
Notice the procedure is completely mechanical, however, it is not guaranteed to select the model
with the lowest overall generalization error. The benefit of forward selection is naturally that we
don’t have to compute all the generalization errors beforehand but can compute them as they are
required.
</p>
<p>10.2.2 Backward Selection
</p>
<p>Backward selection builds upon the same idea as forward selection, but instead of starting with the
empty model M•, we start with the full model M12...M and instead of adding features, features
are now removed one at a time. To continue the example from before:
</p>
<p>• Start with model M1234 with an error of 0.79.
• Compare models M1234 and M123,M124,M134,M234.
• Optimal model is M123 with error of 0.64.
• Compare models M123 and M12,M13,M23.
• Optimal model is M13 with error of 0.62.
• Compare models M13 and M1,M3.
• Since M13 has lowest error, backward selection terminates and select features 1, 3.
</p>
<p>Notice forward selecting selected modelM124 and backward selection selected modelM13. We are
thus not guaranteed that these two methods will select the same set of features or that forward
selection will select less features than backward selection (or the reverse). In general, compare both
methods and see which has the lowest estimated generalization error.
</p>
<p>The disadvantage of sequential feature selection is that we are not comparing all models and thus
we might miss the model with the lowest generalization error. The advantage is runtime. Comparing
all models requires 2M model evaluations, while in the worst case forward (or backward) selection
requires estimating the generalization error of
</p>
<p>(1) + (M) + (M − 1) + (M − 2) + · · ·+ (1) = M(M + 1)
2
</p>
<p>+ 1
</p>
<p>models For M = 20 this correspond to only 211 models compared to more than a million models
using exhaustive search. As a final note, it is strongly recommended to use the same cross-validation
splits when estimating the generalization error of the models to reduce variance in the estimates of
the generalization error. In fact, in the calculation above we have used that M• does not need to
be recalculated if using the same cross-validation splits.
</p>
<p>10.3 Cross validation of time-series dataF
</p>
<p>Our discussion so far has assumed data is atemporal. This choice does not reflect the methods we
have considered are irrelevant for time-series data, but rather that we wished to avoid unnecessary
complications or caveats when discussing the particular methods.
</p>
<p>For completeness, we have nevertheless decided to include a section about validation of time
series data. A reader should be aware time-series analysis is a specialized subject, and one in which
it is more difficult to provide general advice; we will therefore briefly introduce a few common-sense
data-processing suggestions and warn the reader to approach this section with her critical faculties
engaged and take note all of the following subsections are marked with a F.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>174 10 Overfitting and performance evaluation
</p>
<p>Table 10.2: The seven entries of the traffic data set
</p>
<p>ID Time t xrain xtemperature xstation xcars y
</p>
<p>1 13:20, Sep 6th, 2019 1 23.7 120 236 164
2 13:40, Sep 6th, 2019 1 22.9 141 249 163
3 14:00, Sep 6th, 2019 1 24.6 168 243 186
4 14:20, Sep 6th, 2019 0 25 179 250 184
5 14:40, Sep 6th, 2019 0 26.4 189 254 196
6 15:00, Sep 6th, 2019 0 26.1 215 271 197
7 15:20, Sep 6th, 2019 1 25.3 227 278 211
</p>
<p>10.3.1 The setup
</p>
<p>Time-series data is data which contains a feature corresponding to time, and where the temporal
dependency between observations may be considered vital for the prediction task. In other words,
if we scrambled the time-coordinate, the machine-learning task should seem difficult or impossible.
Examples could be the exchange rate of currencies, temperature as recorded throughout a day or a
persons future decisions based on recordings of her brain.
</p>
<p>The basic setup is as follows: We will assume we have access to a set of sensors which make
recordings of a set of features x, and a predictive target y, over time t. We denote the value at a
particular time by:
</p>
<p>x(t) =
[
x1(t) x2(t) · · · xM (t)
</p>
<p>]T
, y(t)
</p>
<p>Then, our goal can be described as predicting the value of y at a future time t + ∆t based on
whatever data we have recorded, or otherwise have available, up to time t.
</p>
<p>Concretely, assume measurements are taken of x(t) and y(t) at N discrete time steps t =
t1, t2, . . . , tN . As an example, suppose we use a security camera to estimate the number of people
at a specific train station, and it is this quantity we are interested in predicting based on 4 features.
All in all:
</p>
<p>y(t): Estimated number of people at the station of interest
xrain(t): Whether it rains or not
xtemperature(t): Temperature in Celsius
xstation(t): Estimated number of people at (another) station
xcars(t): Car velocity at a major intersection km/hour
</p>
<p>We have shown an (unrealistically small) example of this dataset in table 10.2 comprised of just
N = 7 observations.
</p>
<p>The standard format
</p>
<p>Our first task is to bring our dataset into the standard format used throughout this course. To
simplify the presentation, we will make the assumption that all sensors (values of features x(t))
are recorded simultaneously and at equidistant time points (i.e., that the interval between when
observations are taken is the same). If the later assumption is violated one can do one of three
things:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.3 Cross validation of time-series dataF 175
</p>
<p>Table 10.3: The traffic from table 10.2 processed by introducing the hour-of-day and day-of-week
features
</p>
<p>xrain xtemperature xstation xcars xhour xday y
</p>
<p>1 23.7 120 236 13 4 164
1 22.9 141 249 13 4 163
1 24.6 168 243 14 4 186
0 25 179 250 14 4 184
0 26.4 189 254 14 4 196
0 26.1 215 271 15 4 197
1 25.3 227 278 15 4 211
</p>
<p>• Ignore the problem; this may lead to useful results if the data is nearly uniformly sampled or
the relative spacing between observations is not that important.
• If the data is super-sampled, that is, the intervals between each observation is very small, we
</p>
<p>can consider subsampling the dataset at regularly spaced intervals.
• One can interpolate the values at regularly spaced intervals (note this is probably inappropriate
</p>
<p>if the features are categorical).
</p>
<p>When the data is divided into equidistant time points we can (linearly) index them by the time point
ti at which the observations was recorded which we call time steps or simply steps. In the traffic
example table 10.2 all observations are made at the same time points and with regular intervals of
20 minutes and the step corresponds to the ID column.
</p>
<p>In addition, we will make the assumption the dataset is stationary, which we will here loosely de-
fine as not being affected by long-running trends (for instance that the number of people at the train
station is increasing over time as the train service becomes more popular). If the dataset appears
to be obviously non-stationary, one should attempt to standardize it, for instance by regressing out
a linear trend.
</p>
<p>It is at this point tempting to turn the prediction problem into simply predicting y at step i
based on features corresponding to step i− 1 and so on. However, in the traffic example this would
mean we lost the most important pieces of information, namely when during the day we wish to
predict y. It is therefore (in this particular example) appropriate to introduce an xhour-feature. In a
similar vein, weekends implies that the day of the week is probably also important (as commuters
are likely to change habits during the weekend) and we introduce an xday-feature.
</p>
<p>Note these new observations are special, as we can pre-compute them for the future time y(t)
which we wish to predict. It makes sense to separate these features from the remaining features
which cannot be predicted; simply put, if we know we are predicting y(t) for a Monday, we are not
provided with additional information to know that at earlier times it was Sunday, Saturday, etc.
In the following, we assume that such special variables are separated from those that cannot be
pre-computed, and we will focus on the non-pre-computed variables in the following.
</p>
<p>As a final comment, note that it may make sense to represent the hour and day-variables as
categorical variables and perform an one-out-of-K encoding, as representing them with real numbers
(artificially) implies the distance between 23:00 and 02:00 is greater than 08:00 and 11:00.
</p>
<p>At any rate, after dropping the irrelevant ID feature, the dataset table 10.2 will now be assumed
to look like table 10.3.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>176 10 Overfitting and performance evaluation
</p>
<p>Fig. 10.13: Illustration of different choices of embedding size and horizons. A dataset (left) consisting
of two feature-attributes (green, blue) and prediction targets (red) in the format defined in eq. (10.6)
is converted into the (standard) (X,y) format used elsewhere in the book for three different choices
of embedding size (number of past time steps) and horizons (number of steps to predict into the
future), see eq. (10.7)
</p>
<p>Embedding size and horizon
</p>
<p>The horizon h ≥ 1 is an integer denoting how far into the future we are trying to make predictions
whereas the embedding size p ≥ 1 refers to how many past observations are relevant to make
predictions about the future.
</p>
<p>To make this concrete, suppose the (non-instantaneous) part of the dataset, i.e. the label values
and the first four attributes in table 10.3 are denoted by:
</p>
<p>X̃ =
</p>
<p>
x̃T1
x̃T1
...
</p>
<p>x̃TN
</p>
<p> =

x̃11 x̃12 · · · x̃1M
x̃21 x̃22 · · · x̃2M
</p>
<p>...
...
</p>
<p>. . .
...
</p>
<p>x̃N1 x̃N2 · · · x̃NM
</p>
<p> , ỹ =

ỹT1
ỹT1
...
ỹTN
</p>
<p> . (10.6)
For an embedding of size p and horizon h, we will therefore predict ỹi+h−1 based on data available
at steps i− p to i− 1, i.e.:
</p>
<p>predict ỹi+h−1 based on
[
x̃i−p x̃i−p+1 · · · x̃i−2 x̃i−1 ỹi−p ỹi−p+1 · · · ỹi−2 ỹi−1
</p>
<p>]
. (10.7)
</p>
<p>Naturally, if we had access to additional, instantaneous features such as xhour and xday these would
be added to the vector of attributes in eq. (10.7). This construction likely seem a bit abstract and
we have illustrated it in fig. 10.13 for three different choices of horizons and embedding sizes.
</p>
<p>In a concrete application, the embedding size should be selected based on a consideration of
how far into the past observations remain predictive of the presence, whereas the horizon should be
selected based on what our modeling goal is. If the goal is a traffic planner for commuters, a horizon
of about an hour (h = 3, as each step corresponds to 20 minutes) might be appropriate, whereas
if we are trying to plan if extra trains should be included a planning horizon of several hours is
probably more appropriate. It is difficult to judge how large the embedding size should be in this
example; one could argue an embedding size of 1 (i.e. just the preceding time step) is relevant, or
a large embedding size because commuters who took the train in the morning are likely to also use
it at the end of their work day. The recommendation is to begin with a small embedding size for
simplicity, and use cross-validation to make informed decisions.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.3 Cross validation of time-series dataF 177
</p>
<p>Without embedding With embedding of size 2
</p>
<p>R
a
n
d
o
m
</p>
<p>C
o
n
se
</p>
<p>q
u
ti
</p>
<p>v
e
</p>
<p>R
o
ll
in
</p>
<p>g
</p>
<p>Training data Unused dataTest data
</p>
<p>Fig. 10.14: Examples of different cross-validation approaches. Notice each square corresponds to
both (xi, yi) values transformed using the approach outlined in section 10.3.1, see fig. 10.13. For
each approach, we only show the first 4 of the cross-validation folds.
</p>
<p>What about multiple time series?
</p>
<p>In the case of multiple time series what should be done depends slightly on the specifics of the
dataset. If for instance the time series represents instances of the same process, but recorded in
sequences with breaks in between, we recommend each section is simply processed using the pro-
cedure in this section and the data is pooled at the end. An example of this in the context of the
traffic example could be if data is only recorded every other week.
</p>
<p>If on the other hand the different time series represent the same basic phenomena, but can
nevertheless be grouped into a fixed number of groups representing a (systematical) difference
in the recording procedure, one could consider introducing a categorical variable such that the
machine-learning method understands which type of recording is currently under consideration. An
example of this in the traffic-example could be if in some weeks we record the number of people at
the train station using one method, and in other weeks with another, and we think these may yield
systematically different estimates.
</p>
<p>10.3.2 Cross-validation
</p>
<p>We will limit ourselves to K-fold cross-validation as leave-one-out cross-validation is just a special
case and the hold-out method should be trivial. The temporal dependency makes it difficult to
provide a single, unique recommendation for doing cross-validation, and we will therefore suggest
different approaches and their possible strengths/weaknesses. An interested reader can consult
Bergmeir et al. [2018] and references therein for a more comprehensive discussion.
</p>
<p>Consider the schemes indicated in fig. 10.14. In the figures, each of the N squares correspond
to a training observation and label information (xi, yi), and it will be assumed these have been
converted using the procedure outlines in the previous section (see fig. 10.13).
</p>
<p>The rows represent three schemes for selecting training and test splits:
</p>
<p>Random splits Training/test splits are selected non-overlapping and at random</p>
<p></p>
</div>, <div class="page"><p></p>
<p>178 10 Overfitting and performance evaluation
</p>
<p>Consequtive splits As above, but the K folds consist of consecutive observations
Rolling The test set is always in the future relative to the training set
</p>
<p>A difficulty is that training observation xi will contain yi−1, and in general for embeddings of size
p observation xi will contain observations yi−p, . . . , yi−1. This means the training and test sets
will overlap which in turn means we are likely to under-estimate the generalization error. For that
reason, it is often desirable to pad the test set by removing a set of observations on each side
corresponding to the correlation length in the data, here assumed to be roughly the embedding
size. This is shown in the right-column of fig. 10.14.
</p>
<p>A few remarks are at this point in order for (possible) advantages and disadvantages of the
different methods: The random and consecutive splits are likely to behave quite similar without
padding, however with padding, and in particular if the dataset is very small, padding will remove
proportionally more of the data for random splits (biasing the generalization error estimate up-
wards). On the other hand, selecting splits consecutively rather than randomly mean the data in
the test set in each fold will overlap, thereby increasing the variance of the generalization error
estimate.
</p>
<p>For both the random and consecutive selections, a problem is we include the future in the
training set, whereas in the real world we will train on past data and test on future. The rolling
method removes the future data from the splits, however, this (in turn) means we have access to
less training data and will possibly bias the generalization error estimate slightly upwards.
</p>
<p>10.3.3 Two-layer cross-validation
</p>
<p>The extension to two-layer cross-validation is technically straight-forward: Simply choose one of the
approaches indicated in fig. 10.14 for the outer fold, and another for the inner fold. Note, however,
that the various biases, advantages and disadvantages of the outlined approaches might interact
in undesirable ways and degree of appropriateness of a given approach is likely going to be highly
problem-dependent and the estimates of the generalization error is likely to differ, especially for
datasets of limited size. We therefore recommend a preference is given to simpler models, with a
primary focus on demonstrating (a minimum of) past information predicts the (immediate) future
better than simply using immediately available information. In the case of the traffic example, even
assuming we had access to a non-trivial number of observations, it is not immediately clear that
past number of passengers during a day improves predictions of the present beyond knowing the
day of week and time of day.
</p>
<p>10.4 Visualizing learning curvesF
</p>
<p>The generalization error is such an important quantity it can be used as a qualitative pointer to
model problems, here illustrated using learning curves. A learning curve refers to how the perfor-
mance of a given method change as a function of a key quantity or parameter. We have already
encountered examples of learning curves, for instance in fig. 7.15 we considered a schematic illus-
tration of learning as a function of computation time and in fig. 1.11 we considered performance as
a function of training set size.
</p>
<p>In this chapter, we will try to provide an intuitive feeling for how different shapes of learning
curves may point to different problems (or lack therefore) with a given method. A word of warning:
This discussion should not be taken too literal, and a reader should not expect their curves to</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.4 Visualizing learning curvesF 179
</p>
<p>Fig. 10.15: Illustration of good and bad learning outcomes. the left-hand pane shows stereotypical
behaviour of a good learning outcome: The training/generalization error is well below baseline
performance (dotted blue line), and as more data is added the generalization error drops towards
the target performance. That the generalization error is higher than the training error is to be
expected in a complex model and should drop with more data. The right-hand pane shows a model
which overfit badly. The training error remains very low, but the generalization error does not
budge when more data is added. This is probably due to a poor model choice.
</p>
<p>re-produce those in this section exactly, as they will depend quite strongly on the specifics of the
chosen method, evaluation metric and data set. For this reason the section is marked with a F.
</p>
<p>10.4.1 The setup
</p>
<p>We will consider a setting where we are trying to solve a supervised learning problem using a
somewhat advanced method. Using the techniques considered in this chapter, we can compute the
training error and estimate the generalization error, for instance by simply computing the error on
a test set as discussed earlier in this chapter.
</p>
<p>In addition to these two quantities, we assume (this may be realistic or not) we have some
target performance in mind (i.e. the point where the method is considered useful, good enough
or equivalent to state-of-the-art), as well as a baseline of some kind, for instance a linear/logistic
regression. Within this setup, we will plot the (estimate of) the generalization and training error
as a function of the amount of training data.
</p>
<p>Consider the learning curves shown in fig. 10.15. In the left-hand pane we see an example of
a learning curve where the training error is fairly low, the generalization error is low (and falling
rapidly when more data is available).
</p>
<p>This is the expected outcome for a well-applied method: The training error will, within statistical
uncertainty, be lower than the generalization error, however the gab should close with more data
and the error should approach target performance.
</p>
<p>On the other hand, the right-hand pane of fig. 10.15 provides an example of what we do not want
to see. The generalization error is very high (nearly corresponding to random guessing), whereas
the training error is extremely low. Moreover, none of these appears to budge. This is because the
method is overfitting the training set: The method extracts information specific to each training</p>
<p></p>
</div>, <div class="page"><p></p>
<p>180 10 Overfitting and performance evaluation
</p>
<p>Fig. 10.16: Examples of learning curves. Left: a weak/misapplied model where learning converge,
but it is too inflexible to learn the right relationship. Middle: A bad method which is too inflex-
ible to learn the correct relationship in the data (high training error), however the generalization
error remains much higher than the training error indicating it is simultaneously overfitting. Right:
Example where learning seems to be converging; consider more data/tweaking and see if the gen-
eralization error sustains the drop.
</p>
<p>observation, while learning nearly nothing about the general (true) relationship in the test data.
This corresponds to a method which learns to identify cars by observing images of cars contain blue
sky and therefore learn this relationship. Moreover the curves are nearly flat, indicating adding
more data will not change the situation. When one encounters this behavior, the go-to suggestion
is to switch to a simpler model where training does not fail.
</p>
<p>These two examples provides extremes, however learning curves can, with some care, be used to
diagnose other more complex problems. We should here stress our warning from the introduction,
namely that the cartoonish examples of learning curves encountered in the happy world of pedagogy,
and the ones a reader will likely encounter in practice, will only be approximately similar. For
instance, the learning curve in fig. 10.15 (left) should be compared to the (comparable) learning
curve in fig. 7.14 (right) obtained using logistic regression.
</p>
<p>Method is too weak or misapplied
</p>
<p>In the left-hand pane of fig. 10.16 we see another example of a bad outcome: the generalization error
seems to have plateaued, but moreover the training error is very high. This method is probably too
weak: it is unable to extract enough information about the true relationship in the data to learn it,
even on the training set. In this case one should consider if the method is applied correctly (perhaps
it is not learning anything?), or consider a more flexible method.
</p>
<p>Wrong method
</p>
<p>Next consider the center-pane of fig. 10.16. This is another bad outcome where the method is both
too weak to capture the true relationship in the data (And thus, as in the previous case, we cannot
expect more data will fix our problems), but the generalization error is higher than the training
error, indicating some degree of overfitting. The method is therefore learning too little and the little
it is learning is the wrong thing. Something is seriously wrong; either with the method, or with the
way the training/test set is selected, and more data is unlikely to fix the problems.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.4 Visualizing learning curvesF 181
</p>
<p>More data and tweaking
</p>
<p>Finally consider the right-hand pane of fig. 10.16. In this case the training curve seems to have
plateaued around the expected level of performance, whereas the generalization error seems to drop
as more data is added. In this case we can hope adding more data, and possible tweaking the
method a bit to account for hard cases, will get us the rest of the way.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>182 10 Overfitting and performance evaluation
</p>
<p>Problems
</p>
<p>10.1. Fall 2014 question 27: Alice is considering a lin-
ear regression model for a dataset comprised of N = 1000
observations. She wishes to both select the optimal regu-
larization strength as well as estimate the generalization
error of the model at the optimal regularization strength.
To simplify the problem, she only considers the following
6 possible values of the regularization strength λ:
</p>
<p>λ = 10−2, 10−1, 100, 101, 102, 103.
</p>
<p>Alice opts for a two-level strategy in which she uses the
hold-out method to estimate the generalization error and
cross-validation is used to select the optimal regulariza-
tion strength, i.e. the dataset is first divided into a valida-
tion set Dvalidation, comprised of 20% of the full dataset,
and the remainder DCV is used for cross-validation. Al-
ice uses standard K = 10 fold cross-validation to select
the optimal regularization strength on DCV and, having
estimated the optimal regularization strength, uses the
hold-out method on DCV and Dvalidation to estimate the
generalization error.
</p>
<p>Suppose for any fixed value of the regularization
strength, the time taken to train the weights of the linear
regression model on a dataset of size Ntrain is N
</p>
<p>2
train units
</p>
<p>of time and the time taken to test a trained model on a
dataset of size Ntest is
</p>
<p>1
2N
</p>
<p>2
test units of time. Suppose the
</p>
<p>duration of all other tasks is neglible, what is the total
time taken for the entire procedure?
</p>
<p>A 12.78 · 106 units of time.
B 15.98 · 106 units of time.
C 31.30 · 106 units of time.
D 31.96 · 106 units of time.
E Don’t know.
</p>
<p>10.2. Spring 2013 question 13: We would like to fit
an artificial neural network to the PM10 dataset shown
in Table 10.4. It is decided that DAY should not be in-
cluded in the model as this cannot be influenced by de-
cision makers. We therefore only consider x1, x2, x3 and
x4 corresponding to logCAR, TEMP, WIND and TEM-
PDIF respectively. An artificial neural network is applied
to the data with these four attributes. The neural net-
work has three hidden units and is trained using different
combinations of the four attributes x1, x2, x3 and x4. Ta-
ble 10.5 gives the training and test performance of the
artificial neural network for different combinations of the
four attributes. Which one of the following statements is
correct?
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Logarithm of number logCAR
of cars per hour
</p>
<p>x2 Temperature 2 meter TEMP
above ground (degree Celsius)
</p>
<p>x3 Wind speed (meters/second) WIND
x4 Temperature difference between TEMPDIF
</p>
<p>25 and 2 meters (degree Celsius)
x5 Wind direction WINDDIR
</p>
<p>(degrees between 0 and 360)
x6 Whole hour of the day HOUR
x7 Day number from DAY
</p>
<p>October 1. 2001
</p>
<p>y Logarithm of PM10 logPM10
concentration
</p>
<p>Table 10.4: The attributes of the PM10 data. The out-
put is given by the hourly values of the logarithm of the
concentration of PM10 particles (logPM10).
</p>
<p>Feature(s) Training Test
rmse rmse
</p>
<p>x1 0.71 0.75
x2 0.58 0.64
x3 0.60 0.62
x4 0.92 0.94
</p>
<p>x1 and x2 0.60 0.69
x1 and x3 0.35 0.44
x1 and x4 0.52 0.66
x2 and x3 0.56 0.69
x2 and x4 0.45 0.52
x3 and x4 0.62 0.64
</p>
<p>x1 and x2 and x3 0.36 0.34
x1 and x2 and x4 0.28 0.33
x1 and x3 and x4 0.27 0.45
x2 and x3 and x4 0.20 0.43
</p>
<p>x1 and x2 and x3 and x4 0.10 0.35
</p>
<p>Table 10.5: Root mean square error (rmse) for the train-
ing and test set when using an artificial neural network
with three hidden units to predict the level of pollution
(logPM10) based only on the first four attributes (x1–x4)
using the hold-out method with 50 % of the observations
hold-out for testing.
</p>
<p>A Neither forward nor backward selection will identify
the optimal feature combination for this problem.
</p>
<p>B Backward selection will result in a better model be-
ing selected than using forward selection.
</p>
<p>C Backward selection will use a model that include all
the features x1, x2, x3, and x4.
</p>
<p>D Forward selection will select the features x1, x2 and
x4.
</p>
<p>E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.4 Visualizing learning curvesF 183
</p>
<p>10.3. Spring 2013 question 14: Which one of the
following statements is incorrect?
</p>
<p>A Cross-validation can be used to quantify a models
generalization error.
</p>
<p>B K-fold cross-validation requires the fitting of K mod-
els such that for K=N where N is the total number
of observations K-fold cross-validation is the same as
leave-one-out cross-validation.
</p>
<p>C When using cross-validation to estimate model pa-
rameters an extra level of cross-validation is needed
in order to evaluate how well the model generalizes
to new data.
</p>
<p>D For least squares linear regression the test error will
always decrease as we include more attributes in the
model.
</p>
<p>E Don’t know.
</p>
<p>10.4. Fall 2013 question 7: We would like to fit an
artificial neural network (ANN) model to the Galagapos
dataset shown in Table 10.6. To reduce the computa-
tional cost of fitting the models it was decided to not
include Elev, i.e. x4, and AreaNI, i.e. x7 in the ANN
models. We therefore only consider x1, x2, x5 and x6 in
order to predict Area, i.e x3. An artificial neural net-
work with three hidden units is applied to the data with
these four attributes and trained using different combina-
tions of the four attributes x1, x2, x5 and x6. Table 10.7
gives the training and test performance in terms of root
mean squared error (rmse) of the ANN for different com-
binations of the considered attributes. Which one of the
following statements is correct?
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Number of plant species Plants
x2 Number of endemic plant species E-Plants
x3 Area of island (in km
</p>
<p>2) Area
x4 Max. elevation above sea-level (in m) Elev
x5 Distance to nearest island (in km) DistNI
x6 Distance to Santa Cruz Island (in km) StCruz
x7 Area of adjacent island (in km
</p>
<p>2) AreaNI
</p>
<p>Table 10.6: The seven attributes of the data on a selec-
tion of 29 of the Galápagos islands.
</p>
<p>Feature(s) Training Test
rmse rmse
</p>
<p>x1 81.2 80.1
x2 62.3 84.3
x5 68.0 72.9
x6 98.9 100.5
</p>
<p>x1 and x2 57.1 69.1
x1 and x5 40.2 43.3
x1 and x6 55.2 66.4
x2 and x5 32.2 36.3
x2 and x6 20.3 22.5
x5 and x6 48.4 50.3
</p>
<p>x1 and x2 and x5 36.6 39.1
x1 and x2 and x6 18.8 23.0
x1 and x5 and x6 33.3 36.7
x2 and x5 and x6 40.4 43.0
</p>
<p>x1 and x2 and x5 and x6 30.0 35.2
</p>
<p>Table 10.7: Root mean square error (rmse) for the train-
ing and test set when using an artificial neural network
with three hidden units to predict the Area of an island
based only on the four attributes x1, x2, x5 and x6 us-
ing the hold-out method with 40 % of the observations
hold-out for testing.
</p>
<p>A Neither forward nor backward selection will identify
the optimal feature combination for this problem.
</p>
<p>B Forward selection will result in a better model being
selected than using backward selection.
</p>
<p>C Backward selection will terminate at the model that
includes the features x1, x2, and x6.
</p>
<p>D Forward selection will select the features x2 and x5.
E Don’t know.
</p>
<p>10.5. Fall 2013 question 16: A logistic regression
model was trained on Haberman’s data shown in Ta-
ble 10.8 using leave-one-out cross-validation and the con-
fusion matrices given in Figure 10.17 were obtained.
Which one of the following statements is correct?</p>
<p></p>
</div>, <div class="page"><p></p>
<p>184 10 Overfitting and performance evaluation
</p>
<p>Fig. 10.17: The confusion matrix for the logistic regres-
sion and decision tree classifiers used to predict survival
based on leave-one-out cross validation.
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Young (&lt; 60 years), x1 = 0 or Age
Old (≥ 60 years), x1 = 1
</p>
<p>x2 Operated before, x2 = 0 or OpT
after 1960, x2 = 1
</p>
<p>x3 Positive axillary nodes detected PAN
No, x3 = 0 or Yes, x3 = 1
</p>
<p>y Lived after 5 years Surv
No, y = 0 or Yes, y = 1
</p>
<p>Table 10.8: A modified version of Haberman’s Survival
Data taken from http://archive.ics.uci.edu/ml/machine-
learning-databases/haberman/haberman.names. The at-
tributes x1-x3 denoting the age, operation time and can-
cer size as well as the output denoting survival after five
years are binary. The data contains a total of N = 306
observations.
</p>
<p>A The error rate of the logistic regression classifier is
larger than the error rate of the decision tree classi-
fier.
</p>
<p>B Predicting every observation to be in the died class
would give a better accuracy than the accuracy ob-
tained by the decision tree classifier.
</p>
<p>C The classification problem does not have any issues
of imbalanced classes.
</p>
<p>D Using leave-one-out cross validation a total of 306−
1 = 305 logistic regression models are trained.
</p>
<p>E Don’t know.
</p>
<p>10.6. Fall 2014 question 6: Suppose a neural network
is trained on input/output pairs (xi, yi) on a dataset of
N = 5 observations. The response of the neural network
for input xi is denoted as ŷi and the values can be seen
in table 10.9.
</p>
<p>i yi ŷi
1 1 0.6
2 0 0.4
3 1 0.5
4 1 0.1
5 0 0.1
</p>
<p>Table 10.9: Table desired responses and output of the
neural network
</p>
<p>We convert the continuous output ŷi to a proper bi-
nary class label by the transformation
</p>
<p>ŷi ←
{
</p>
<p>1 if ŷi ≥ θ
0 otherwise
</p>
<p>Which of the following values of θ will give the highest
accuracy for binarized outputs?
</p>
<p>A θ = 0.35
B θ = 0.45
C θ = 0.55
D θ = 0.65
E Don’t know.
</p>
<p>10.7. Fall 2013 question 25: Which one of the follow-
ing statements pertaining to cross-validation is correct?
</p>
<p>A 2-fold cross-validation is the same as the hold-out
method when 50% is hold out.
</p>
<p>B For datasets with very few observations it is in
general better to use leave-one-out cross-validation
rather than 10-fold cross-validation.
</p>
<p>C Two levels of cross-validation is necessary in order to
determine the optimal set of parameters for a model.
</p>
<p>D Leave-one-out cross validation is the most computa-
tional efficient procedure as only one observation is
in the test set at a time.
</p>
<p>E Don’t know.
</p>
<p>10.8. Fall 2014 question 18: Consider a system which
attempts to classify observations based on the value of
four observed features x1, x2, x3, x4. Suppose we wish to
examine which subset of features gives the least misclas-
sified observations on test data. In table 10.10 is shown
how different combinations of features give rise to dif-
ferent number of misclassified observations on a training
and test set. Which of the following statements is true?</p>
<p></p>
</div>, <div class="page"><p></p>
<p>10.4 Visualizing learning curvesF 185
</p>
<p>Feature(s) Ctrain Ctest
None 30 69
x1 43 70
x2 14 50
x3 41 76
x4 18 81
</p>
<p>x1, x2 59 73
x1, x3 34 59
x1, x4 15 32
x2, x3 18 58
x2, x4 23 36
x3, x4 26 33
</p>
<p>x1, x2, x3 17 40
x1, x2, x4 37 54
x1, x3, x4 7 15
x2, x3, x4 27 34
</p>
<p>x1, x2, x3, x4 17 25
</p>
<p>Table 10.10: Number of misclassified training observa-
tions Ctrain and test observations Ctest for a neural net-
work model trained on different sets of features. Lower
is better.
</p>
<p>A Forward and backward selection will select the same
set of features.
</p>
<p>B Forward selection will select a model with higher mis-
classification rate on the test set than backward se-
lection.
</p>
<p>C Forward selection will select a model with lower mis-
classification rate on the test set than backward se-
lection.
</p>
<p>D Forward selection will select the features x1, x3, x4.
E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>11
</p>
<p>Performance evaluation
</p>
<p>This chapter will be updated soon. It will discuss ways to statistically compare two classifiers
according to their generalization error.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>12
</p>
<p>Nearest neighbour methods
</p>
<p>Classifying observations based on the labels of their nearest neighbours is a simple idea and has
been re-invented several times, however the first discussion of a nearest-neighbour classification
rule was by statisticians Evelyn Fix and Joseph Hodges in an (unpublished!) technical report in
1951 [Fix and Hodges, 1951].
</p>
<p>12.1 K-nearest neighbour classification
</p>
<p>We will introduce the K-nearest neighbour classifier with an example. In fig. 12.1 is shown a subset
of the Fisher Iris data where only the two first attributes are plotted. Suppose we ask a human
to guess the name of a flower at the black square. Most humans would properly say Setosa (the
blue class) because the nearby flowers are predominantly blue. For similar reasons, labelling an
observation at the black cross would be more difficult. Two things seem to play a role
</p>
<p>• The closeness of the nearby points.
• How many there are of a particular color.
</p>
<p>This intuition is the idea behind the K-nearest neighbour method. Consider again the data in
fig. 12.1 but focus on a test point at the black cross. Imagine we draw a circle around the black
cross and slowly increases its radius. At some point the circle will contain one point which (as
it happens) is yellow in our case, see the upper-left pane of fig. 12.2 where the selected point is
highlighted with red. If we increases the radius of the circle it will at some point contain two and
then three points, see the upper-right pane of fig. 12.2 where these correspond to a yellow and two
green points. In general, we can define the K-neighbourhood of a point x as: 1
</p>
<p>NX(x,K) = {The K observations in X which are nearest to x} . (12.1)
</p>
<p>Notice we have included the dataset X in the above definition, however, sometimes the K-
neighbourhood is simply written as N(x,K) if it is clear from the context what X is. The lower-row
of fig. 12.2 shows NX(x,K = 5) and NX(x,K = 7) respectively. A simple classification method
</p>
<p>1 What if several points have the same distance to x? This borderline case might appear in the case
where there are duplicate observations in the dataset and can be handled in several ways, for instance
by (randomly) selecting between the tied points. We will assume such a scheme exist and therefore the
K-neighbourhood always consist of K observations.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>190 12 Nearest neighbour methods
</p>
<p>setosa
</p>
<p>versicolor
</p>
<p>virginica
</p>
<p>x
</p>
<p>y
</p>
<p>4.5 5 5.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>Fig. 12.1: A subset of the Fisher Iris dataset where we only consider two features. Which class and
why would you assign the test points at the black square and cross if you were to just make a
qualified guess?
</p>
<p>is then to fix K (for instance K = 5) and just assign x to the class which has the most elements
in NX(x,K). In the case where two classes has equally many members (we say they are tied), for
instance the lower-right pane of fig. 12.2, x is assigned to the class which has a closest member
to x in NX(x,K), in this case the green class. This is simply the KNN classification rule for an
observation x:
</p>
<p>• Compute NX(x,K).
• Classify x to the class k which has the most members in NX(x,K).
• In the case of ties, simply classify x to the class which has a member nearest to x.
</p>
<p>An alternative tie-breaking rule is simply to select a random of the tied classes. Notice in particular
the case where K = 1, here we simply assign x to the class of the nearest observation in the training
set. This is known as the nearest neighbour classification rule. In fig. 12.3 is shown the classification
boundary for the full problem, i.e. the colors indicate what class a point at that given location will
be classified to for K = 1, 3, 5, 7.
</p>
<p>12.1.1 A Bayesian view of the KNN classifierF
</p>
<p>As presented, the KNN classifier is simply a heuristic. However, it is possible to give the KNN
classifier a Bayesian interpretation [Bishop, 2013]. Suppose we denote by Kc the number of elements
in NX(x,K) (we will suppress X in this section) which belong to class c and Nc the number of
observations in the entire dataset which belongs to class c:
</p>
<p>Kc = Number of observations xi ∈ NX(x,K) where yi = c. (12.2)
Nc = Number of observations xi where yi = c. (12.3)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>12.1 K-nearest neighbour classification 191
</p>
<p>K = 1
</p>
<p>4.5 5 5.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>K = 3
</p>
<p>4.5 5 5.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>K = 5
</p>
<p>4.5 5 5.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>K = 7
</p>
<p>4.5 5 5.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>Fig. 12.2: Illustration of the K-nearest neighbourhood NX(x,K) of the black cross x for K =
1, 3, 5, 7, observations within the neighbourhood is highlighted with the red circles. The KNN clas-
sifier simply assigns the observation x to the class with the most observations within the circle.
</p>
<p>Then clearly K =
∑C
c=1Kc and N =
</p>
<p>∑C
c=1Nc. If we select a random observation, the probability
</p>
<p>it belongs to class c is:
</p>
<p>p(y = c) =
Nc
N
</p>
<p>Then, notice for any volume V by the definition of probability:∫
V
</p>
<p>p(x|y = c)dx = {Probability an observation of class c is in V }
</p>
<p>If we now consider the volume V to be the size of the K-nearest neighbourhood of x, i.e. the area
of the discs in fig. 12.2) around x, the left-hand side and right-hand side of the above becomes:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>192 12 Nearest neighbour methods
</p>
<p>K = 1
</p>
<p>4.5 5 5.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>K = 3
</p>
<p>4.5 5 5.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>K = 5
</p>
<p>4.5 5 5.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>K = 7
</p>
<p>4.5 5 5.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>3
</p>
<p>3.5
</p>
<p>Fig. 12.3: KNN classification boundary for the problem in fig. 12.1 for K = 1, 3, 5, 7. Notice as K
increases, the boundary becomes more smooth. For K = 3 (upper-right corner), the blue point in
the lower left corner is able to induce a small blue area due to the tie-breaking rule.
</p>
<p>{lhs.} =
∫
V
</p>
<p>p(x|y = c)dx ≈ V p(x|y = c)
</p>
<p>{rhs.} = Number of observations of class y = c in Vx
Total number of observations of class c
</p>
<p>≈ Kc
Nc
</p>
<p>If we put this together we obtain p(x|y = c) = KcNcV . Then simply applying Bayes theorem we
obtain:
</p>
<p>p(y = c|x) = p(x|y = c)p(y = c)∑C
c′=1 p(x|y = c′)p(y = c′)
</p>
<p>=
Kc
K
</p>
<p>(12.4)
</p>
<p>So when the KNN classification method selects the class c where Kc is the highest it corresponds
to selecting the most probable class according to Bayes theorem and the above approximations.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>12.2 K-nearest neighbour regression 193
</p>
<p>x
</p>
<p>y
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1 Mean
</p>
<p>f(x,K = 1)
</p>
<p>x
</p>
<p>y
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Mean
</p>
<p>f(x,K = 2)
</p>
<p>x
</p>
<p>y
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1 Mean
</p>
<p>f(x,K = 3)
</p>
<p>x
</p>
<p>y
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 12.4: 1D example dataset (upper-right pane) generated as noisy observations of the sinusoidal
signal. KNN regression for an observation x indicated by the vertical dotted line first finds the
K-nearest neighbourhood of x, NX(x,K), and then simply outputs the mean of the observations
yi in NX(x,K). The three panes illustrates K = 1, 2, 3.
</p>
<p>12.2 K-nearest neighbour regression
</p>
<p>The K-nearest neighbour classification rule is easily modified for regression. Suppose we have the
dataset shown in fig. 12.4 (top left pane) which consists of N = 16 noisy observations of a sinusoidal
signal. If we wish to make predictions around x = 4 (the vertical bar), this can be accomplished
finding the K closest elements to x in the dataset, NX(x,K), (shown as the circles) and simply
predicting the mean value of the elements in NX(x,K) (shown as the horizontal bar). The prediction
at x = 4 is then just the red square. In general, the prediction rule is:
</p>
<p>f(x,K) =
1
</p>
<p>K
</p>
<p>∑
i∈NX(x,K)
</p>
<p>yi. (12.5)
</p>
<p>which of course works for arbitrary dimensions. Notice in particular theK = 1 prediction rule simply
corresponds to finding the observation xi closest to a test point x and predicting f(x,K = 1) = yi.
In fig. 12.5 the prediction rule is visualized for K = 1, 2, 3, 4. Notice, as K increases the rule becomes
less driven by an error in any single value (less variation), however, it also becomes more biased
towards predictions near the mean.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>194 12 Nearest neighbour methods
</p>
<p>f(x,K = 1), d = 0
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>f(x,K = 2), d = 0
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>f(x,K = 3), d = 0
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>f(x,K = 4), d = 0
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 12.5: Illustration of the prediction rule for KNN regression from fig. 12.4 (red line) when
computed over the entire dataset for K = 1, 2, 3, 4. Notice the prediction rule is piece-wise linear
corresponding to different neighbourhoods.
</p>
<p>12.2.1 Higher-order KNN regressionF
</p>
<p>If we return to the KNN regression dataset in fig. 12.4 and the prediction curves in fig. 12.5 notice
that the curve is piece-wise constant. It might be better if the curve within each neighbourhood
is fitted to the dataset with a more powerful model. A simple way to obtain this is to rather
than predicting the mean within each local neighbourhood, fitting a polynomial of degree d. The
piece-wise linear model then corresponds to d = 0 (the constant polynomial). This is illustrated in
fig. 12.6 for K = 3, 5 and d = 1, 2.
</p>
<p>The corresponding prediction curve is shown in fig. 12.7. Compared to the piece-wise linear case
in fig. 12.5, the high-order polynomials allow much smoother interpolation of the underlying curve,
however, in general they also require higher values of K in order not to overfit locally.
</p>
<p>12.3 Cross-validation and nearest-neighbour methods
</p>
<p>Selecting K in nearest neighbour methods can easily be accomplished using cross-validation. Simply
let M1, · · · ,MS in algorithm 5 correspond to different values of K (for instance K = 1, · · · , S)
and let the error measure be (in case of classification) the classification error, or for regression the
sum of square error. One-layer cross-validation for model selection can be used to select K and</p>
<p></p>
</div>, <div class="page"><p></p>
<p>12.3 Cross-validation and nearest-neighbour methods 195
</p>
<p>1-degree regression
</p>
<p>f(x,K = 3)
</p>
<p>x
</p>
<p>y
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1 2-degree regression
</p>
<p>f(x,K = 3)
</p>
<p>x
</p>
<p>y
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1-degree regression
</p>
<p>f(x,K = 5)
</p>
<p>x
</p>
<p>y
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1 2-degree regression
</p>
<p>f(x,K = 5)
</p>
<p>x
</p>
<p>y
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 12.6: KNN regression can be extended by instead of computing the mean within each region,
we carry out a polynomial regression (similar to the example encountered in the previous section
on linear regression) for the observations in each neighbourhood NX(x,K). For different degree
d = 2, 3 this allows smoother regression curves, but higher d also requires a larger neighbourhood.
</p>
<p>two-layer cross-validation used for selecting K and estimating the generalization error. A particu-
larly useful simplification is when we apply leave-one-out cross-validation. Recall in leave-one-out
cross-validation we have to iterate over all observations in our data set, train a model on N − 1
observations and test on the left-out observation. In the case of nearest-neighbour methods this can
be accomplished by first defining X\i as X with observation xi removed:
</p>
<p>XT\i =
[
x1 x2 · · ·xi−2 xi−1 xi+1 xi+2 · · · xN
</p>
<p>]
(12.6)
</p>
<p>then NX\i(xi,K) is the K-neighbourhood of xi when xi has been left out of the dataset X. The
generalization error for a given value of K can then be estimated as:
</p>
<p>ẼgenK =
1
</p>
<p>N
</p>
<p>[
N∑
i=1
</p>
<p>Error of observation xi when predicted using NX\i(xi,K)
</p>
<p>]
</p>
<p>Where the error in question could be either classification error or the sum-of-square error in case
of regression. As usual the generalization error is estimated for each K and the K with the lowest
generalization error is selected.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>196 12 Nearest neighbour methods
</p>
<p>f(x,K = 3), d = 1
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>f(x,K = 3), d = 2
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>f(x,K = 5), d = 1
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>f(x,K = 5), d = 2
</p>
<p>0 2 4 6 8 10
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 12.7: The prediction curves for KNN regression with polynomials introduced in fig. 12.6, here
shown for linear polynomials and second-degree polynomials for different neighborhood sizes.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>12.3 Cross-validation and nearest-neighbour methods 197
</p>
<p>Problems
</p>
<p>12.1. Fall 2013 question 9: In order to predict if an
observation corresponds to a relatively small or large is-
land we will use a k-nearest neighbor (KNN) classifier
based on the Euclidean distance between the eight ob-
servations given in Table 12.1. We will use leave-one-out
cross-validation for the KNN in order to classify whether
the eight considered observations constitute small islands
(given in red, i.e. observation O1, O2, O3, O4) or large
island (given in blue, i.e. observation O5, O6, O7, O8)
using a three-nearest neighbor classifier, i.e. K = 3. The
analysis will be based only on the data given in Ta-
ble 12.1. Which one of the following statements is cor-
rect?
</p>
<p>O1 O2 O3 O4 O5 O6 O7 O8
</p>
<p>O1 0 2.39 1.73 0.96 3.46 4.07 4.27 5.11
O2 2.39 0 1.15 1.76 2.66 5.36 3.54 4.79
O3 1.73 1.15 0 1.52 3.01 4.66 3.77 4.90
O4 0.96 1.76 1.52 0 2.84 4.25 3.80 4.74
O5 3.46 2.66 3.01 2.84 0 4.88 1.41 2.96
O6 4.07 5.36 4.66 4.25 4.88 0 5.47 5.16
O7 4.27 3.54 3.77 3.80 1.41 5.47 0 2.88
O8 5.11 4.79 4.90 4.74 2.96 5.16 2.88 0
</p>
<p>Table 12.1: Pairwise Euclidean distance, i.e d(Oa,Ob) =
||xa − xb||2 =
</p>
<p>√∑
m(xam − xbm)2, between eight ob-
</p>
<p>servations of the Galápagos data. Red observations (i.e.,
O1, O2, O3, and O4) correspond to the four smallest is-
lands whereas blue observations (i.e., O5, O6, O7, and
O8) correspond to the four largest islands.
</p>
<p>A The error rate of the classifier will be 1/8
B The error rate of the classifier will be 1/4
C The error rate of the classifier will be 3/8
D The error rate of the classifier will be 1/2
E Don’t know.
</p>
<p>12.2. Fall 2014 question 23: Consider a two-
dimensional data set consisting of N = 7 observations
as shown in fig. 12.8. The dataset consist of two classes
indicated by the black crosses (class 1) and red circles
(class 2). In the figure, the decision boundary for four K-
nearest neighbor classifier (KNN) is shown such that the
lighter brown color indicates Class 2 (red circles) and the
darker brown color indicates Class 1 (black crosses). Sup-
pose K is restricted to the values 1, 3, 5, 7, which of the
following statements are true about values of k1, k2, k3
and k4?
</p>
<p>
</p>
<p>
</p>
<p>K = k2
</p>
<p>K = k4
</p>
<p>Class 2
</p>
<p>Class 1
</p>
<p>K = k1
</p>
<p>K = k3
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−0.1
</p>
<p>0
</p>
<p>0.1
</p>
<p>−0.1
</p>
<p>0
</p>
<p>0.1
</p>
<p>−0.1
</p>
<p>0
</p>
<p>0.1
</p>
<p>−0.1
</p>
<p>0
</p>
<p>0.1
</p>
<p>Fig. 12.8: Decision boundaries for four KNN classifiers.
</p>
<p>A k1 = 1, k2 = 7, k3 = 3, k4 = 5
B k1 = 1, k2 = 5, k3 = 7, k4 = 3
C k1 = 5, k2 = 7, k3 = 1, k4 = 3
D k1 = 3, k2 = 7, k3 = 1, k4 = 5
E Don’t know.
</p>
<p>12.3. Fall 2014 question 12: Consider again the dis-
tances in table 12.2. We will predict the label indicated
by the blue color, i.e. observations o1, . . . , o4 belong to
class C1 and observations o5, . . . , o8 to class C2. This
will be done using a k-nearest neighbor (KNN) classifier
based on the cityblock distance measure indicated in ta-
ble 12.2. We will use leave-one-out cross validation (i.e.
the observation that is being predicted is left out) using
one-nearest classifier, i.e. k = 1. What is the accuracy if
all N = 8 observations are classified?
</p>
<p>o1 o2 o3 o4 o5 o6 o7 o8
o1 0 4 7 9 5 5 5 6
o2 4 0 7 7 7 3 7 8
o3 7 7 0 10 6 6 4 9
o4 9 7 10 0 8 6 10 9
o5 5 7 6 8 0 8 6 7
o6 5 3 6 6 8 0 8 11
o7 5 7 4 10 6 8 0 7
o8 6 8 9 9 7 11 7 0
</p>
<p>Table 12.2: Pairwise Cityblock distance, i.e d(oi, oi) =
</p>
<p>‖xi − xj‖1 =
∑M
k=1 |xik − xjk|, between 8 observations.
</p>
<p>Each observation oi corresponds to a M = 15 dimen-
sional binary vector, xik ∈ {0, 1}. The blue observations
{o1, o2, o3, o4} belong to class C1 and the black observa-
tions {o5, o6, o7, o8} belong to class C2.
</p>
<p>A accuracy = 18
B accuracy = 14
C accuracy = 12
D accuracy = 58
E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>13
</p>
<p>Bayesian methods
</p>
<p>Bayesian methods is the application of the basic rules of probability, in particular Bayes’ theorem
</p>
<p>p(y|x) = p(x|y)p(y)∑C
y′=1 p(x|y′)p(y′)
</p>
<p>to machine learning. We have already seen several such application, for instance the analysis of the
coin in ?? and as an element of the credibility intervals in chapter 10. However, in this chapter we
will consider the problem more heads on and discuss additional terminology particular to Bayesian
methods, namely Bayesian networks also called Bayesian belief networks, which is a graphical way
of representing a distribution of many variables. Before this we will consider the distinction between
discriminative and generative models.
</p>
<p>13.1 Discriminative and generative modelling
</p>
<p>Consider a standard classification problem in which we try to determine what class yi an observation
xi belongs to. For instance, consider trying to learn to distinguish between cats (yi = 0) and dogs
(yi = 1) based on features xi of each animal. Logistic regression essentially tries to fit a straight line
– a decision boundary– that separates the cats from the dogs. A new instance xi is then classified
by observing which side of the decision boundary it lies on. This can be seen as directly coming up
with a mapping of
</p>
<p>p(y|x,w). (13.1)
</p>
<p>This is known as discriminative analysis. Bayesian inference also tries to determine this mapping,
but considers a very different approach. First, we look at all instances of cats, and then we build a
model of what cats look like. Then we look at all instances of dogs, and we build a model of what
dogs look like. Then to classify a new instance, we consider how well it corresponds to what we
expect a cat or a dog will look like according to respectively the cat model and the dog model, and
we make our decision accordingly. This way of first coming up with models of what respectively dogs
and cats look like is known as generative modelling and Bayesian inference naturally corresponds
to generative modelling.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>200 13 Bayesian methods
</p>
<p>y = 0
</p>
<p>y = 1
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−2 −1 0 1 2 3
−3
</p>
<p>−2.5
</p>
<p>−2
</p>
<p>−1.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>Fig. 13.1: Example of a Bayes classifier fitted to a two-class example dataset. The contours indicate
the multivariate Gaussians fitted to each of the two classes separately.
</p>
<p>13.1.1 Bayes classifier
</p>
<p>Continuing the above discussion, consider Bayes’ theorem in the two-class setting:
</p>
<p>p(y|x) = p(x|y)p(y)
p(x|y = 0)p(y = 0) + p(x|y = 1)p(y = 1) (13.2)
</p>
<p>So when this model considers if a new animal should be classified as a cat, y = 0, it considers how
much the animal looks like a cat
</p>
<p>p(x|y = 0),
multiply by the prior probability the animal is a cat p(y = 0) and divide this by the same quantity
including the similar expression for dogs, p(x|y = 1) and p(y = 1). To make this more concrete, let’s
suppose we observe n0 instances of cats, X
</p>
<p>Cats, and n1 instances of dogs, X
Dogs, each observation
</p>
<p>consisting of two features. The labelled dataset is plotted in fig. 13.1. Then we can model the
observations for instance as two multivariate normal distributions
</p>
<p>p(x|y = 0) = N (x|µ0,Σ0), (13.3)
p(x|y = 1) = N (x|µ1,Σ1), (13.4)
</p>
<p>where the parameters µ0,Σ0 and µ1,Σ1 can be estimated from the data as (here given for Cats):
</p>
<p>µ0 =
1
</p>
<p>n0
</p>
<p>n0∑
i=1
</p>
<p>xCatsi , and Σ0 =
1
</p>
<p>n0 − 1
</p>
<p>n0∑
i=1
</p>
<p>(xCatsi − µ0)(xCatsi − µ0)T , (13.5)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>13.2 Näıve-Bayes classifier 201
</p>
<p>y = 0
</p>
<p>y = 1
</p>
<p>x1
x2
</p>
<p>p
(y
</p>
<p>=
0|
x
)
</p>
<p>−2
−1
</p>
<p>0
1
</p>
<p>2
3
</p>
<p>−3
−2
</p>
<p>−1
0
</p>
<p>1
</p>
<p>2
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 13.2: Decision rule, i.e. the probability p(y = 0|x), of the Bayes classifier when trained on the
two-class dataset from fig. 13.1. Notice, the decision rule is quite steep at the boundary.
</p>
<p>corresponding to the two contour plots in fig. 13.1. Using this together with the priors p(y = 0) =
n0
</p>
<p>n0+n1
, p(y = 1) = n1n0+n1 allows us to compute the probability the animal belongs to either of the
</p>
<p>two classes as:
</p>
<p>p(y = c|x) = p(x|y = c)p(y = c)
p(x|y = 0)p(y = 0) + p(x|y = 1)p(y = 1) . (13.6)
</p>
<p>The decision boundary, i.e. p(y = 0|x), is plotted in fig. 13.2 as the gray surface.
</p>
<p>13.2 Näıve-Bayes classifier
</p>
<p>Näıve-Bayes is simply the standard Bayesian approach with a particular simplification. Consider
the Bayes classifier for C classes using a dataset with M features:
</p>
<p>p(y|x1, x2, . . . , xM ) =
p(x1, x2, . . . , xM |y)p(y)∑C
</p>
<p>c=1 p(x1, . . . , xM |y = c)p(y = c)
(13.7)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>202 13 Bayesian methods
</p>
<p>A problem with the Bayes classifier is if M is very large, representing the conditional distribution
</p>
<p>p(x1, x2, . . . , xM |y),
</p>
<p>may be very expensive. For instance, for the multivariate normal distribution this requires storing
a symmetric covariance matrix Σ and the mean vector µ, in total M + 12M(M + 1) numbers. This
is not only costly, but if we do not have much data, estimating this many parameters may not be
possible to do reliably. The Näıve-Bayes assumption is simply that we assume that the conditional
distribution factorizes:
</p>
<p>p(x1, x2, . . . , xM |y) = p(x1|y)p(x2|y) . . . p(xM |y).
</p>
<p>If we still represent each factor as a 1D normal distribution this only requires 2M numbers (i.e.,
the mean value and variance for each of the attributes x1, x2, . . . , xM ). Plugging this into eq. (13.7)
we obtain:
</p>
<p>p(y|x1, x2, . . . , xM ) =
p(x1|y)× · · · × p(xM |y)p(y)∑C
</p>
<p>c=1 p(x1|y = c)× · · · × p(xM |y = c)p(y = c)
. (13.8)
</p>
<p>We will illustrate the procedure with an example. Consider the data shown in table 13.1. The
dataset consists of N = 8 students and for each student we record two binary features x1 and x2
corresponding to the sex of the student and if the student is typically going out in the evening
or not. The first column y = 1, 2, 3 corresponds to the grade of the student, where y = 1 means
a low grade, y = 2 means a medium grade and y = 3 a high grade. Suppose we want to train
</p>
<p>y x1 x2
1 1 0
1 0 1
1 1 1
2 1 1
2 1 0
2 0 0
3 1 1
3 0 1
</p>
<p>Table 13.1: A dataset consisting of N = 8 students and for each student we record two binary
features x1 and x2 corresponding to the sex of the student and if the student is typically going out
in the evening or not. The first column y = 1, 2, 3 correspond to the grade of the student, where
y = 1 means a low grade, y = 2 a medium grade and y = 3 a high grade.
</p>
<p>a näıve-Bayes classifier on the dataset and use it to determine the probability a new observation
x1 = 0 and x2 = 1 belong to any of the three classes. We first compute the class-priors to be
</p>
<p>p(y = 1) = p(y = 2) =
3
</p>
<p>8
and p(y = 3) =
</p>
<p>2
</p>
<p>8
=
</p>
<p>1
</p>
<p>4
</p>
<p>Then we can compute the probability of p(xi = 0|y = c) as:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>13.2 Näıve-Bayes classifier 203
</p>
<p>p(xi = 0|y = c) =
Number of times where xi = 0 and y = c
</p>
<p>Total number of times where y = c
. (13.9)
</p>
<p>In particular, we obtain:
</p>
<p>p(x1 = 0|y = 1) =
1
</p>
<p>3
, p(x1 = 0|y = 2) =
</p>
<p>1
</p>
<p>3
, p(x1 = 0|y = 3) =
</p>
<p>1
</p>
<p>2
,
</p>
<p>p(x2 = 0|y = 1) =
1
</p>
<p>3
, p(x2 = 0|y = 2) =
</p>
<p>2
</p>
<p>3
, p(x2 = 0|y = 3) =
</p>
<p>0
</p>
<p>2
.
</p>
<p>Using that p(x2 = 1|y = c) = 1 − p(x2 = 0|y) we then compute the probability of the new
observation as
</p>
<p>p(x1 = 0, x2 = 1|y = 1) = p(x1 = 0|y = 1)p(x2 = 1|y = 1) =
1
</p>
<p>3
× (1− 1
</p>
<p>3
) =
</p>
<p>2
</p>
<p>9
,
</p>
<p>p(x1 = 0, x2 = 1|y = 2) = p(x1 = 0|y = 2)p(x2 = 1|y = 2) =
1
</p>
<p>3
× (1− 2
</p>
<p>3
) =
</p>
<p>1
</p>
<p>9
,
</p>
<p>p(x1 = 0, x2 = 1|y = 3) = p(x1 = 0|y = 3)p(x2 = 1|y = 3) =
1
</p>
<p>2
× (1− 0) = 1
</p>
<p>2
.
</p>
<p>In our case p(y=c|x1 =0, x2 =1) can then be computed using eq. (13.8) to be:
</p>
<p>p(x1 =0, x2 =1|y=c)p(y=c)
p(x1 =0, x2 =1|y=1)p(y=1) + p(x1 =0, x2 =1|y=2)p(y=2) + p(x1 =0, x2 =1|y=3)p(y=3)
</p>
<p>and simply plugging in the above numbers we obtain
</p>
<p>p(y = 1|x1 = 0, x2 = 1) =
2
9 × 38
</p>
<p>2
9 × 38 + 19 × 38 + 12 × 14
</p>
<p>=
1
</p>
<p>3
,
</p>
<p>p(y = 2|x1 = 0, x2 = 1) =
1
9 × 38
</p>
<p>2
9 × 38 + 19 × 38 + 12 × 14
</p>
<p>=
1
</p>
<p>6
,
</p>
<p>p(y = 3|x1 = 0, x2 = 1) =
1
2 × 14
</p>
<p>2
9 × 38 + 19 × 38 + 12 × 14
</p>
<p>=
1
</p>
<p>2
.
</p>
<p>The näıve-Bayes assumption is often used when the number of features M is very large; a popular
application is spam-filtering where each of the binary features correspond to the presence or absence
of a word in the email.
</p>
<p>13.2.1 Robust estimation
</p>
<p>Consider again the previous example where Näıve-Bayes was applied to the example in table 13.1.
As part of the computation we made approximations of the form:
</p>
<p>p(x = 0) =
n−
</p>
<p>n+ + n−
, p(x = 1) =
</p>
<p>n+
</p>
<p>n+ + n−
,
</p>
<p>where n− was the number of times we observed x = 0 and n+ was the number of times we observed
x = 1. A drawback of this way of estimating the probability p(x = 0) or p(x = 1) is that we might</p>
<p></p>
</div>, <div class="page"><p></p>
<p>204 13 Bayesian methods
</p>
<p>get probabilities of 0 which will dominate all other probabilities in the computation when multiplied
together. A numerically safer way to get around this problem is to use a robust esimator such as:
</p>
<p>p(x = 1) =
n+ + b+
</p>
<p>n+ + n− + b+ + b−
</p>
<p>where b+ and b− are so-called “pseudo-counts” (consider for instance if b+ = b− = 1 then using
the above robust estimator is simply equivalent to assuming we had “pseudo-observed” a member
of the negative and positive class). It is common to select b− = b+ = 12 or b
</p>
<p>− = b+ = 1, however in
general the value of b+, b− could be found by cross-validation.
</p>
<p>Where does the robust estimator come from?*
</p>
<p>Suppose we define θ as the chance of observing a positive outcome of x. Then the above information
is exactly equivalent to the Bernoulli-coin example we encountered previously in ?? and in particular
??. If we instead of the Beta( 12 ,
</p>
<p>1
2 ) prior had chosen a general Beta(b
</p>
<p>+, b−) prior the posterior ??
would be:
</p>
<p>p(θ|n+, n−, b+, b−) = Beta(θ|n+ + b+, n− + b−).
The probability of observing a positive outcome P (x = 1) can then be obtained as the average of
θ:1
</p>
<p>P (x = 1) =
</p>
<p>∫
p(x = 1|θ)p(θ|n+, n−, b+, b−)dθ =
</p>
<p>∫
θBeta(θ|n++b+, n−+b−)dθ = n
</p>
<p>+ + b+
</p>
<p>n+ + b+ + n− + b−
.
</p>
<p>Thus there is nothing “special” about this robust estimator; it is simply about a Bayesian analysis
of the chance of observing a positive outcome of x using Beta(b+, b−) prior.
</p>
<p>13.3 Bayesian networks
</p>
<p>A Bayesian network also called a belief network or Bayesian belief network is not as such adding
a new method to our toolbox, but it provides a convenient and often-used notation for presenting
existing probabilities. Consider the following example adapted from Pearl [2014], MacKay [2003]
</p>
<p>Fred lives in Los Angeles and commutes 60 miles to work. Whilst at work, he receives
a phone-call from his neighbour saying that Fred’s burglar alarm is ringing. What is the
probability that there was a burglar in his house today? While driving home to investigate,
Fred hears on the radio that there was a small earthquake that day near his home. ‘Oh’, he
says, feeling relieved, ‘it was probably the earthquake that set off the alarm’. What is the
probability that there was a burglar in his house?
</p>
<p>To analyse this story we first introduce the variables:
</p>
<p>a : The alarm is ringing.
</p>
<p>b : A burglar was in Fred’s house.
</p>
<p>c : Fred received a phone-call reporting the alarm.
</p>
<p>e : A small earthquake took place today near Fred’s house.
</p>
<p>r : The radio report of the earthquake is heard by Fred.
</p>
<p>1 If θ follows a beta distribution then the average of θ is: E[θ] =
∫
θBeta(θ|a, b)dθ = a
</p>
<p>a+b
.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>13.3 Bayesian networks 205
</p>
<p>Earthquake Burglar
</p>
<p>Radio Alarm
</p>
<p>Phonecall
</p>
<p>Fig. 13.3: Bayesian network of the burglar example. Each vertex corresponds to a variable, and
incident edges corresponds to conditional dependence.
</p>
<p>In a case like this, we know (from our experience) that some of these events must be independent.
That there is a burglar or a minor earthquake is presumable unrelated events, so p(b, e) = p(b)p(e).
In general, the probability of these variables will factorize as follows:
</p>
<p>p(a, b, c, e, r) = p(b)p(e)p(a|b, e)p(c|a)p(r|e). (13.10)
</p>
<p>This factorization of the probability has important consequences. Firstly, as for the näıve-Bayes
assumption, it makes the probability density much less costly to store on a computer and reliable
to estimate as there are fewer parameters than the full (un-factorized) joint distribution. Secondly,
it allows faster computation by exploiting the factorization structure and finally it allows us easier
to see what quantities are independent of each other. It is common to represent the factorization
as a network where the vertices correspond to the variables and the edges correspond to statistical
dependence, see fig. 13.3 for an illustration. So for instance, if there is an edge from B to A, that
means that in the joint distribution, then A must be conditional on B and possible other variables
connected to A. To solve the Burglar problem, assume the probability of there being a burglar is
p(b = 1) = 0.1%, earthquake p(e = 1) = 0.1% (corresponding to roughly one burglar and earthquake
every four years) and that the alarm is triggered either by (1) false alarms (very low probability),
(2) if an earthquake takes place (low probability) and finally if a burglar enters the home (high
probability). In our example these probabilities are:2
</p>
<p>2 For instance, suppose we let f = 0.1% denote the chance a false alarm triggers a, αe = 1% the chance an
earthquake triggers a and finally αb = 99% the chance a burglar triggers a. The probabilities can then
be obtained as:
</p>
<p>p(a = 1|b = 0, e = 0) = f,
p(a = 1|b = 0, e = 1) = 1−(1−f)(1−αe),
p(a = 1|b = 1, e = 0) = 1−(1−f)(1−αb),
p(a = 1|b = 1, e = 1) = 1−(1−f)(1−αb)(1−αe).</p>
<p></p>
</div>, <div class="page"><p></p>
<p>206 13 Bayesian methods
</p>
<p>p(a = 1|b = 0, e = 0) = 0.1%,
p(a = 1|b = 0, e = 1) = 1.099%,
p(a = 1|b = 1, e = 0) = 99.001%,
p(a = 1|b = 1, e = 1) = 99.011%.
</p>
<p>Finally assume the neighbour would never phone if the alarm is not ringing (p(c = 1|a = 0) = 0)
and that the radio reported is also trustworthy (p(r = 1|e = 0) = 0) and let’s return to the problem:
Suppose first the phone calls c = 1; then we know the alarm is ringing a = 1 and so the posterior
probability of b, e (burglary and earthquake) becomes:
</p>
<p>p(b, e|a = 1) = p(a = 1|b, e)p(b, e)
p(a = 1)
</p>
<p>.
</p>
<p>We can use the Bayes network to compute these probabilities. For instance when computing p(a =
1), we must compute this by summing over all other variables than a:
</p>
<p>p(a = 1) =
∑
</p>
<p>b∈{0,1}
</p>
<p>∑
c∈{0,1}
</p>
<p>∑
e∈{0,1}
</p>
<p>∑
r∈{0,1}
</p>
<p>p(a = 1, b, c, e, r), (13.11)
</p>
<p>However, if we plug in the expression of the likelihood (13.10) we see that variables c and r can
trivially be summed out (i.e., marginalized):
</p>
<p>p(a = 1) =
∑
</p>
<p>b∈{0,1}
</p>
<p>∑
c∈{0,1}
</p>
<p>∑
e∈{0,1}
</p>
<p>∑
r∈{0,1}
</p>
<p>p(b)p(e)p(a = 1|b, e)p(c|a = 1)p(r|e)
</p>
<p>=
∑
</p>
<p>b∈{0,1}
</p>
<p>∑
e∈{0,1}
</p>
<p>p(b)p(e)p(a = 1|b, e)
 ∑
c∈{0,1}
</p>
<p>p(c|a = 1)
∑
</p>
<p>r∈{0,1}
</p>
<p>p(r|e)
</p>
<p>
=
</p>
<p>∑
b∈{0,1}
</p>
<p>∑
e∈{0,1}
</p>
<p>p(b)p(e)p(a = 1|b, e) (13.12)
</p>
<p>Comparing to the Bayesian network in fig. 13.3 we see that to determine what variables remain in
the sum when computing p(a), we look at all other vertices in a network where we can move to a
by going in the direction of the edges. See fig. 13.4 where we have illustrated the two nodes that
remain, e, b, with red. Using the above numbers we obtain:
</p>
<p>p(a = 1|b = 0, e = 0)p(b = 0)p(e = 0) = 0.000998,
p(a = 1|b = 1, e = 0)p(b = 1)p(e = 0) = 0.0000989,
p(a = 1|b = 0, e = 1)p(b = 0)p(e = 1) = 0.000010979,
p(a = 1|b = 1, e = 1)p(b = 1)p(e = 1) = 9.9× 10−7.
</p>
<p>By inserting these four numbers into eq. (13.12) and summing we obtain p(a = 1) = 0.002 and so
from eq. (13.11)
</p>
<p>p(b = 0, e = 0|a = 1) = 0.4993, (13.13)
p(b = 1, e = 0|a = 1) = 0.4947, (13.14)
p(b = 0, e = 1|a = 1) = 0.0055, (13.15)
p(b = 1, e = 1|a = 1) = 0.0005. (13.16)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>13.3 Bayesian networks 207
</p>
<p>Earthquake Burglar
</p>
<p>Radio Alarm
</p>
<p>Phonecall
</p>
<p>Fig. 13.4: To determine what variables must be summed out when computing the marginal of a
variable such as a, we look at all variables such that one can move in the direction of the arrows
from those variables to a. This gives p(a, e, b)
</p>
<p>So returning to the initial question, when we determine if there was a burglar at the house we must
compute p(b = 1|a = 1) which can be accomplished by marginalizing over the burglar-variable:
</p>
<p>p(b = 0|a = 1) = p(b = 0, e = 0|a = 1) + p(b = 0, e = 1|a = 1) = 0.505
p(b = 1|a = 1) = p(b = 1, e = 0|a = 1) + p(b = 1, e = 1|a = 1) = 0.494
</p>
<p>so after receiving the call, we believe there to be a 50% chance there was a burglar in the house. An
important point to take away from this example is that b and e, which were initially independent:
p(e, b) = p(e)p(b), are made dependent by the information a. Now consider the final part of the
example. Suppose we also learn that e = 1 (i.e. there was an earthquake). The probability there
was a burglar can now be computed as:
</p>
<p>p(b|e, a) = p(b, e|a)
p(e|a) =
</p>
<p>p(b, e|a)
p(e, b = 0|a) + p(e, b = 1|a) .
</p>
<p>If we plug in numbers we obtain p(e = 1|a = 1) = 0.006 and so
</p>
<p>p(b = 0|e = 1, a = 1) = p(b = 0, e = 1|a = 1)
p(e = 1|a = 1) = 0.92
</p>
<p>p(b = 1|e = 1, a = 1) = p(b = 1, e = 1|a = 1)
p(e = 1|a = 1) = 0.08
</p>
<p>So after learning the alarm was triggered, this lowers our probability there was a burglar in the
house from about 50% to about 8%. This is in according to everyday intuition: when we learn about
the earthquake, we consider that to be the more plausible explanation of the alarm.
</p>
<p>13.3.1 A brief comment on causality
</p>
<p>Using the implied rules any factorization of a joint distribution is easily translated into a network:
Vertices implies variables and there is an edge from variable x to y if there is a term p(y|x, · · · ) in</p>
<p></p>
</div>, <div class="page"><p></p>
<p>208 13 Bayesian methods
</p>
<p>z
</p>
<p>x y
</p>
<p>p(x|z)p(y|x, z)p(z) p(y|z)p(x|y, z)p(z)
</p>
<p>z
</p>
<p>x y
</p>
<p>Fig. 13.5: Two bayesian networks which both represent the same distribution p(x, y, z). Since the
two networks are not similar this shows a Bayesian network cannot be interpreted as a causal graph.
</p>
<p>the factorization of the joint distribution. A point that is sometimes confused is to interpret the
Bayesian network as having a causal meaning. Consider a general distribution p(x, y, z). We are
always allowed to write this distribution as:
</p>
<p>p(x, y, z) = p(x|z)p(y|x, z)p(z), p(x, y, z) = p(y|z)p(x|y, z)p(z),
</p>
<p>since the two distributions are the same, but clearly give rise to different Bayesian networks as
shown in fig. 13.5, this shows we cannot interpret a Bayesian network as a causal graph.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>13.3 Bayesian networks 209
</p>
<p>Problems
</p>
<p>13.1. Fall 2015 question 16: Nine of the fifteen obser-
vations in Table 13.2 have chronic kidney disease (i.e.,
O1–O9 given in red) whereas six of the observations do
not have chronic kidney disease (i.e., O10–O15) given in
black). We would like to predict whether a subject has
chronic kidney disease or not using the data in Table 13.2
and the attributes RBC, PC, DM , and CAD. We will
apply a Näıve Bayes classifier that assumes independence
between the four attributes. Given that a subject has
these four attributes (i.e., RBC = 1, PC = 1, DM = 1,
and CAD = 1) what is the probability that the person
has chronic kidney disease, i.e., what is
P (CKD = 1|RBC = 1, PC = 1, DM = 1, CAD = 1)
according to the Näıve Bayes classifier?
</p>
<p>RBC PC PCC HTN DM CAD PE
O1 0 0 0 0 1 0 0
O2 0 1 1 1 0 0 1
O3 0 0 0 0 0 0 0
O4 0 1 0 0 1 0 1
O5 0 1 1 1 1 0 0
O6 1 1 1 1 1 0 0
O7 1 1 1 1 1 0 1
O8 0 1 1 1 1 1 1
O9 0 1 0 1 0 0 0
O10 1 1 0 0 0 1 0
O11 0 0 0 0 1 0 0
O12 0 0 0 0 0 0 0
O13 0 0 0 0 0 0 0
O14 0 0 0 0 0 0 0
O15 0 0 0 0 0 0 0
</p>
<p>Table 13.2: For each observation there are M = 7 binary
features and N = 15 observations O1, . . . , O15 belong-
ing to two categories (i.e., CKD=1 for O1, . . . , O9 and
CKD=0 for O10, . . . , O15).
</p>
<p>A 2.56 %
B 96.14 %
C 98.03 %
D 100 %
E Don’t know.
</p>
<p>13.2. Fall 2015 question 17: We will consider a Bayes
classifier using the attributes RBC, PC, and DM in Ta-
ble 13.2 (i.e., we no longer consider the attribute CAD).
What is P (CKD = 1|RBC = 1, PC = 1, DM = 1)
according to a Bayes classifier (i.e. we are no longer im-
posing independence as in the Näıve Bayes classifier)?
</p>
<p>A 26.67 %
B 97.07 %
C 98.03 %
D 100 %
E Don’t know.
</p>
<p>13.3. Fall 2013 question 19: Five of the ten con-
sidered subjects in Table 13.3 survived after five years
(S1−S5) given in black whereas five subjects died within
</p>
<p>five years (NS1 − NS5) given in red. We would like
to predict whether a subject survived using the data
in Table 13.3 and the attributes Y AY , OAY , PAY . We
will apply a Näıve Bayes classifier that assumes indepen-
dence between the three attributes. Given that a subject
had these three attributes (i.e., Y AY = 1, OAY = 1,
PAY = 1) what is the probability that the subject sur-
vived according to the Näıve Bayes classifier. I.e., what
is P (S|Y AY = 1, OAY = 1, PAY = 1) according to the
Näıve Bayes classifier?
</p>
<p>Y AY Y AN OAY OAN PAY PAN
S1 1 0 1 0 1 0
S2 1 0 1 0 0 1
S3 0 1 0 1 1 0
S4 0 1 1 0 1 0
S5 0 1 1 0 1 0
</p>
<p>NS1 0 1 1 0 1 0
NS2 0 1 0 1 1 0
NS3 1 0 0 1 0 1
NS4 0 1 1 0 1 0
NS5 0 1 1 0 1 0
</p>
<p>Table 13.3: Given are five subjects that survived in
Haberman’s study (denoted S1, S2, . . ., S5) as well as the
five subjects that did not survive in Haberman’s study
(denoted NS1, NS2, . . ., NS5) including whether these
subjects are young or old (Y AY , Y AN ), were operated
after 1960 or not (OAY , OAN ), and had positive axillary
nodes or not (PAY , PAN ).
</p>
<p>A 16125
B 311
C 12
D 811
E Don’t know.
</p>
<p>13.4. Fall 2014 question 15: Consider the observa-
tions in table 13.4. Suppose we only consider the first
two features f1, f2 and train a Naive-Bayes classifier to
classify between class C1 (black) and C2 (blue) based
on these two features alone. Suppose an observation has
f1 = 0, f2 = 1, what is the probability this observation
belongs to class C1 according to the Naive-Bayes classi-
fier?</p>
<p></p>
</div>, <div class="page"><p></p>
<p>210 13 Bayesian methods
</p>
<p>f1 f2 f3 f4 f5 f6
s1 0 1 1 0 1 0
s2 0 1 1 1 0 1
s3 1 1 1 0 1 0
s4 1 1 1 0 1 0
s5 0 1 1 0 1 1
s6 0 0 1 1 1 1
s7 1 1 0 1 1 1
s8 1 1 1 0 0 0
s9 1 0 1 1 0 0
s10 1 1 1 0 0 1
</p>
<p>Table 13.4: N = 10 observations s1, . . . , s10 belong-
ing to two categories. The black category C1 (observa-
tions s1, . . . , s5) and the blue category C2 (observations
s6, . . . , s10). For each observation there are M = 6 binary
features f1, . . . , f6.
</p>
<p>A pNB(C1|f1 = 0, f2 = 1) = 0.83
B pNB(C1|f1 = 0, f2 = 1) = 0.70
C pNB(C1|f1 = 0, f2 = 1) = 0.67
D pNB(C1|f1 = 0, f2 = 1) = 0.75
E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>14
</p>
<p>Regularization and the bias-variance decomposition
</p>
<p>As we already saw in chapter 10, “Overfitting and performance evaluation”, a too flexible model
can easily overfit the dataset leading to a high generalization error. In this chapter we will consider
a general technique for controlling model complexity known as regularization, which is useful in
many supervised learning settings but it is particulary apt for linear and logistic regression as well
as neural network modelling. We will then consider the problem (and need) to control the model
complexity in a more general setting and analyse the tradeof between having a very flexible model
that may overfit and a very stable model that might underfit in what is known as the bias-variance
decomposition of the generalization error. Regularization has been re-invented many times, but
was first considered by Andrey Nikolayevich Tikhonov in 1943 [Tikhonov, 1943], meanwhile a good
introduction to the tradeoff between bias and variance can be found in the discussion by James
et al. [2014].
</p>
<p>14.1 Least squares regularization
</p>
<p>In this section, we will look at a general approach for managing model complexity known as regu-
larization. Just as in the polynomial example, regularization allows us to make different models (by
adding different degrees of regularization), and the most appropriate choice of regularization is then
made using cross-validation for model selection. We illustrate the technique using least-squares re-
gression. Consider the simple linear regression model we previously encountered in with prediction
rule:
</p>
<p>yi = f(xi,w) = x̃
T
i w,
</p>
<p>as the reader may recall from section 8.1.1, the linear regression model was trained by minimizing
the sum-of-squares error term:
</p>
<p>E(w) =
∥∥∥y − X̃w∥∥∥2 . (14.1)
</p>
<p>The optimal weights w∗ can be found by minimizing the error and are given by:
</p>
<p>w∗ = arg min
w
</p>
<p>E(w) = (X̃
T
X̃)\(X̃Ty). (14.2)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>212 14 Regularization and the bias-variance decomposition
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>Fig. 14.1: A regularized linear regression model is fitted to the dataset of 9 observations and 10
test observations. The solutions, corresponding to three different values of λ, are shown in the three
panes. Notice for larger values of λ, the solution is dragged towards the x-axis because the solution
for the weights w∗ becomes smaller according to eq. (14.3). The left-most pane has high variance
but low bias, the right-most pane has high bias but low variance.
</p>
<p>The way we arrived at this formulation was a simple application of the general likelihood frame-
work discussed in section 6.5, see in particular eq. (6.38).
</p>
<p>There are two potential issues in linear regression. Firstly, the matrix X̃
&gt;
X̃ might not be
</p>
<p>invertible, which will happen if N ≤ M or if X̃ contains many linearly dependent rows, and
secondly, if M is large relative to N the linear regression model can overfit.
</p>
<p>Regularization attempt to solve these problems, by simply altering the cost function to have a
stronger preference small weights. However, note the magnitude of the weights are affected by the
relative scaling of the columns of X, and we therefore transform X by subtracting the mean and
dividing by the standard deviation of the columns:
</p>
<p>X̂ij =
Xij − µj
</p>
<p>ŝj
, µj =
</p>
<p>1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>Xkj , ŝj =
</p>
<p>√√√√ 1
N − 1
</p>
<p>N∑
i=1
</p>
<p>(Xij − µj)2
</p>
<p>Next, we don’t want the constant term in the regression to be affected by regularization, and we
therefore consider a cost function of the form:
</p>
<p>Eλ(w, w0) =
∥∥∥y − w01− X̂w∥∥∥2 + λ‖w‖2, λ ≥ 0. (14.3)
</p>
<p>The last term, λ‖w‖2, is called the regularization term, and the constant λ is called the regularization
constant. The regularization constant influence the relative importance of the regularization term,
starting with λ = 0 which correspond to the ordinary least-squares cost function eq. (14.1) asides
the standardization. This form of regularization term is commonly referred to as L2 regularization.
</p>
<p>Solving regularized linear regressionF
</p>
<p>Note our new objective still only depends on terms which are linear or quadratic in w, and can
therefore still be solved. To do so, first note that for any w, the minimal value of the intercept term
w0 is</p>
<p></p>
</div>, <div class="page"><p></p>
<p>14.1 Least squares regularization 213
</p>
<p>dEλ(w, w0)
</p>
<p>dw0
=
</p>
<p>N∑
i=1
</p>
<p>−2(yi − w01− x̂i&gt;w) = −2NE[y]− 2Nw0 −N
(
</p>
<p>1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>x̂i
&gt;
</p>
<p>)
w
</p>
<p>Since we have subtracted the column-wise mean from X̂, the term involving w disappears. Setting
the derivative equal to zero and solving gives:
</p>
<p>w0 = E[y] =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>yi
</p>
<p>which, retrospectively, might seem fairly obvious. Therefore, suppose we define ŷi = yi − E[y], we
can then re-write the objective as:
</p>
<p>Eλ =
∥∥∥ŷ − X̂w∥∥∥2 + λ‖w‖2, λ ≥ 0.
</p>
<p>This objective can be solved by computing the derivative and setting it equal to zero. We get:
</p>
<p>dEλ
dw
</p>
<p>= −X̂&gt;
(
ŷ − X̂w
</p>
<p>)2
+ 2w
</p>
<p>⇒ w∗ = arg min
w
</p>
<p>E(w) = (X̂
&gt;
X̂ + λI)\(X̂&gt;ŷ) (14.4)
</p>
<p>This this is very nearly the linear regression solution, except for the diagonal term λI and that
matrices have been transformed. To make a prediction, we have to be careful to apply the right
feature transformations. Specifically, to make predictions for a test observation x compute:[x1−µ1
</p>
<p>σ1
</p>
<p>x2−µ2
σ2
</p>
<p>· · · xM−µMσM
]
w∗ + E[y]
</p>
<p>where µi, σi, and E[y] are all computed on the training data. Note that asides analytical convenience,
the L2 regularization can be motivated using Bayes’ theorem, see technical note 14.1.1.
</p>
<p>14.1.1 The effect of regularization
</p>
<p>If we simply look at the expression for Eλ(w) in eq. (14.3) then if λ = 0 we get ordinary linear
regression. If on the other hand λ is large, the error term prefers each coordinate of w, wi, to be
as small as possible. This is also evident from the expression for the solution eq. (14.4): If we for a
moment näıvely ignore the standardization and assume X and y are scalars we get:
</p>
<p>w∗ =
Xy
</p>
<p>X2 + λ
,
</p>
<p>so the larger λ is, the smaller w∗ becomes and in the limit λ→∞ then w∗ = 0. In fig. 14.1 is shown
a small dataset with 9 observations (blue dots) and 10 test data points (green dots) and the solution
for three different values of λ. The linear regression model is in this case a 6’th degree polynomial.
We see that for the larger λ, since w∗ is smaller the fitted polynomial is dragged (biased) towards
the x-axis. If on the other hand λ is very small, the polynomial wiggles quite a lot (high variance)
as can be expected for a 6-degree polynomial on such a small dataset. The full evolution of the size
of each coordinate of the weights w∗i for many values of λ is shown in fig. 14.2.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>214 14 Regularization and the bias-variance decomposition
</p>
<p>10
-6
</p>
<p>10
-4
</p>
<p>10
-2
</p>
<p>10
0
</p>
<p>-3
</p>
<p>-2
</p>
<p>-1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>Fig. 14.2: A regularized linear regression model is fitted to the dataset shown in fig. 14.1 and the
coordinates of the optimal weights are plotted. When λ is small, the weights are large indicating
high variance but low bias. When λ is larger, the weights become smaller indicating lower variance
but higher bias in the solutions.
</p>
<p>This also holds in general: When the regularization λ is small, the models have high variance
and low bias. When λ is large, the models have low variance (they are all dragged towards the
x-axis) but high bias. As a rule, varying λ to search for an optimal value of the generalization error
will therefore lead to better models. In fig. 14.3 the variable λ is tweaked from a very small value
of λ = 10−6 to a higher value λ = 100 and the training and test error (normalized by the number
of observations) of the small dataset in fig. 14.1 displayed. The three particular values plotted in
fig. 14.1 are plotted as circles. We see that the training error generally increases as λ increases (after
all, for small λ the model will overfit the training data set), however, the test error has an optimum
when λ ≈ 10−2. In practice when we search for the optimal value of λ, we test S different values
of λ, λ1, . . . , λS selected beforehand and then compare each of the corresponding linear regression
models using cross-validation for model selection.
</p>
<p>Other choices of regularizationF
</p>
<p>A reader may wonder why we chose the particular L2 square-loss regularization. An alternative is
the is the L1-norm regularization term: λ (
</p>
<p>∑
i |wi|) = λ‖w‖1. The advantage of the L1 regularization
</p>
<p>term is that it prefers sparse solutions where many of the coordinates of wi becomes equal to zero,
as opposed to L2 regularization where they in general only become approximately equal to 0. This
is useful when the data set is known to contain many irrelevant attributes we wish to disregard. A</p>
<p></p>
</div>, <div class="page"><p></p>
<p>14.1 Least squares regularization 215
</p>
<p>10
-6
</p>
<p>10
-4
</p>
<p>10
-2
</p>
<p>10
0
</p>
<p>10
2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
10
</p>
<p>-3
</p>
<p>Fig. 14.3: Effect of varying the regularization parameter λ on the training and test error. The
colored dots indicate three models shown in fig. 14.1.
</p>
<p>disadvantage of pure L1 regularization is small changes in regularization may mean very different
weights are selected.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>216 14 Regularization and the bias-variance decomposition
</p>
<p>Technical note 14.1.1: Why use L2 regularization?
</p>
<p>Regularization, as explained here, is simply adding a factor λwTw to our error which may
appear rather arbitrary. It is however possible to give regularization a natural Bayesian
interpretation using our general likelihood learning framework discussed in section 6.5. To
simplify the discussion, we will assume the bias is treated similar to the other parameters,
and assume that just as in our original discussion of linear regression in chapter 8, the
likelihood of the data is
</p>
<p>p(y|X,w) =
N∏
i=1
</p>
<p>N (yi|x&gt;i , σ2).
</p>
<p>Our discussion then proceeded by assuming the prior term p(w) could be ignored. However,
let’s make the assumption the prior term cannot be ignored. The simplest case is to assume
the prior is normally distributed with diagonal covariance matrix δI:
</p>
<p>p(w) = N (w|0, δ2I)
</p>
<p>The maximum-likelihood formulation (see eq. (6.37) in summary box 6.5.1) consist of max-
imizing :
</p>
<p>p(w) +
</p>
<p>N∑
i=1
</p>
<p>log p(yi|xi,w) = −
1
</p>
<p>2σ2
</p>
<p>N∑
i=1
</p>
<p>‖y −Xw‖2 − N
2
</p>
<p>log(2πσ2)− 1
2δ2
</p>
<p>wTw − M
2
</p>
<p>log(2πδ2)
</p>
<p>If we drop constant terms and re-scale the expression by a factor −2σ2
</p>
<p>N∑
i=1
</p>
<p>‖Xw − y‖2 + σ
2
</p>
<p>δ2
wTw
</p>
<p>Therefore, if we define λ = σ
2
</p>
<p>δ2 , we nearly recover eq. (14.3) asides the standardization and
difference in how the bias is treated (this discrepancy can be solved by assuming a flat prior
for w0). Since the key assumption that lead to the particular form of the regularization term
was the distribution of the prior, other choices would lead to other forms of regularization.
</p>
<p>14.2 Bias-variance decomposition
</p>
<p>In this section, we will analyse the generalization error from a more theoretical perspective using
what is known as the bias-variance decomposition. Recall bias is how far away from the true mean
we are on average and variance measures how spread out our observations are, see fig. 14.4 for an
intuitive illustration. It turns out that the generalization error can in general be decomposed into
a systematic error known as the bias term and a term depending on how much our trained models
vary known as the variance term.
</p>
<p>Showing this is not too difficult but requires some math. We will therefore first illustrate the
result with a linear regression example and leave the proof as optional reading. Suppose we are in
a standard, supervised situation with a square loss where we predict y from observations x. If D
denotes our training data, a given model learns a function f on the training data to accomplish this</p>
<p></p>
</div>, <div class="page"><p></p>
<p>14.2 Bias-variance decomposition 217
</p>
<p>Fig. 14.4: Illustration of bias and variance
</p>
<p>task. In fig. 14.5 this is illustrated for two different (random) training data sets and the modelM2
corresponding to second-degree polynomials. Notice the learned function f depends on the training
sets and to keep track of this dependency we will write it as fD.
</p>
<p>Suppose we want to know how the generalization error behaves on average. Recall from chap-
ter 10 the generalization error was defined as how well our model performed on a test set on average
when trained on the training set D (see eq. (10.4))
</p>
<p>EgenM = E(x,y) [L(y,fM(x))]
</p>
<p>=
</p>
<p>∫
L(y,fM(x))p(x,y)dxdy. (14.5)
</p>
<p>Since the generalization error Egen depends on the training data set D, we will indicate this by the
notation Egen(D), and we will in this section consider the expectation of the generalization error
over all training data set:
</p>
<p>ED [Egen] =
∫
Egen(D)p(D)dD.
</p>
<p>The above, i.e., the averaged generalization error, is what we in this section consider our true
objective estimate of how well our model performs: How well it generalizes based on averaging over
all training data sets.
</p>
<p>To get insight into the average generalization error let’s consider the average behavior of the by
now well-known linear regression model when trained on different training sets. In fig. 14.6 the three
linear regression models are each trained on 10 different training sets and the prediction curves, fD,
are plotted as the thin red lines. Of particular importance will be the average of all these curves
shown as the thick red line in fig. 14.6. Formally, this is written as
</p>
<p>f̄(x) = ED [fD(x)]
</p>
<p>The black line is the true average of the training sets, i.e. the training points (which are not shown
in fig. 14.6) are distributed around this curve. Formally, it is defined as the average value of y given
x, i.e.:
</p>
<p>ȳ(x) = Ey|x [y] .
</p>
<p>Considering fig. 14.6 we can make some general observations. Firstly, the thin red curves (each
of the 10 models trained on different training sets) are quite close together in the first two plots,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>218 14 Regularization and the bias-variance decomposition
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>Fig. 14.5: A linear regression model corresponding to a second-order polynomial trained on two
different training data sets. The learned model (red line) depends on the training sets. The training
set is distributed around the black line.
</p>
<p>0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 14.6: A linear regression model corresponding to a second-order polynomial trained on two
different training data sets. The learned models (the thinner red lines) depends on the training sets.
The training set is distributed around the black line. The average of all models is shown as the
thicker red line.
</p>
<p>perhaps even the closest together in the first plot: They are said to have low variance. Meanwhile,
in the third plot the curves are spread out quite a lot because the model is too flexible and we say
this model has a high variance. If we turn to the average behaviour of the curves (the thick red
line), in the second and third plot the average of all the models is quite similar to the thick black
line (the true average of the training data) and we say the curves have a low bias. Meanwhile, the
first model, which is too inflexible, has a high bias because the average of the model f̄(x) and the
average of the data ȳ(x) is quite different.
</p>
<p>In the following, we will show these two effects –bias and variance– is all we need to describe
the average generalization error for any model.
</p>
<p>Derivation of the bias-variance decomposition*
</p>
<p>The average generalization error for a training set D for a square loss is:
</p>
<p>ED [Egen] = ED,(x,y)
[
(y − fD(x))2
</p>
<p>]
,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>14.2 Bias-variance decomposition 219
</p>
<p>where the expectation can be written out as ED,(x,y) [ · ] =
∫
</p>
<p>[ · ] p(x, y,D)dxdydD. We first assume
x to be fixed and consider the average:
</p>
<p>ED,y|x
[
(y − fD(x))2
</p>
<p>]
= ED,y|x
</p>
<p>[
(y − ȳ(x) + ȳ(x)− fD(x))2
</p>
<p>]
= Ey|x
</p>
<p>[
(y − ȳ(x))2
</p>
<p>]
+ ED
</p>
<p>[
(ȳ(x)− fD(x))2
</p>
<p>]
+ 2ED,y|x [(y − ȳ(x)) (ȳ(x)− fD(x))] .
</p>
<p>The last term is zero since
</p>
<p>ED,y|x [(y − ȳ(x)) (ȳ(x)− fD(x))] = Ey|x [y − ȳ(x)]ED [ȳ(x)− fD(x)] ,
</p>
<p>and Ey|x [y − ȳ(x)] = 0. If we look at the second term we can do the same trick once again:
</p>
<p>ED
[
(ȳ(x)− fD(x))2
</p>
<p>]
= ED
</p>
<p>[(
ȳ(x)− f̄(x) + f̄(x)− fD(x)
</p>
<p>)2]
= ED
</p>
<p>[(
ȳ(x)− f̄(x)
</p>
<p>)2]
+ ED
</p>
<p>[(
f̄(x)− fD(x)
</p>
<p>)2]
+ 2ED
</p>
<p>[(
ȳ(x)− f̄(x)
</p>
<p>) (
f̄(x)− fD(x)
</p>
<p>)]
.
</p>
<p>Again, since ED
[
f̄(x)− fD(x)
</p>
<p>]
= 0, the third term is zero in the above. Putting all these things
</p>
<p>together, we obtain:
</p>
<p>ED,y|x
[
(y − fD(x))2
</p>
<p>]
(14.6)
</p>
<p>= Ey|x
[
(y − ȳ(x))2
</p>
<p>]
+
(
ȳ(x)− f̄(x)
</p>
<p>)2
+ ED
</p>
<p>[(
f̄(x)− fD(x)
</p>
<p>)2]
. (14.7)
</p>
<p>The last term is simply the variance of fD(x) computed with respect to x and the first term too is
just the variance of y conditional on x. Using this the above can be written as:
</p>
<p>ED,y|x
[
(y − fD(x))2
</p>
<p>]
= Vary|x [y] + VarD [fD(x)] +
</p>
<p>(
ȳ(x)− f̄(x)
</p>
<p>)2
. (14.8)
</p>
<p>Taking the expectation with respect to x and rearranging we finally obtain the result:
</p>
<p>ED [Egen] = Ex
[
Vary|x [y] +
</p>
<p>(
ȳ(x)− f̄(x)
</p>
<p>)2
+ VarD [fD(x)]
</p>
<p>]
. (14.9)
</p>
<p>Interpreting the bias-variance decomposition
</p>
<p>The last equation in the previous section is known as the bias-variance decomposition. The term
on the left-hand side of the equality sign is how well we expect the model to generalize to new data:
This is the objective measure for how well our model performs. The terms on the right-hand side of
the equality sign tells us that the error of any model is decomposed into the following three parts
</p>
<p>• The first term Vary|x [y] is just a constant. It does not depend at all upon our choice of model
but simply represents the intrinsic difficulty of the problem. We cannot make this term any
larger or smaller by selecting one model over another.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>220 14 Regularization and the bias-variance decomposition
</p>
<p>0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0 0.5 1
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>Fig. 14.7: Bias-variance decomposition for the three linear regression models. In the top row is
shown the bias term. Namely, how much the average values of the models trained on different
random data sets (illustrated with the thick red line) differ from the true mean values of the data
illustrated by the black line. In the bottom row is shown the variance term. Namely, how much
each model wiggles around the mean of all models.
</p>
<p>• The second term
(
ȳ(x)− f̄(x)
</p>
<p>)2
is the bias term. It tells us how much the average values of
</p>
<p>models trained on different training datasets differ compared to the true mean of the data ȳ(x).
• The third term VarD [fD(x)] is the variance term. It tells us how much the model wiggles when
</p>
<p>trained on different sets of training data. That is, when you train the models on N different
(random) sets of training data and the models (the prediction curves) are nearly the same this
term is small.
</p>
<p>To illustrate the variance and bias terms for the linear regression example, in fig. 14.7 we have shown
what contributes to the two terms. In the top-row are given the bias terms (the difference between
the models mean values and the data mean values) and in the bottom row are given the variance
terms (the spread of the models). The generalization error is the sum of these two contributions,
plus the third contribution we cannot do anything about. We can thus see the first model does
badly because it has a high bias (but low variance), the third model does also poorly because it has
a high variance (but low bias) and the second model does well because both of these terms are low.
</p>
<p>When we think about how well different models perform, each model has different values of the
bias and variance terms which explains their generalization error. Often it is possible to construct
models such that e.g. the bias or variance term is low, but at the expense of a larger value of the
other term. This is known as the bias-variance tradeoff. The purpose of regularization in the context
of this bias-variance tradeoff is to substantially reduce the variance without introducing too much
bias.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>14.2 Bias-variance decomposition 221
</p>
<p>Problems
</p>
<p>14.1. Spring 2013 question 26: Which one of the fol-
lowing statements pertaining to regression is incorrect?
</p>
<p>A In regularized least squares regression the aim is to
reduce the model’s variance without introducing too
much bias.
</p>
<p>B Linear regression where the inputs are transformed
can only model linear relations between the original
untransformed inputs and the outputs.
</p>
<p>C To investigate what attribute transformations may
be relevant to consider it is useful to plot each at-
tribute versus the residuals.
</p>
<p>D Forward selection can be used both for regression and
classification problems.
</p>
<p>E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>15
</p>
<p>Neural Networks
</p>
<p>Artificial neural networks (ANNs) were originally invented as mathematical models of the infor-
mation processing by neurons McCulloch and Pitts [1943]. Today it is clear there are important
differences between ANNs and biological neurons, however, the basic structure is very similar. In
this chapter, we will consider the most simple forms of ANNs, the feedforward network, which is
nevertheless an extremely powerful approach to both classification and regression.
</p>
<p>15.1 The feedforward neural network
</p>
<p>An average adult human brain consists of about 86 billion neurons. Each neuron (a neuron is simply
a special type of cell) is connected to up to 10 000 other neurons by synapses. Each neuron has
an electric activity (for simplicity this can be considered as a real number) which depends on how
many of the neurons connected to the neuron are active. That is, if sufficiently many of the neurons
connected to a given neuron becomes active, the neuron itself becomes active and may then in turn
excite other neurons connected to it. It is surprising how such a simple mechanism can give rise
to interesting information processing and how intelligence arise from neuronal activity remains the
greatest open problem in neuroscience.
</p>
<p>15.1.1 Artificial neural networks
</p>
<p>In ANNs we consider a set of information processing units also called neurons and each neuron
is connected to other neurons by weighted connections. The neurons are organized in layers with
connections from one layer feeding into the next. In this way information is processed sequentially
(layer-wise) in the network: First, the input pattern (which is just a vector x = (x1, . . . , xM )) is
presented to the input layer such that neuron i in the input layer is given an activation equal to
xi. The activation is then propagated to one or several hidden layers and finally to the output layer
consisting of one or more neurons corresponding to the coordinates of the output vector.
</p>
<p>This process is known as a forward pass through the network. In fig. 15.11 is illustrated a simple
neural network with one hidden layer. The input layer consists of three neurons, the hidden layer
of four neurons and the output of a single neuron.
</p>
<p>1 By Glosser.ca [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Com-
mons</p>
<p></p>
</div>, <div class="page"><p></p>
<p>224 15 Neural Networks
</p>
<p>Fig. 15.1: Simple artificial neural network (ANN) consisting of three input units in the input layer,
a single hidden layer with four hidden units and two output units in the output layer. This neu-
ral network would implement a mapping f : R3 → R2 and would be suitable for regression or
classification in the case of two output variables.
</p>
<p>If M is the number of neurons in the input layer and D is the number of neurons in the output
layer a neural network is then simply a mapping:
</p>
<p>f : RM → RD
</p>
<p>which maps from x to y: y = f(x); thus the neural network is useful for solving a (multi-
dimensional) regression or classification problem.
</p>
<p>15.1.2 The forward pass in details
</p>
<p>Recall the basic linear regression model in which the output y is predicted from the rule
</p>
<p>f(x,w) =
</p>
<p>M∑
i=1
</p>
<p>xiwi + w0
</p>
<p>If we let
x̃ =
</p>
<p>[
1 x1 x2 . . . xM
</p>
<p>]T
we can write this in a more condensed form
</p>
<p>f(x,w) = x̃Tw.
</p>
<p>The forward pass in a neural network now proceeds as follows for vector x:
</p>
<p>• Each neuron i in the input layer is initialized to have activity xi.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>15.1 The feedforward neural network 225
</p>
<p>x
</p>
<p>h
(x
)
</p>
<p>−2 0 2
−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>x
</p>
<p>h
(x
)
</p>
<p>−2 0 2
0
</p>
<p>0.5
</p>
<p>1
</p>
<p>x
</p>
<p>h
(x
)
</p>
<p>−2 0 2
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>Fig. 15.2: Different choices of activation function. (Left:) hyperbolic tangent: h(x) = tanh(x) =
ex−e−x
ex+e−x , (middle:) logistic sigmoid h(x) = (1 + e
</p>
<p>−x)−1 and (right:) rectified linear unit: h(x) = 0 if
x &lt; 0 and otherwise h(x) = x. The basic information-processing ability is similar for all activation
functions, but during training the choice may be important as the gradients will differ in magnitude.
For this reason it is also common to apply a fixed transformations such as h(x) = b tanh(ax).
</p>
<p>• Neuron j in the hidden layer is given activity a(1)j = x̃Tw
(1)
j . Notice this is just a real number.
</p>
<p>• Each of the H hidden unit are transformed using a nonlinear activation function h to give
z
(1)
j = h(a
</p>
<p>(1)
j ). We then define
</p>
<p>z(1) =
[
z
(1)
1 z
</p>
<p>(1)
2 . . . z
</p>
<p>(1)
H
</p>
<p>]T
• Output neuron k is given an activation of a(2)k =
</p>
<p>(
z̃(1)
</p>
<p>)T
w
</p>
<p>(2)
k
</p>
<p>• The output neurons are transformed using a function h(2) to give z(2)j = h(2)(a
(2)
k )
</p>
<p>• The value of the neural network (output) is simply
</p>
<p>f(x) =
[
z
(2)
1 z
</p>
<p>(2)
2 . . . z
</p>
<p>(2)
D
</p>
<p>]T
.
</p>
<p>These steps may look daunting and it is perhaps useful to consider what they concretely mean.
Suppose we collect the various weight-terms into matrices and define
</p>
<p>W (1) =
[
w
</p>
<p>(1)
1 w
</p>
<p>(1)
2 . . .w
</p>
<p>(1)
H
</p>
<p>]
and W (2) =
</p>
<p>[
w
</p>
<p>(2)
1 w
</p>
<p>(2)
2 . . .w
</p>
<p>(2)
D
</p>
<p>]
.
</p>
<p>The activation of the kth output neuron is simply:
</p>
<p>fk(x,w) = h
(2)
</p>
<p> H∑
j=1
</p>
<p>W
(2)
kj z
</p>
<p>(1)
j
</p>
<p> (15.1)
= h(2)
</p>
<p> H∑
j=1
</p>
<p>W
(2)
kj h
</p>
<p>(1)
(
x̃Tw
</p>
<p>(1)
j
</p>
<p>) . (15.2)
The activation function h(1) of the hidden units could be chosen as the hyperbolic tangent
</p>
<p>h(1)(x) = tanh(x) =
ex − e−x
ex + e−x</p>
<p></p>
</div>, <div class="page"><p></p>
<p>226 15 Neural Networks
</p>
<p>
</p>
<p>Input Layer Hidden Layer Output Layer
</p>
<p>Fig. 15.3: Simple neural network of 6 weights and with one hidden layer with 2 neurons.
</p>
<p>Many choices of activation function can be found in the literature all with roughly the same basic
information-processing abilities but with different characteristics under training. A few common
examples can be found in fig. 15.2.
</p>
<p>Example 15.1.1: Forward pass of a neural network
</p>
<p>Consider the feedforward neural network shown in fig. 15.3. The network has no bias weights.
Suppose the weights of the neural network after training are
</p>
<p>w31 = 0.05, w41 = 0, w32 = 0.1,
</p>
<p>w42 = −0.05, w53 = 0.1, w54 = −10
</p>
<p>and the activation functions of the neurons in the hidden layer and output layer, i.e., n3,
n4, and n5 all are given by the following leaky rectified linear unit
</p>
<p>h(x) =
</p>
<p>{
x if x &gt; 0
1
10x otherwise.
</p>
<p>Suppose the network is evaluated on input x1 = 0.5, x2 = 1, the output is then computed
by first evaluating the hidden layer:
</p>
<p>x3 = h(x10.05 + x20.1) =h(1/8) =
1
</p>
<p>8
,
</p>
<p>x4 = h(x10 + x2(−0.05))=h(−1/20)=
−1
200
</p>
<p>.
</p>
<p>and then the output layer:
</p>
<p>x5 = h(x30.1 + x4(−10)) = h(1/16) = 1/16.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>15.2 Training neural networks 227
</p>
<p>The general L-layer neural network
</p>
<p>The neural network discussed in the previous section is said to have two layers (the hidden layer and
the output layer; the input layer is not counted). The construction can be immediately generalized
to L layers by simply repeating the two steps in the hidden layer. Written in a more condensed
fashion we proceed as follow:
</p>
<p>• We define z(0) = x as the input activation
• For each layer l = 1, . . . , L set z(l) = h(l)
</p>
<p>(
(W (l))T z̃(l−1)
</p>
<p>)
.
</p>
<p>• Return as output f(x,w) = z(L).
In general each hidden unit also contains a bias term corresponding to an additional input to each
neuron of 1 with the corresponding weight term accounting for the bias (i.e., just as we appended a
column of 1 to our input data x in regression to form x̃ we add a bias term as input to the neuron
of the l’th layer using as input to the neuron z̃(l−1) = [1 z(l−1)]).
</p>
<p>15.2 Training neural networks
</p>
<p>Regardless if one choose a two-layer neural network with a single hidden layer, or a general L layer
neural network, one simply obtains a parametric function f(x,w). For a fixed w this function
maps x values to y values such that the vector w contains all the weight-matrices in the network
w = (W (1),W (2), · · · ).
</p>
<p>We are thus faced with a standard supervised learning problem where we are given instances
of observations x1,x2, . . . ,xN and corresponding targets y1,y2, . . . ,yN , and our solution will be
very similar to what we already considered for linear and logistic regression: Since the neural
network cannot be expected to perfectly map from xi to the corresponding yi, we will assume yi
is normally distributed around the prediction of the neural network. If y has dimension K, the
probability density of observation yi is then:
</p>
<p>p(yi|xi,w) = N (yi|f(xi,w), Iσ2) =
1
</p>
<p>(2πσ2)
K
2
</p>
<p>e−
‖yi−f(xi,w)‖
</p>
<p>2
</p>
<p>2σ2 . (15.3)
</p>
<p>Applying the maximum likelihood framework from section 6.5, and for generality including a reg-
ularization term as in chapter 14, we see once more that the value of w that maximize p(w|X,y)
can be found by minimizing the cost function E defined as
</p>
<p>w∗ = arg min
w
</p>
<p>Eλ(w) (15.4)
</p>
<p>Eλ(w) =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>‖f(xi,w)− yi‖2 + λwTw
</p>
<p>where λ is the regularization strength we have to specify. Training a neural network is therefore
reduced to searching for a minimum of the function E. In arriving at this formulation we considered
the simple feed-forward neural network for regression, however, we stress that in nearly all appli-
cations of neural networks, whether they are used to translate from French to English, recognize
images or play Atari videogames, depend on specifying an appropriate function E and searching
for the minimizing w∗. Thus, headway on solving the problem eq. (15.4) can be used in a variety
of contexts.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>228 15 Neural Networks
</p>
<p>w
′
→
</p>
<p>← w
′′
</p>
<p>← w∗
</p>
<p>w
</p>
<p>E
(w
</p>
<p>)
</p>
<p>← w∗
</p>
<p>← w
(3)
</p>
<p>← w
(0)
</p>
<p>w
</p>
<p>E
(w
</p>
<p>)
</p>
<p>Fig. 15.4: (left:) Value of error function in a one-dimensional example. Weights at w′ should move
right and weights at w′′ should move left in order to approach the minimum point w∗ of E(w).
(right:) Gradient descent algorithm applied for three steps starting at w(0)
</p>
<p>15.2.1 Gradient DescentF
</p>
<p>The problem is that it is impossible to analytically solve for w∗. Instead, the following iterative
algorithm is proposed:
</p>
<p>• Start from an initial guess at w∗, w(0).
• At step t, modify w(t−1) by a small amount dw to produce a better guess w(t):
</p>
<p>w(t) = w(t−1) + dw,
</p>
<p>where we leave it for later how to compute dw.
• Do this for a large number T of iterations to produce (hopefully!) better and better guesses,
</p>
<p>i.e., w(0),w(1), . . . ,w(T ).
</p>
<p>After T iterations w(T ) is then used as the “best” available guess of w∗. This algorithm is very
simple if not for the unspecified step 2. To solve this we will use gradient descent which only requires
E to be differentiable.
</p>
<p>The one-dimensional case
</p>
<p>To introduce gradient descent, suppose w is one dimensional (the neural network only contains a
single “weight”) and suppose E as a function of w looks like fig. 15.4. If we suppose at step t of the
algorithm w(t−1) is located at position w′ in the figure, a “better” guess at w∗ can be obtained by
moving w(t−1) slightly to the right by a positive amount dw′:
</p>
<p>w(t) = w′ + dw′, dw′ &gt; 0,
</p>
<p>on the other hand if w(t−1) equals w′′ then a “better” guess at w∗ can be obtained by moving w(t−1)
</p>
<p>slightly to the left by a negative amount dw′′
</p>
<p>w(t) = w′′ + dw′′, dw′′ &lt; 0.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>15.2 Training neural networks 229
</p>
<p>This obviously leave the question of how we compute dw′ or dw′′. Notice, if we compute the gradient
of E at w′ or w′′ we have:
</p>
<p>dE
</p>
<p>dw
(w′) &lt; 0 and
</p>
<p>dE
</p>
<p>dw
(w′′) &gt; 0.
</p>
<p>Thus, if we let dw = −�dEdw (w(t−1)) be the gradient of E evaluated at w(t−1) multiplied by � &gt; 0
which is called the learning rate of the method (usually set somewhere in the interval [0, 1] for
instance � = 1/5), we can consider the simple update rule:
</p>
<p>θ(t) = θ(t−1) + dw.
</p>
<p>It is easy to check this indeed works – in fig. 15.4 is plotted w(t) and E(w(t) as a function of t when
this rule is applied for 3 steps. Notice that the method “slows down” when w(t) is closer to w∗ as
the magnitude of the gradient dEdw (w
</p>
<p>(t−1)) becomes smaller; this is useful to prevent overshooting
the target, however, it also potentially slows down the algorithm.
</p>
<p>So why does this work? We can formalize the above argument as follows. Suppose for simplicity
we define w′ = w(t). Then we can Taylor expand2 E around w′ to obtain:
</p>
<p>E(w′ + dw) ≈ E(w′) + dwdE
dw
</p>
<p>(w′) (15.5)
</p>
<p>≈ E(w′) + dwg (15.6)
</p>
<p>where g =
dE
</p>
<p>dw
(w′). (15.7)
</p>
<p>Thus, if we select dw = −�g in the above we get:
</p>
<p>E(w′ + dw) = E(w′) + dwg = E(w′)− �g2.
</p>
<p>In other words we are guaranteed that if we let w(t) be equal to w′ + dw = w(t−1) − �g then
</p>
<p>E(w(t)) ≤ E(w(t−1)).
</p>
<p>This decreases the error with roughly an amount �g2 (this also explains why the error changes less
and less in fig. 15.4). In this view, it is surprising why we don’t select � to be very large – perhaps
� = 1000. The reason is that the Taylor expansion is only accurate for small values of dw, thus we
can’t trust the above result when � is very large.
</p>
<p>Multiple dimensions
</p>
<p>We have spent some time on the one-dimensional case, however, the multi-dimensional case can be
treated very similar. In this case we can consider a small, perturbation dw of w′. The multivariate
Taylor expansion now gives:
</p>
<p>E(w′ + dw) ≈ E(w′) + dwTg (15.8)
≈ E(w′) + dwTg (15.9)
</p>
<p>where g = ∇E(w′) (15.10)
</p>
<p>2 See also https://en.wikipedia.org/wiki/Taylor_series and appendix A.</p>
<p></p>
<div class="annotation"><a href="https://en.wikipedia.org/wiki/Taylor_series">https://en.wikipedia.org/wiki/Taylor_series</a></div>
</div>, <div class="page"><p></p>
<p>230 15 Neural Networks
</p>
<p>← w
(3) ← w
</p>
<p>(0)
</p>
<p>← w∗
</p>
<p>w1
</p>
<p>w
2
</p>
<p>w
′
→
</p>
<p>← w
′′
</p>
<p>← w∗
</p>
<p>w
</p>
<p>E
(w
</p>
<p>)
</p>
<p>Fig. 15.5: (left:) Value of error function in a two dimensional example as a contour plot along
with three steps of the gradient descent algorithm. Notice the step size slows down when moving
towards the minimum. (right:) An example with two local minima. If the gradient descent method
is initialized at w′ it will converge to the global minima w∗, whereas if it is initialized at w′′ it will
converge to a local minima at the bottom of the right-most valley.
</p>
<p>The multi-dimensional Taylor expansion is briefly reviewed in appendix A. Thus, if we select dw =
−�g we again get
</p>
<p>E(w(t)) = E(w(t−1) + dw) ≈ E(w(t−1))− �‖g‖2 ≤ E(w(t−1)),
</p>
<p>which again is seen to decrease the error assuming the Taylor expansion is fairly accurate. This
allows us to define the Gradient-descent algorithm as:
</p>
<p>• Start from an initial guess at w∗, w(0)
• For each t = 1, . . . , T , compute the divergence g(t−1) = ∇E(w(t−1))
• Compute w(t) = w(t−1) − �g.
• Do this for a large number T of iterations to produce a sequence of (hopefully!) better and
</p>
<p>better guesses of w∗: w(0),w(1), . . . ,w(T ).
</p>
<p>In fig. 15.5 we have illustrated how w is updated for three iterations in an example where w is
two-dimensional.
</p>
<p>Training neural networks in practice
</p>
<p>Gradient descent is the prototypical training algorithm for neural networks. Most advanced appli-
cations of neural networks use either plain gradient descent, or gradient descent with very simple
modifications. A serious omission of the preceding discussion is how to compute the gradient g
efficiently. If we consider the i’th coordinate of g, gi, this can be computed as:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>15.3 Neural networks for classification 231
</p>
<p>gi =
∂E(w)
</p>
<p>∂wi
(15.11)
</p>
<p>=
∂
</p>
<p>∂wi
</p>
<p>(
1
</p>
<p>2
</p>
<p>N∑
i=1
</p>
<p>‖f(xi,w)− yi‖2 +
1
</p>
<p>2
λwTw
</p>
<p>)
(15.12)
</p>
<p>=
</p>
<p>N∑
i=1
</p>
<p>[
D∑
k=1
</p>
<p>(fk(xi,w)− yik)
∂fk(xi,w)
</p>
<p>∂wi
</p>
<p>]
+ λwi. (15.13)
</p>
<p>It should be stressed these computations are in principle just simply algebra: computing the
derivative of a function with respect to a single variable wi. In practice there is a simple trick for how
to organize the derivatives layer-wise to re-use computations which can lead to dramatic speedup
compared to an naive computation, the resulting algorithm, which compute the same derivative but
in an intelligent manner, is known as back-propagation. A further issue which should be mentioned
is when E has different local minima. In fig. 15.5 is shown a function E with two local minima. If
w(0) is initially selected to be at either w′ or w′′ it will find different solutions as indicated by the
arrows and no amount of training will cause a move from the suboptimal solution (the first valley)
to the optimal solution (the second valley). This is a difficulty of considerable practical interest as
in higher dimensions there will typically be many local minima and so the solution w(T ), as well
as the training error E(w(T )), will depend on how the model is initialized w(0) as well as other
parameters of the training.
</p>
<p>15.3 Neural networks for classification
</p>
<p>Making a neural network suitable for regression useful for classification is very similar to how we
changed linear regression into logistic regression by the use of the Bernoulli distribution. As neural
networks are nearly always applied to situations with multiple classes, the multi-class setting is
the more relevant, however for completeness, and a warm up exercise, we have included the binary
classification setting as a special case.
</p>
<p>15.3.1 Neural networks for binary classification
</p>
<p>In the case of a binary classification problem, where y = 0 and y = 1, the procedure is entirely similar
to how we derived the logistic regression model using the re-parameterization trick in section 5.4.3.
Specifically, we assume:
</p>
<p>p(y|x,w) = Bernouilli(y|ŷ) = ŷy(1− ŷ)1−t, ŷ = σ(f(x,w)) (15.14)
</p>
<p>Using this probability density in place of eq. (15.3), the cost function to be minimized becomes:
</p>
<p>Eλ(w) = −
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>[yi log ŷi + (1− yi) log(1− ŷi)] + λw&gt;w, ŷi = f(xi,w). (15.15)
</p>
<p>As usual, the minimization is done using gradient descend. Note the particular case where the
neural network network is linear (i.e., the activation function is just the identity function), we have
f(x,w) = x̃&gt;w and the neural network is simply implementing standard logistic regression.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>232 15 Neural Networks
</p>
<p>15.3.2 Neural networks for multi-class classification
</p>
<p>Suppose y corresponds to a classification problem with C classes 1, 2, . . . , C. As an example, suppose
C = 3 corresponding to “dog”, “cat”, and “cow”.
</p>
<p>We will assume y is one-of-K encoded in the usual manner:
</p>
<p>y =
</p>
<p>
0 1 0
1 0 0
1 0 0
0 0 1
</p>
<p>...
</p>
<p> ,
</p>
<p>implying that the first observation is in class 2 (a cat), the second and third observations are in class
1 (a dog) and the fourth observation is in class 3, a cow. As indicated, this is one-of-K encoded
in the matrix y such that yik = 1 if observation i is in class k and otherwise yik = 0. Notice∑C
k=1 yik = 1 because each observation is in exactly one class.
What we need in order to apply the familiar maximum-likelihood machinery of section 6.5 is a
</p>
<p>way to express the probability
</p>
<p>p(yi =
[
yi1 yi2 · · · yiC
</p>
<p>]
|xi,w)
</p>
<p>To do this, we will use the parameter transformation trick previously discussed in section 5.4.3
applied to the categorical distribution eq. (5.28). Specifically, we will assume f(x,w) outputs a
C-dimensional vector, and apply the softmax function to transform this into a probability vector:[
</p>
<p>ŷi1 ŷi2 · · · ŷiC
]
</p>
<p>= softmax(f(xi,w)) (15.16)
</p>
<p>where fk(x,w) is the value of the k’th output neural of the neural network. For an example of how
the softmax function works, see example 15.3.1. The probability of a given observation can then be
expressed using the categorical distribution eq. (5.28)
</p>
<p>p(yi|xi,w) = Catagorical(yi|ŷi) =
C∏
k=1
</p>
<p>ŷyikik (15.17)
</p>
<p>Applying the maximum-likelihood framework to this cost function, and including a regularization
term, we obtain the multi-class equivalent of eq. (15.15)
</p>
<p>Eλ(w) = −
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>[
C∑
k=1
</p>
<p>yik log ŷik
</p>
<p>]
+ λw&gt;w. (15.18)
</p>
<p>where: ŷik =
efk(xi,w)∑C
c=1 e
</p>
<p>fc(xi,w)
</p>
<p>As usual, the optimal weights should be found using gradient descend.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>15.3 Neural networks for classification 233
</p>
<p>Example 15.3.1: Softmax function
</p>
<p>Let’s consider a concrete example. Suppose the output of a neural network is f1 = 2, f2 = 1
and f3 = −1. We then have that
</p>
<p>ef1 ≈ 7.39, ef2 ≈ 2.72, ef3 ≈ 0.37
</p>
<p>and so
ŷ = softmax(f) ≈
</p>
<p>[
7.39
10.5
</p>
<p>2.72
10.5
</p>
<p>0.37
10.5
</p>
<p>]
=
[
0.7 0.26 0.04
</p>
<p>]
.
</p>
<p>Therefore, the neural network indicate this observation should be classified as belonging to
class 1 with probability 0.7.
</p>
<p>15.3.3 Multinomial regression
</p>
<p>Since logistic regression corresponds to a linear neural network with no activation function, it should
be apparent the multi-class neural network allows us to extend logistic regression to the multi-class
setting.
</p>
<p>One way to accomplish this is to simply replace f(x,w) in eq. (15.18) with a linear function;
while this is certainly a valid way to proceed, there is one slightly annoying side-effect. Recall from
section 15.3.1 that in the case where we applied neural networks to a two-class classification task,
the neural network had a single output neuron. However, in the multi-class setting considered in
section 15.3.2, the neural network had as many outputs C as there was classes. This means that this
approach to multi-class regression would not directly generalize the binary classification case. To
get around this, it is customary to implement linear multi-class classification using the (modified)
softmax with C−1 inputs which we encountered in eq. (5.31). Specifically, assume X̃ is our dataset
transformed in the usual manner by pre-fixing it with 1, and X̃ has dimensions N ×M . We then
have:
</p>
<p>f(xi,W ) = Wx̃i
</p>
<p>where W is a general C − 1×M -dimensional matrix of the form
</p>
<p>W =
</p>
<p>
w&gt;1
w&gt;1
</p>
<p>...
w&gt;C−1
</p>
<p>
and each wk is a M × 1 vector of weights. We then define the objective using the modified softmax
eq. (5.31)
</p>
<p>E(W ) = −
N∑
i=1
</p>
<p>[
C∑
k=1
</p>
<p>yik log ỹik
</p>
<p>]
+ λw&gt;w, where: ỹik =
</p>
<p>
ew
&gt;
k xi
</p>
<p>1+
∑C−1
c=1 e
</p>
<p>w&gt;c xi
if k ≤ C − 1
</p>
<p>1
</p>
<p>1+
∑C−1
c=1 e
</p>
<p>w&gt;c xi
if k = C.
</p>
<p>This raises the obvious question why we didn’t use this parameterization of the softmax (which,
after all, contains fewer parameters) for the multi-class neural network. One answer is that the
alternative parameterization creates an asymmetry between the classes, in that class C = 1 is</p>
<p></p>
</div>, <div class="page"><p></p>
<p>234 15 Neural Networks
</p>
<p>special. This does not matter as much the simple multinomial regression model which provides
more robust parameter estimates, and where it is often considered important to be able to interpret
the parameters. However, for neural networks, nobody expects to interpret the parameters anyway,
and the asymmetry is considered to be undesirable.
</p>
<p>15.3.4 Flexibility and cross-validation
</p>
<p>The strength of neural networks derives from their great flexibility. If we consider the sigmoid
activation function, the first layer of the neural network can be considered as performing as many
logistic regressions as there are internal neurons; to draw a parallel to the decision tree, each neuron
in the first hidden layer corresponds to asking one “question” about the input observation but with
the added flexibility that the output can be graduated (rather than binary) and will involve a
combination of features rather than asking if one feature is greater than another. However it is
what happens at the subsequent layers that really sets neural networks aside from decision trees:
A decision tree would use the output of a single question to ask further questions, however a
neural network combines the output of many other questions. It is this ability that allows neural
networks, especially deep neural networks (i.e. neural networks with several/many hidden layers),
to be extremely flexible.
</p>
<p>The downside of this flexibility is that neural networks are prone to overfitting the data and it
is therefore important to use cross-validation in conjunction with neural network training. Neural
networks provide many knobs to limit overfitting, most importantly the regularization parameters
λ which should be tuned in most settings. In addition to λ, it is worth experimenting with other
parameters in the neural network, for instance the number of hidden layers, the number of units
in each hidden layer and the choice of activation function. Starting with the simplest settings (for
instance a single hidden layer), it is important to tune the parameters using cross-validation and
use two-layer cross-validation to estimate the generalization error in a fair manner as discussed in
chapter 10.
</p>
<p>15.4 Advanced topicsF
</p>
<p>In this section we will briefly sketch upon some advanced topics of neural network training
</p>
<p>15.4.1 Mini-batching
</p>
<p>Gradient descent requires computing the divergence of the error ∇E(w) which in turn requires
iterating over all observations in the data set. If the data set contains millions of images (or billions of
words) this would be completely infeasible. Mini-batching is a simple yet very widely used approach
to overcome this problem. In mini-batching with a batch size of B the observations in the data set
is divided into m = NB smaller data sets D1, . . . ,Dm each containing B observations. Instead of
using the gradient:
</p>
<p>g = ∇E(w) = ∇
(
</p>
<p>N∑
i=1
</p>
<p>‖f(xi,w)− yi‖2
)
</p>
<p>we use the approximate gradients computed for the observations in each batch k:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>15.4 Advanced topicsF 235
</p>
<p>gk = ∇Ẽ(w) =
N
</p>
<p>B
∇
(∑
i∈Dk
</p>
<p>‖f(xi,w)− yi‖2
)
.
</p>
<p>The gradient-descent method is thus simply
</p>
<p>• Start at w(0)
• For each iteration t:
• For each batch k = 1, . . . ,m:
• Update w(t) = w(t−1) − �g(t)k
</p>
<p>In realistic applications, we might have that N range from thousands to billions whereas B is
usually selected at around 100 to 1000. So why does this work? From a theoretical point of view,
we want the learning rate � to be as high as possible. However the neural network function is highly
non-linear meaning that when the weights are changed even just slightly by −�g the local Taylor
expansion becomes inaccurate and we have to re-compute the gradients. This implies we have to
select � fairly small for the gradient descent method to work.
In mini-batching, we replace the true gradient g at a point w(t) with an approximate gradient
gk. Even though this (as a rule) introduces more uncertainty in the algorithm, this uncertainty is
relatively small comparable to the uncertainty already present due to the Taylor expansion not being
very exact. And since computing the approximate gradient is extremely inexpensive (scales with
B and not N) taking many smaller steps in mini-batching becomes better than taking one step in
ordinary gradient descent which is only slightly more exact than the smaller steps in mini-batching.
</p>
<p>15.4.2 Convolutional neural networks
</p>
<p>Suppose we wish to apply a neural network to classify semi large (for instance 999× 999) images.
If we use 1000 neurons in the first hidden layer, the first layer of the neural network alone would
contain 999 × 999 × 1000 ≈ 109 weights (here we haven’t included the bias term for each neuron,
which would add 1000 additional parameters). Not only is this a considerable computational burden,
it is doubtful we have enough images to tune this many parameters in a meaningful manner. A way
to significantly cut down on the computational cost is using convolutions which can be sketched
as follows: Suppose we consider a very small neural network with no hidden layer which takes an
11 × 11 input image and maps it onto a single neuron. We can then “translate” this small neural
network over the entire image by moving it in strides of F = 4. That is, if we let A be the matrix
representing the image, we first apply the neural network to pixels A[1:11]×[1:11], then A[5:16]×[1:11],
then A[9:20]×[1:11] and so on in both the horizontal and vertical direction until we apply the neural
network to A[989:999]×[989:999].
</p>
<p>If we keep track of the output of the small neural network over all these patches, this leads to
a new “hidden layer” of dimensions 247× 247 where 247 = 999−11F , however only about 121 = 112
weights were used to produce this output. Including D such convolutional filters we obtain a hidden
layer of dimensions 247 × 247 ×D using only about 121 ×D = 112 ×D weights. The process can
(and should) be made more elaborate by using several such convolutional layers to allow greater
flexibility and the process is typically repeated on the second hidden layer to produce an even
smaller set of neurons, however these details need not concern us at this stage: The important point
is that the same set of weights is “re-used” over the entire image which both cuts down on the
number of weights and allow each weight to be trained using much more data. At some point the
number of neurons becomes manageable and the neural network can proceed using one or more</p>
<p></p>
</div>, <div class="page"><p></p>
<p>236 15 Neural Networks
</p>
<p>fully connected layers. This kind of architecture is known as a convolutional neural network and
forms the basis of the best image-recognition systems.
</p>
<p>15.4.3 Autoencoders
</p>
<p>Neural networks can be used as a powerful dimensionality reduction method known as an autoen-
coder . Take the completely standard feed-forward neural network considered in this chapter and
suppose we have access to MNIST handwritten digit dataset. However instead of predicting the
identity of the digits yi from xi, we simply predict xi from xi. That is we model
</p>
<p>xi = f(xi,w) + �,
</p>
<p>where � is noise. Notice, this is entirely trivial when one has a working neural network implementa-
tion – simply replace yi with xi. The benefit of this approach is if one of the hidden layers contains
less dimensions than there are pixels in the image, for instance H = 100, then the neural network
will effectively learn a 100-dimensional representation of handwritten digits. This can be seen as
a variant of PCA in that it also finds a lower-dimensional representation of the digits, however, it
allows a highly non-linear mapping.
</p>
<p>15.4.4 Recurrent neural networks
</p>
<p>In the brain information clearly does not simply flow in one direction as in the feedforward neu-
ral network. An attempt to create more realistic neural networks, where information is processed
multiple times by the same neural network, is a recurrent neural network. Suppose we wish to train
a neural network to read parts of a DNA sequence (a DNA sequence is simply a sequence of four
letters, ACGT , repeated a varying number of times) and determine if the sequence is coding for a
protein or not. We assume we have access to example sequences xi as well as if they express genes
or not, yi = 0, 1.
</p>
<p>This is a standard classification problem were it not for the fact the DNA sequences can have
varying length. One attempt to overcome this is as follows: Suppose each gene x is a sequence of
letters in a one-of-K coding i.e. x = (b1, b2, . . . , bS) for an S-long sequence. We then introduce a
new vector h which is initially zero. The idea is to train a neural network which takes h and a letter
b as inputs and returns an output y consisting of both the label y and a new state h′ concatenated
</p>
<p>as a vector
</p>
<p>[
y
h′
</p>
<p>]
. I.e. [
</p>
<p>y
h′
</p>
<p>]
= f
</p>
<p>([
b
h
</p>
<p>]
,w
</p>
<p>)
.
</p>
<p>This is just a standard feed-forward neural network. We can then apply it to an arbitrary long
sequence by first initializing h(0) = 0 and evaluating[
</p>
<p>y(1)
</p>
<p>h(1)
</p>
<p>]
= f
</p>
<p>([
b1
h(0)
</p>
<p>]
,w
</p>
<p>)
and again for the second digit [
</p>
<p>y(2)
</p>
<p>h(2)
</p>
<p>]
= f
</p>
<p>([
b2
h(1)
</p>
<p>]
,w
</p>
<p>)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>15.4 Advanced topicsF 237
</p>
<p>and so on until for the nth digit: [
y(n)
</p>
<p>h(n)
</p>
<p>]
= f
</p>
<p>([
bn
</p>
<p>h(n−1)
</p>
<p>]
,w
</p>
<p>)
.
</p>
<p>Continuing in this manner for S iterations produces a output y(S) which can then be compared
against the ground truth. This model is quite complicated, but writing out the function evaluation
one can see that the final output y(S) is simply a function of w and the input string x:
</p>
<p>y = F (x,w) = f1
</p>
<p>([
bS f
</p>
<p>([
bS−1 f
</p>
<p>([
bS−2 · · ·
</p>
<p>]T
,w
)]T
</p>
<p>,w
</p>
<p>)]T
,w
</p>
<p>)
.
</p>
<p>Thus, we can train the neural network using gradient descent on the combined function F . The
network is called recurrent since it (recursively) updates the intermediate variable h which allows
it to “remember” information found in the beginning of the gene. Many popular architectures for
working with text is based on recurrent neural networks.
</p>
<p>15.4.5 Serious neural network modelling
</p>
<p>The recent success in neural network modelling is partly due to the creation of powerful com-
putational environments which can automate much of the construction of neural network algo-
rithms. Two of the most popular frameworks are the open-source framework Theano http://
deeplearning.net/software/theano/ and Tensorflow https://www.tensorflow.org/ by google.
Both of these frameworks rely on python and powerful GPU-implementations of the underlying op-
erations. Students who has a serious interest in neural networks should try to learn one of these
frameworks and not try to build the neural networks from the ground up. The benefits of the
framework include
</p>
<p>• Automatic computation of derivations and building of inference code.
• Automatic tuning of relevant parameters.
• Automatic validation.
• Automatic GPU-implementations and (more recently) automatic parallelization of code to run
</p>
<p>on many CPUs and GPUs.
</p>
<p>In addition to this, model validation play a central role in testing different neural network archi-
tectures. It is highly recommended to keep a log book to track the performance of different neural
architectures to see if progress is being made towards solving the problem.</p>
<p></p>
<div class="annotation"><a href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</a></div>
<div class="annotation"><a href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</a></div>
<div class="annotation"><a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a></div>
</div>, <div class="page"><p></p>
<p>238 15 Neural Networks
</p>
<p>Problems
</p>
<p>15.1. Fall 2013 question 23: Which one of the follow-
ing statements pertaining to regression is correct?
</p>
<p>A In regularized least squares regression the aim is to
introduce more variance by reducing substantially
the model’s bias.
</p>
<p>B In least squares regularized regression the regulariza-
tion strength λ is chosen to be the value of λ that
minimizes the term λw&gt;w.
</p>
<p>C An artificial neural network with linear transfer func-
tions (q(t) = t) can be written in terms of a linear
regression model.
</p>
<p>D For regression problems backward or forward selec-
tion can be used to define which part of the output
that is relevant for modeling.
</p>
<p>E Don’t know.
</p>
<p>15.2. Fall 2014 question 5: Consider a feedforward
neural network shown in fig. 15.6. The network has no
bias weights.
</p>
<p>Suppose the weights of the neural network are trained
to be w31 = 0.5, w41 = 0.4, w32 = −0.4, w42 = 0,
w53 = −0.4, w54 = 0.1 and the activation function of
all five n1, . . . , n5 nodes is the thresholded linear func-
tion
</p>
<p>f(x) =
</p>
<p>{
x if x &gt; 0
0 otherwise
</p>
<p>Suppose the network is called evaluated on input x1 =
1, x2 = 2, what is the output?
</p>
<p>
</p>
<p>Input Layer Hidden Layer Output Layer
</p>
<p>Fig. 15.6: Simple neural network of 6 weights
</p>
<p>A ŷ = 0.04
B ŷ = 0.0
C ŷ = 1.0
D ŷ = 0.16
E Don’t know.
</p>
<p>15.3. Fall 2013 question 12: Consider the classifica-
tion problem given in Figure 15.7. The problem is solved
using a 1-nearest neighbor classifier, a decision tree, an
artificial neural network with four hidden units and a
</p>
<p>logistic regression model. All the classifiers are only us-
ing the attributes x1 and x2. The decision boundaries
are indicated in gray and white. We would like to know
which classifier each of the four decision boundaries in
Figure 15.7 correspond to. Which one of the following
statements is correct?
</p>
<p>Fig. 15.7: The decision boundaries given in white and
gray of four different classifiers used to separate red
crosses from black circles.
</p>
<p>A Classifier 1 is the decision tree, Classifier 2 is the arti-
ficial neural network, Classifier 3 is the logistic regres-
sion model, and classifier 4 is the 1-nearest neighbor
classifier.
</p>
<p>B Classifier 1 is the artificial neural network , Classifier
2 is the 1-nearest neighbor, Classifier 3 is the decision
tree, and classifier 4 is the logistic regression model.
</p>
<p>C classifier 1 is the logistic regression model, Classifier 2
is the decision tree, Classifier 3 is the 1-nearest neigh-
bor classifier, and classifier 4 is the artificial neural
network classifier.
</p>
<p>D Classifier 1 is the logistic regression model, Classifier
2 is the 1-nearest neighbor, Classifier 3 is the decision
tree, and classifier 4 is the artificial neural network.
</p>
<p>E Don’t know.
</p>
<p>15.4. Fall 2014 question 19: Consider the classifica-
tion problem given in fig. 15.9. Suppose the problem is
solved using the following four classifiers
</p>
<p>(1NN) A 1-nearest neighbour classifier
(TREE) A decision tree
(LREG) Logistic regression
(NNET) An artificial neural network with four hidden
</p>
<p>units</p>
<p></p>
</div>, <div class="page"><p></p>
<p>15.4 Advanced topicsF 239
</p>
<p>All classifiers are using only the two attributes x1, x2,
corresponding to the position of each observation, as
well as the class label. Which of the descriptions
(1NN),(TREE),(LREG),(NNET) matches the boundari-
ers of the four plots (P1, P2, P3, P4) indicated in fig. 15.8?
</p>
<p>P3
</p>
<p>P1
</p>
<p>P4
</p>
<p>P2
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>Fig. 15.8: Two-class classification problem
</p>
<p>
</p>
<p>
</p>
<p>Class 1
</p>
<p>Class 0
</p>
<p>x
2
</p>
<p>x1
</p>
<p>−1 −0.5 0 0.5 1
</p>
<p>−1
</p>
<p>−0.8
</p>
<p>−0.6
</p>
<p>−0.4
</p>
<p>−0.2
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 15.9: Two-class classification problem
</p>
<p>A P1 is LREG, P2 is 1NN, P3 is TREE, P4 is NNET.
B P1 is LREG, P2 is TREE, P3 is NNET, P4 is 1NN.
C P1 is LREG, P2 is TREE, P3 is 1NN, P4 is NNET.
D P1 is TREE, P2 is LREG, P3 is NNET, P4 is 1NN.
E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>16
</p>
<p>Performance evaluation and class imbalance
</p>
<p>Class imbalance refers to the situation where the classes in a dataset are not represented equally.
The problem with class imbalance is that it confounds our ability to fairly assess the performance
of our model using for instance accuracy. Consider the following example: Suppose Ken is devising
a test for Ebola. Ken is very impressed by his test accuracy of 0.99999999, however, suppose you
learn Ken’s test actually just consists of a card which says: “Ebola negative”, but since there are so
few people with Ebola it still obtains an accuracy of roughly:
</p>
<p>Accuracy of Kens Ebola test = 1− #Cases of Ebola
#Number of people
</p>
<p>(16.1)
</p>
<p>≈ 1− 80
8 000 000 000
</p>
<p>= 1− 10−8. (16.2)
</p>
<p>This is probably the worst Ebola test imaginable – but nobody is ever going to discover it by looking
at the accuracy.
</p>
<p>In this section, we will consider strategies for evaluating models in the presence of class imbalance
for a binary classifier. While class imbalance can certainly be present in the multiclass setting, the
binary setting is simpler and many of the same comments apply. Because class imbalance is a so
frequently occurring feature of many datasets, it has a long history in a variety of fields, see Chawla
[2005] for an overview. The main measure we will consider in this chapter, the area under curve
(AUC) of the receiver operating characteristic (ROC), was originally invented by British radar
engineers around the beginning of the world war II to analyse radar signals [Collinson, 1998].
</p>
<p>16.1 Dealing with class imbalance
</p>
<p>As the example with the Ebola test illustrates class imbalance can make ordinary measures of
performance such as accuracy highly misleading because if we just put everything in the largest
class our method will seem to have a high accuracy. Furthermore, in many situations class imbalance
is not just common but expected, for instance if we are trying to detect fraud in a set of credit card
transactions or build a system to recognize obstacles on the road. In this chapter, we will consider
a few ways to combat class imbalance in increasing degree of sophistication:
</p>
<p>Resampling: where the dataset is changed.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>242 16 Performance evaluation and class imbalance
</p>
<p>Positive class
</p>
<p>Negative class
</p>
<p>0 0.2 0.4 0.6
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>TP = 5
</p>
<p>TN = 2
</p>
<p>FN = 1
</p>
<p>FP = 2
</p>
<p>N = 10
</p>
<p>Actually
Positive
</p>
<p>Actually
Negative
</p>
<p>Predicted
Positive Negative
</p>
<p>Predicted
</p>
<p>(False Positive) (True Negative)
</p>
<p>(False Negative)(True Positive)N+ = 6
</p>
<p>N− = 4
</p>
<p>Fig. 16.1: (Left:) A small N = 10 observation binary classification problem and the classification
boundary. (Right:) The confusion matrix of the classifier in the left-hand pane. The inserts (ticks on
background) indicate which observations counts towards which numbers in the confusion matrix.
</p>
<p>Penalization: where the relative importance of the classes are changed.
Change performance measure: where we use a performance measure such as area-under-curve
</p>
<p>(AUC) of the receiver operating characteristic (ROC) which is invariant to class imbalance.
</p>
<p>16.1.1 Resampling
</p>
<p>The simplest way to handle class imbalance is to change the dataset. There are two variants: If
the dataset is very large, we can consider under-sampling where we simply remove (by random)
observations of the over-represented class until the two classes have the same size. Alternatively, we
can try over-sampling where we add copies from the under-sampled class until the two classes have
the same size. These approaches are very simple to implement and therefore provide an excellent
starting point, however, they also have obvious drawbacks: In the first case we loose information,
in the second case we must take into account some methods can be very influenced by duplicated
observations.
</p>
<p>16.1.2 Penalization
</p>
<p>Penalization works by scaling the relative importance of the two classes. Recall the definition of the
confusion matrix which is reproduced for convenience in fig. 16.1 and which we encountered earlier
in section 8.2.1 of chapter 8. In the notation of the confusion matrix the accuracy of the classifier
can be written as:
</p>
<p>Accuracy =
TP + TN
</p>
<p>N
.
</p>
<p>Let’s assume that it is the positive class which is over-represented. We can then consider a “scaled”
accuracy measure of the form:
</p>
<p>Accuracy-scaled =
TP
</p>
<p>2N+
+
</p>
<p>TN
</p>
<p>2N−
.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>16.1 Dealing with class imbalance 243
</p>
<p>Let’s assume we are in the imbalanced setting where N+ = 1000 and N− = 10. A classifier that
puts everything in the positive class would have an accuracy of 10001010 ≈ 99%, but a scaled accuracy
of only 10002×1000 +
</p>
<p>0
2×10 = 50% corresponding to random guessing (also notice the scaled and true
</p>
<p>accuracy are both 1 if the classifier is perfect). A disadvantage of the scaled accuracy is that it is also
50% if everything is classified as belonging to the negative class which might seem counterintuitive
because all but 10 observations are then classified incorrectly!
</p>
<p>In general, the errors of the classifier are not equally important. Suppose we have credit-card
transaction system where the positive class corresponds to a fraudulent transaction and the negative
to a non-fraudulent (normal) transaction. In this case labelling a few good transactions as fraud-
ulent, FP, (transactions that are actually negative labelled positive) is not so bad, but labelling
fraudulent transactions as good, FN, correspond to a loss of money. We can therefore consider a
general measure of the quality of the classifier of the form
</p>
<p>w1TP + w2FN + w3FP + w4TN, (16.3)
</p>
<p>where w1, · · · , w4 are constants. As a crude example, in the credit-card system we could choose
w1 = 2, w2 = −1000, w3 = −1 and w4 = 0.01 to signify that classifying non-fraudulent transactions
as non-fraudulent (which happen very often) is good (weight 0.01), labelling fraudulent transactions
as fraudulent is even better (weight 2; keep in mind this happens rarely) but incorrectly labelling a
fraudulent transaction as non-fraudulent is very bad (weight −1000). The drawback of this method
is that the user has to specify w1, w2, w3 and w4.
</p>
<p>If we consider the credit card transaction problem, we should ask ourselves why it is so bad to
classify all transactions as being without fraud. The obvious answer is that it is bad because we
loose customers and, ultimately, money. A way around the problem could therefore be to figure out
the expected loss of money for each classification outcome: How much do we expect to loose by
(incorrectly) closing a credit card and annoy a customer and how much do we expect to loose by
not closing a credit card in time? This information could in turn be used to select w1, . . . , w4 in
the penalization scheme eq. (16.3). Keep in mind that especially in medical applications this may
lead to fairly uninviting utilitarianism when bad medical decisions are balanced against monetary
concerns.
</p>
<p>Precision and recall
</p>
<p>Two terms relating to the performance of a classifier which roughly falls within the above category
is the precision and recall. They are simply defined as:
</p>
<p>Recall:
TP
</p>
<p>TP + FN
=
</p>
<p>TP
</p>
<p>#{Observations in the positive class} ,
</p>
<p>Precision:
TP
</p>
<p>TP + FP
=
</p>
<p>TP
</p>
<p>#{Observations predicted as positive} .
</p>
<p>The recall is also known as the true positive rate which we will see again in a moment. The recall
can trivially be improved by labelling all observations as positive, however the precision will suffer
if all observations are labelled positive. Both of these measures are different from for instance the
accuracy in that they place more emphasis on the positive class. For instance in a credit-card fraud
detection system, where fraud corresponds to the positive class, high recall is the measure of how
many actually cases of fraud are caught. Meanwhile precision might be appropriate in a case where
false positive comes at a significant cost, for instance medical screening.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>244 16 Performance evaluation and class imbalance
</p>
<p>Negative, yi = 0
</p>
<p>Positive, yi = 1
</p>
<p>ŷi = f(x,w)
</p>
<p>−5 0 5 10
0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>Negative, yi = 0
</p>
<p>Positive, yi = 1
</p>
<p>ŷi = f(x,w)
</p>
<p>D
en
si
ty
</p>
<p>o
f
o
b
se
rv
a
ti
o
n
s
</p>
<p>−5 0 5 10
0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>Fig. 16.2: A dataset consisting of a positive yi = 1 class and a negative yi = 0 class. The x-position
indicates the predicted y-value by a classification model. In the right plane we have plotted the
same dataset but with the density of each class which is easier to visualize and which will be used
in the following.
</p>
<p>16.2 Area-under-curve (AUC)
</p>
<p>The final strategy we will consider for the class-imbalance problem is to change the performance
measure to implicitly take class imbalance into account. In order to do so we need to take a step back
and consider what a classifier actually does. Consider therefore a standard two-class classification
problem with observations xi and output yi where yi = 0 means observations i belongs to the
negative class and yi = 1 means observations i belongs to the positive class. Suppose we build a
classifier that assigns to each observation i a number ŷi
</p>
<p>ŷi = f(xi,w).
</p>
<p>In many practical situations, the number ŷi will be continuous and only indicate a relative “propen-
sity” for i to belong to a given class according to the classifier, see fig. 16.2. For instance in logistic
regression, ŷi is a (continuous) probability in the interval [0, 1] such that the higher ŷi is the more
likely it is to belong to the positive class.
</p>
<p>How do we evaluate such a classifier? The first step is to translate the continuous numbers ŷi
into binary class-predictions. The simplest way is to introduce a parameter θ and simply assign all
i where ŷi &gt; θ to the positive class and all i where ŷi ≤ θ to the negative class. In fig. 16.3 is shown
two different thresholds. Notice, the different thresholds have a large influence on the behaviour of
the classifier: If we use the high (shown in the left plot) threshold, it is very unlikely to ever say
an observation that in fact is negative (yi = 0) belongs to the positive class, whereas it will be
slightly prone to falsely saying an observation which is in fact positive (yi = 1) is negative. The
other threshold has the opposing effect. Since the threshold is chosen arbitrarily, when we wish to
discuss the performance of a classifier f , we must take into account the different threshold values.
This requires some terminology.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>16.2 Area-under-curve (AUC) 245
</p>
<p>Negative, yi = 0
</p>
<p>Positive, yi = 1
</p>
<p>Labelled as positive
</p>
<p>Labelled as Negative
← θ
</p>
<p>ŷi = f(x,w)
</p>
<p>D
en
si
ty
</p>
<p>o
f
o
b
se
rv
a
ti
o
n
s
</p>
<p>−5 0 5 10
0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>Negative, yi = 0
</p>
<p>Positive, yi = 1
</p>
<p>Labelled as positive
</p>
<p>Labelled as Negative
← θ
</p>
<p>ŷi = f(x,w)
</p>
<p>D
en
si
ty
</p>
<p>o
f
o
b
se
rv
a
ti
o
n
s
</p>
<p>−5 0 5 10
0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>Fig. 16.3: The dataset considered in fig. 16.2 with a positive yi = 1 class and a negative yi = 0 class
and where the x-position indicates the predicted y-value by a classification model. If we threshold
the classifier at the value θ the choice of this threshold value has a large influence on what will be
labelled as positive and negative.
</p>
<p>16.2.1 The confusion matrix and thresholding
</p>
<p>Each thresholding level θ produce a confusion matrix. For convenience this is illustrated in fig. 16.4
and are:
</p>
<p>True Positives, TPθ: Number of observations which are in fact positive yi = 1 which the clas-
sifier correctly labels as positive ŷi &gt; θ.
</p>
<p>False Positives, FPθ: Number of observations which are in fact negative yi = 0 which the clas-
sifier incorrectly labels as positive ŷi &gt; θ.
</p>
<p>False Negatives, FNθ: Number of observations which are in fact positive yi = 1 which the
classifier incorrectly labels as negative ŷi &lt; θ.
</p>
<p>True Negatives, TNθ: Number of observations which are in fact negative yi = 0 which the
classifier correctly labels as negative ŷi &lt; θ.
</p>
<p>Notice, these numbers depend on θ. Since the class sizes (the total number of positive examples
and negative examples) may differ significantly it is common to normalize with the class sizes. We
thus define the true positive rate and false positive rate as:
</p>
<p>FPRθ =
FPθ
</p>
<p>FPθ + TNθ
, (16.4)
</p>
<p>TPRθ =
TPθ
</p>
<p>TPθ + FNθ
, (16.5)
</p>
<p>where by definition TPθ + FNθ is the total number of positive examples and FPθ + TNθ is the
total number of negative examples. An illustration of how these numbers depend on θ is probably
in order. In fig. 16.5 is illustrated three values of the threshold θ. In fig. 16.6 we have plotted the
TPR and FPR for each of these values as solid dots whereas the line indicate all other values of θ.
Notice, the colors agree with fig. 16.4. Let’s try to make some sense of why the curves look the way
they look. When θ is very low, everything is labelled as positive, and so the true positives has to</p>
<p></p>
</div>, <div class="page"><p></p>
<p>246 16 Performance evaluation and class imbalance
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>TPθ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>FPθ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>FNθ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>TNθ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>Fig. 16.4: Everything left of the black line θ is labelled to belong to the negative class and everything
to the right of the black line is labelled as belonging to the positive class. The areas then respectively
indicate the number of true positives, false positives, false negatives and true negatives. See text
for details.
</p>
<p>be all the positives and therefore the true positive rate (TPR) is 1. When θ increases, eventually
everything is labelled negative and so the TPR becomes 0. Roughly, the same goes for the false
positives. First, all elements in the negative class are (incorrectly) labelled as positive, and therefore
the false positive rate is 1. However, as θ increases, the false positive rate will drop faster than the
true positive rate simply because the red hump (of the negatives) is to the left of the blue hump of
the positives. Eventually, everything is labelled as negative and so there are no false positives. This
is why the curves start at 1 and ends at 0 and the TPR is normally above the FPR.
</p>
<p>With these definitions, we can simply plot values of (FPRθ,TPRθ) for all conceivable values
of θ forming the receiver operating charecteristic (ROC) curve given in the right-pane of fig. 16.6.
Since the TPR is normally higher than the FPR, the curve will generally be above the diagonal
indicated by the dotted line. This allows us to define the Area Under Curve (AUC) as simply the</p>
<p></p>
</div>, <div class="page"><p></p>
<p>16.2 Area-under-curve (AUC) 247
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>Fig. 16.5: Illustration of our two classifiers with three different threshold values.
</p>
<p>TPR
</p>
<p>FPR
</p>
<p>Threshold θ
</p>
<p>R
a
t
e
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>False Positive Rate (FPR)
</p>
<p>T
ru
e
P
o
si
ti
v
e
R
a
te
</p>
<p>(T
P
R
)
</p>
<p>0 0.2 0.4 0.6 0.8 1
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 16.6: (left:) Illustration of the TPR and FPR curves for different thresholds for the classifier
indicated in fig. 16.5. The solid points corresponds to the three specific threshold values indicated.
(right:) Illustration of the receiver operating characteristic (ROC) curve. The AUC is simply the
area under this curve obtained by plotting TPRθ against FPRθ for different values of θ. High AUC
is good.
</p>
<p>area under the curve of the ROC curve shown in the right-hand side of fig. 16.6 (which can be
obtained by numerical integration). Since the curves are generally above the dotted line, this value
will be between 0.5 and 1.
</p>
<p>Let’s re-assure ourselves the AUC really behaves like a performance evaluator. If the AUC is 1,
this means it goes through the point (0, 1) meaning that for some θ the number of false positives is
0 and the number of true positives is equal to the total number of positives – i.e. the classifier works
perfectly. On the other hand, let’s suppose we have an inferior classifier as indicated in fig. 16.7,
again with three values of θ selected. The corresponding plot of the TPR and FPR and AUC is
given in fig. 16.8. This curve is much closer to the dotted line, indicating a lower value of the
AUC. Hopefully, these examples are sufficient persuasion the AUC evaluates the performance of
the classifier, but the reader is encouraged to investigate what an AUC of 0.5 would correspond to.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>248 16 Performance evaluation and class imbalance
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>Labelled as positiveLabelled as Negative
</p>
<p>← θ
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>Fig. 16.7: Illustration of three different threshold values for an inferior classifier. The classifier is
inferior since the two predicted classes overlap such that no single threshold can tell them apart.
</p>
<p>TPR
</p>
<p>FPR
</p>
<p>Threshold θ
</p>
<p>R
a
t
e
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>False Positive Rate (FPR)
</p>
<p>T
ru
e
P
o
si
ti
v
e
R
a
te
</p>
<p>(T
P
R
)
</p>
<p>0 0.2 0.4 0.6 0.8 1
0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 16.8: (left:) Illustration of the TPR and FPR curves for different thresholds for the inferior
classifier indicated in fig. 16.7. The solid points corresponds to the three specific threshold values
indicated. (right:) Illustration of the AUC; the AUC is simply the area under the curve obtained
by plotting TPRθ against FPRθ for different values of θ. High AUC is good, i.e. this classifier is
worse than that indicated in fig. 16.5
</p>
<p>In conclusion, the area under curve (AUC) is a performance measure for classifiers, which has
two desirable properties: First, it allows us to get rid of the dependence of θ by integrating over all
the conceivable values of θ. Secondly, it accounts for imbalanced classes due to the normalization
of the TPR and FPR by the number of observations in the true and false class respectively. A
drawback of the AUC is that it only allows for two classes.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>16.2 Area-under-curve (AUC) 249
</p>
<p>Problems
</p>
<p>16.1. Fall 2014 question 24: Suppose a small network
is trained on a binary classification dataset containing
N = 1000 points. The output of the neural network is
continuous and restricted to ŷ ∈ [0, 1]. To evaluate the
performance on the network, the network makes predic-
tions on a test set of Ntest = 1000 points. By thresholding
the output of the neural network at different levels, i.e.
for each level θ assign an output ŷi to class 0 if ŷ ≤ θ and
otherwise to class 1, one obtains different values of the
TP, TN, FP and FN. This allows us to draw the ROC
curve shown in fig. 16.10. Which of the true positive rate
(TPR), and false positive rate (FPR) curves A,B,C or
D shown in fig. 16.9 corresponds to the ROC curve in
fig. 16.10?
</p>
<p>
</p>
<p>
</p>
<p>False positive rate (FPR)
</p>
<p>True positive rate (TPR)
</p>
<p>T
P
R
</p>
<p>an
d
F
P
R
</p>
<p>Threshold value
</p>
<p>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
</p>
<p>0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
</p>
<p>A
</p>
<p>T
P
R
</p>
<p>a
n
d
F
P
R
</p>
<p>Threshold value
</p>
<p>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>B
</p>
<p>T
P
R
</p>
<p>a
n
d
F
P
R
</p>
<p>Threshold value
</p>
<p>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>C
</p>
<p>T
P
R
</p>
<p>a
n
d
F
P
R
</p>
<p>Threshold value
</p>
<p>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>D
</p>
<p>Fig. 16.9: Proposed values of the TPR and FPR, as calcu-
lated for N = 1000 predicted values, for different values
of the threshold θ.
</p>
<p>ROC
</p>
<p>T
ru
e
p
os
it
iv
e
ra
te
</p>
<p>(T
P
R
)
</p>
<p>False positive rate (FPR)
</p>
<p>0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>Fig. 16.10: ROC curve of a neural network.
</p>
<p>A Figure A
B Figure B
C Figure C
D Figure D
E Don’t know.
</p>
<p>16.2. Fall 2014 question 25: Consider the true posi-
tive rate (TPR), and false positive rate (FPR) as a func-
tion of the threshold θ for the problem of N = 1000
observations shown in fig. 16.9(A). Suppose we consider
predictions made at a threshold of θ = 0.3, and suppose
we are told that the number of true negatives at θ = 0.3
is TN = 489, the TPR at this threshold is TPR = 0.412
and the FPR at this threshold is FPR = 0.164. What is
the (approximate) number of true positives (TP ) of the
model at this threshold?
</p>
<p>A TP = 93
B TP = 171
C TP = 275
D TP = 381
E Don’t know.
</p>
<p>16.3. Fall 2013 question 17: We will consider sur-
vived as the positive class (i.e, y = 1) and died as the
negative class (i.e., y = 0) in Haberman’s survival dataset
shown in Table 16.1. Considering again the confusion ma-
trices for the logistic regression and decision tree classi-
fiers given in Figure 16.11, which one of the following
statements is correct?</p>
<p></p>
</div>, <div class="page"><p></p>
<p>250 16 Performance evaluation and class imbalance
</p>
<p>Fig. 16.11: The confusion matrix for the logistic regres-
sion and decision tree classifiers used to predict survival
based on leave-one-out cross validation.
</p>
<p>No. Attribute description Abbrev.
</p>
<p>x1 Young (&lt; 60 years), x1 = 0 or Age
Old (≥ 60 years), x1 = 1
</p>
<p>x2 Operated before, x2 = 0 or OpT
after 1960, x2 = 1
</p>
<p>x3 Positive axillary nodes detected PAN
No, x3 = 0 or Yes, x3 = 1
</p>
<p>y Lived after 5 years Surv
No, y = 0 or Yes, y = 1
</p>
<p>Table 16.1: A modified version of Haberman’s Survival
Data taken from http://archive.ics.uci.edu/ml/machine-
learning-databases/haberman/haberman.names. The at-
tributes x1-x3 denoting the age, operation time and can-
cer size as well as the output denoting survival after five
years are binary. The data contains a total of N = 306
observations.
</p>
<p>A The precision of the logistic regression classifier is
higher than the precision of the decision tree classi-
fier.
</p>
<p>B The recall of the logistic regression classifier is higher
than the recall of the decision tree classifier.
</p>
<p>C The true negative rate of the logistic regression classi-
fier is lower than the true negative rate of the decision
tree classifier.
</p>
<p>D The false positive rate of the logistic regression clas-
sifier is higher than the false positive rate of the de-
cision tree classifier.
</p>
<p>E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>17
</p>
<p>Ensemble methods
</p>
<p>Ensemble methods are methods where multiple models are trained and their outputs are then
combined to obtain better predictive performance than each of the methods on their own. The
procedure can be compared to how we might ask the opinion of many doctors and use an average
of their opinions to form our opinion or consider multiple reviews before buying a cinema ticket.
The benefits of this procedure in machine learning is twofold: Firstly, averaging many classifiers
produces a classifier with less variance than each of the individual classifiers. This allows us to use
classifiers with larger variance and therefore better ability to tell observations apart while being less
sensitive to the occasional overfitting of each individual classifier, much like using an aggregate of
several doctors opinion can be used to rule out the occasional crank doctor. Secondly, while most
classifiers may assign two hard-to-tell-apart observations to the same class, it may be that a few
classifiers in the ensemble can tell the two observations apart, much as how many doctors may have
difficulties telling two diagnosis apart where but a few specialist doctors can. (The specialists may
then not be able to tell other observations apart but hopefully other specialists in the ensemble
can).
</p>
<p>In this chapter, we will consider two methods for ensemble methods, bagging and boosting.
Bagging is the simpler method where model predictions are simply averaged whereas boosting
actively seek out hard-to-classify observations and build specialized models for these observations.
The idea behind bagging has roots going back to [Dasarathy and Sheela, 1979] and was applied to
neural networks by [Hansen and Salamon, 1990]. The most famous application of bagging, random
forests where bagging is applied to decision trees, was initially developed by Ho [1995] and later
developed into the method of random forest by Breiman [2001]. Boosting was developed in the
very early 90s by Schapire [1990] and the particular method we will consider here, AdaBoost, was
developed about 10 years later by [Freund and Schapire, 1997].
</p>
<p>17.1 Introduction to ensemble methods
</p>
<p>The basic idea of ensemble methods is very simple: train multiple models and combine their outputs
into a single model as illustrated in fig. 17.1. Let’s consider how this combination takes place before
we discuss how the ensemble of models is produced: SupposeM1, . . . ,MT are T regression models
and model Mt is trained on some data D to learn a regression function y = ft(x), we can then
define a new model M∗ as simply the average:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>252 17 Ensemble methods
</p>
<p>D
</p>
<p>M∗
</p>
<p>Create multiple classifiers
</p>
<p>Combine classifiers
</p>
<p>M1 M2 M3 MT
</p>
<p>Fig. 17.1: Combining T models M1, . . . ,MT to a single classifier M∗ using majority voting (clas-
sification) or averaging (regression) is often a useful strategy to come up with a classifier which
outperforms each individual model.
</p>
<p>(Regression:) y =
1
</p>
<p>T
</p>
<p>T∑
t=1
</p>
<p>ft(x). (17.1)
</p>
<p>Alternatively, suppose M1, . . . ,MT are classifiers, i.e. f(x) = y = 1, . . . , C. We can then combine
their outputs by letting each classifier “vote” for an output and then select the class which most
classifiers agree is the correct one.
</p>
<p>(Classification:) f(x) = arg max
c=1,...,C
</p>
<p>{Number of classifiers which output ft(x) = c}, (17.2)
</p>
<p>this is known as majority voting. In case of ties, the classifier can just select at random from the
tied classes.
</p>
<p>So why might ensemble methods work? Suppose we consider a binary classification problem with
T independent classifiers. If each classifier is correct with probability p, the chance the majority
voting scheme will classify a new point correctly is,
</p>
<p>P (Majority voting is correct) =
</p>
<p>T∑
t=(T+1)/2
</p>
<p>{t of the classifiers are correct}
</p>
<p>=
</p>
<p>T∑
t=dT/2e
</p>
<p>(
T
</p>
<p>t
</p>
<p>)
pt(1− p)T−t,
</p>
<p>where dae denotes rounding a up to the closest integer value larger than or equal to a. The graph
as a function of T is plotted in fig. 17.2. In reality we do not have access to independent classifiers
even if we are using quite different methods, however, in practice combining different classifiers,
especially when they rely on different assumptions, often performs better than simply using the
best classifier, and for machine-learning competitions this is a strategy which is often used by the
winner.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>17.2 Bagging 253
</p>
<p>Number of classifiers T
</p>
<p>C
h
a
n
ce
</p>
<p>co
m
b
in
ed
</p>
<p>cl
a
ss
ifi
er
</p>
<p>is
co
rr
ec
t
</p>
<p>1 3 5 7 9 11 13 15
0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>1
</p>
<p>Fig. 17.2: If T independent classifiers is combined using majority voting, each with an accuracy of
only p = 0.7, the accuracy of the resulting classifier quickly approaches 1. In practice, it is difficult to
find independent classifiers, however, the picture still holds approximately for dependent classifiers.
</p>
<p>A problem with combining T classifiers is that it requires about T times as much work to create
T classifiers as it takes to create one. A strategy which is therefore often used is to use the same
classifier, but train it on different training datasets, see fig. 17.3. There are essentially two strategies:
</p>
<p>• Apply different transformations to the training set, for instance images can be rotated or trans-
lated.
• Select a subset of features.
• Re-sample subsets of the training set.
</p>
<p>The first technique is specific to the application, however, it is very often used in Neural-network
applications to images. The second technique is very popular for decision trees and is used in
random forests which we will consider in section 17.3. The third technique, resampling the dataset,
and depending on how the dataset is resampled we either obtained bagging or boosting which we
will consider in the following sections.
</p>
<p>17.2 Bagging
</p>
<p>Bagging begins with a dataset D of size N , and then randomly selects T new datasets D1, . . . ,DT of
size N ′ ≤ N by randomly subsampling D. The simplest strategy is to set N ′ = N and sample each
Dt by randomly selecting N points from D with replacement. That is, the same points may occur
many times in each Dt and some points may be omitted. The same classification (or regression)
model is then trained on each of the T datasets to produce T different classifiers which are then
combined into a single classifier using eq. (17.1) or eq. (17.2). This procedure is illustrated in fig. 17.3</p>
<p></p>
</div>, <div class="page"><p></p>
<p>254 17 Ensemble methods
</p>
<p>D
</p>
<p>M∗
</p>
<p>Create multiple datasets
</p>
<p>Create multiple classifiers
</p>
<p>Combine classifiers
</p>
<p>D1 D2 D3 DT
</p>
<p>M1 M2 M3 MT
</p>
<p>Fig. 17.3: A different strategy for obtaining multiple classifiers is to create T new datasetsD1, . . . ,DT
from the training dataset D and train classifiers to each of the dataset. The T obtained classifiers
can then be combined as in fig. 17.1.
</p>
<p>and the number of classifiers T can either be selected as a high (but tractable) number or selected
using cross-validation. Typically, about 100-1000 classifiers are used.
</p>
<p>Bagging applied to logistic regression
</p>
<p>We will illustrate the bagging procedure with a small 2-class classification problem with N = 16
points shown in fig. 17.4 (left). The dataset is fitted with a standard logistic regression model giving
the linear decision boundary p(y|x,D) shown in the middle pane. Since we are only interested in the
class labels for the majority-voting scheme eq. (17.2) we will assume the predictions of the logistic
regression model is threshold at 0.5 to produce the decision boundary shown in the right pane.
</p>
<p>In fig. 17.5 bagging is illustrated for T = 8. In each pane, a subset of the datasets are selected at
random and the points not selected are shown as hollow circles. As can be seen, there is quite a lot
of variability in the decision surfaces since the datasets are random and consists of few observations.
In fig. 17.6 the bagging classifiers are combined, i.e., for each point x we plot the bagged classifiers’
predictions:
</p>
<p>y =
1
</p>
<p>T
</p>
<p>T∑
t=1
</p>
<p>ft(x), (17.3)
</p>
<p>and the black line corresponds to y &gt; 12 corresponding to majority voting eq. (17.2). As seen from
the figure, each single classifier is worse than the classifier which used all data shown in fig. 17.4
(middle and left pane), however, the errors average out and produce a decision boundary which
follows the dataset slightly better than any single logistic regression model. In the right-pane of
fig. 17.6 is shown the same bagging setup but using T = 100 classifiers. Again, we see the use of many</p>
<p></p>
</div>, <div class="page"><p></p>
<p>17.3 Random Forests 255
</p>
<p>Class 1
</p>
<p>Class 2
</p>
<p>Fig. 17.4: (left:) A simple 2D classification problem with two classes and two features. (middle:) A
logistic regression model is fitted to the data to give the class-probability indicated with the colors.
(right:) thresholding the logistic regression output at 0.5 gives the classification boundary indicated
by the colors. This is the decision rule of the classifier.
</p>
<p>classifiers average out the errors and produces (some) non-linearity in the classification rule which
(slightly) better follows the data. The reason why bagging does not affect the classification accuracy
very much in this example is because the classifiers are still highly correlated: If all classifiers are the
same, clearly bagging will have no effect at all, and as a rule a diverse pool of classifiers as possible
is desirable. A diverse pool of classifiers can be obtained by for instance including extra features
using feature transformations (for instance high-order Taylor expansions such as x2i ) or varying the
parameters in each of the models in the bagging ensemble. When we consider random forests in
section 17.3 we will look at a technique for creating a diverse class of classifiers by manipulating
the tree-learning method.
</p>
<p>17.3 Random Forests
</p>
<p>Random forests is simply an application of bagging to decision or regression trees. Bagging of
decision trees were first developed in a basic version by Ho [1995], which was later extended into the
random forest method by Breiman [2001], a paper which has garnered more than 23 000 citations.
In order to introduce random forests let’s first discuss the simple bagging procedure applied to
decision or regression trees: Bagging first produces T datasets (by sampling with replacement)
from the original dataset (X,y), then train the standard decision tree algorithm on each sampled
dataset to produce a predictor ft(x) for t = 1, . . . , T and finally combine the predictors using either
eq. (17.1) (regression) or eq. (17.2) (classification). As for the logistic regression example, a problem
is that the decision trees will often select the same splits over and over again at the root and directly
adjacent branches creating very correlated trees. To overcome this, Breiman [2001] proposed that
when generating tree Tt, at each step of Hunt’s algorithm, Hunt’s algorithm should only consider
splits from m &lt; M of the features selected at random from all M features (new sets are considered
for each new node of the tree). Since the root split will (often) not have the same features available
this produces less correlated trees.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>256 17 Ensemble methods
</p>
<p>Fig. 17.5: Example of bagging for the classification problem in fig. 17.4. In each pane, a new training
set Dt is obtained by sampling N points with replacement from D and a logistic regression model
is fitted. Notice, not all observations are selected and some points may be selected multiple times.
Observations not selected are indicated by hollow circles.
</p>
<p>There are a few other ingredients to the method found in Breiman [2001], however, the random-
ness at the feature-selecting step and bagging are the main ones. Typically, T is taken to be of the
order 100-1000 and m =
</p>
<p>√
M for classification and 13M for regression [Hastie et al., 2009, Chapter
</p>
<p>15].
</p>
<p>17.4 Boosting
</p>
<p>As we saw bagging produced some non-linearity in the decision surface of a linear classifier (i.e.,
logistic regression), however, at least for the considered problem it was quite slight and it still
had difficulty with the island of orange points. A message to take away from the problem is that
most of the observations are easy to classify, however some are very hard. An alternative strategy
would therefore be to select the hard-to-classify observations more often than those which are
easy to classify and thereby create classifiers which are better suited to solve the hard part of the
classification problem. This is basically the idea in boosting.
</p>
<p>To make the above idea more concrete, suppose we introduce a parameter wi for each observation
xi in the training data set. wi is the probability of selecting this particular observation when creating
the bagging data set, i.e. wi &gt; 0 and
</p>
<p>∑N
i=1 wi = 1. In the bagging algorithm wi =
</p>
<p>1
N , however, in
</p>
<p>boosting the idea is to iteratively adjust wi depending on how difficult observation i is to classify.
The basic boosting algorithm is illustrated in fig. 17.7 and consists of the following steps:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>17.4 Boosting 257
</p>
<p>Fig. 17.6: (left:) By averaging the individual prediction boundaries from fig. 17.5 using eq. (17.3)
we can define the majority voting rule. The resulting classification boundary (i.e. thresholding at
0.5) is indicated by the black line. In the right pane the same construction is shown but for T = 100
datasets. Notice, the resulting rule is smoother and still slightly non-linear.
</p>
<p>• We first select a training set D1 by sampling N observations with replacement with probability
wi of selecting an observation i; the dataset D1 with a fitted logistic regression model is shown
in the left-most pane. Notice, usually not all points are selected.
• In the next step the decision boundary is used to see which of all points in the training dataset
</p>
<p>are classified incorrectly marked with red in the second pane from the left.
• This information is used to update the weights in a way we will specify later but such that
</p>
<p>weights of the wrongly classified points are increased and the weights of the correctly classified
points are decreased. The weights still sum to 1; this is indicated by the size of the points in
the third pane of fig. 17.7.
• Finally, a new dataset D2 is selected by randomly sampling according to the new weights and the
</p>
<p>procedure is repeated for this new dataset, i.e. a new classifier trained on D2, weights updated
a new dataset sampled and so on.
</p>
<p>Obviously, we still need to specify how the weights are updated. One can try to come up with a
reasonable scheme based on one’s intuition, however the weight-updating problem can be analyzed
using decision theory which has led to the AdaBoost algorithm [Freund and Schapire, 1997].
</p>
<p>17.4.1 AdaBoost
</p>
<p>Suppose we denote by w(t) the weight of the observations at step t which determines how likely
that observation is to be included in the training set. The AdaBoost algorithm then produces T
classifiers f1(x), · · · , fT (x) and importance weights α1, · · · , αT (which determines how important</p>
<p></p>
</div>, <div class="page"><p></p>
<p>258 17 Ensemble methods
</p>
<p>Fig. 17.7: Illustration of a boosting sweep. (Top left:) a dataset Dt is selected by random sampling
from D with replacement but with probability wi of selecting observation i and a logistic regression
model is fitted to the dataset. Points not selected are hollow. (Top right:) all points are classified
using the trained classifier and the misclassified observations are shown in red. (Bottom left:) the
weights wi corresponding to the red misclassified points are increases and the rest are decreased
(indicated by the size). (Bottom right:) The next dataset is selected by random sampling using the
new weights and the procedure is repeated.
</p>
<p>each classifier is) which are combined to produce the output of the method:1
</p>
<p>1 To get a feeling for this definition, recall the delta-function δa,b is defined as δa,b =
</p>
<p>{
1 if a = b
</p>
<p>0 if a 6= b
.
</p>
<p>The combined classifier f∗ can therefore be understood to first compute the number of “votes” for the
positive class:
</p>
<p>∑
i:yt(xi)=1
</p>
<p>αi (and similarly for the negative class, a
− =
</p>
<p>∑
i:yt(xi)=0
</p>
<p>αi) and then output
</p>
<p>1 if α+ &gt; α− and otherwise 0</p>
<p></p>
</div>, <div class="page"><p></p>
<p>17.4 Boosting 259
</p>
<p>Fig. 17.8: The (importance-weighted) decision function eq. (17.4) when Boosting is applied for
T = 10 (left) and T = 500 (right) rounds, The decision boundary is indicated by the black line.
Notice, the decision boundary is highly non-linear and for T = 500 (right) perfectly fits the training
data even though each classifier is linear.
</p>
<p>f∗(x) = arg max
y=1,2
</p>
<p>T∑
t=1
</p>
<p>αtδft(x),y. (17.4)
</p>
<p>The AdaBoost algorithm updates w(t) and αt by first computing the weighted error:
</p>
<p>�t =
</p>
<p>N∑
i=1
</p>
<p>wi(t)
(
1− δft(xi),yi
</p>
<p>)
(17.5)
</p>
<p>The importance of the classifier at step t is then computed as:
</p>
<p>αt =
1
</p>
<p>2
log
</p>
<p>1− �t
�t
</p>
<p>(17.6)
</p>
<p>and finally the new weights w(t+ 1) are updated by computing:
</p>
<p>wi(t+ 1) =
w̃i(t+ 1)∑N
j=1 w̃j(t+ 1)
</p>
<p>(17.7)
</p>
<p>where w̃j(t+ 1) =
</p>
<p>{
wj(t)e
</p>
<p>−αt if ft(xj) = yj
</p>
<p>wj(t)e
αt if ft(xj) 6= yj .
</p>
<p>Thus, this mechanism either up- or downscales the weights with a factor depending on the impor-
tance parameter at the current round, αt. Finally the majority voting classifier M∗ is found by</p>
<p></p>
</div>, <div class="page"><p></p>
<p>260 17 Ensemble methods
</p>
<p>Algorithm 7: AdaBoost algorithm
</p>
<p>1: Initialize wi(1) =
1
N
</p>
<p>for i = 1, . . . , N
2: for t = 1, . . . , T do
3: Create Dt by sampling (with replacement) from D according to w(t)
4: Let ft be the classifier trained on Dt
5: �t =
</p>
<p>∑N
i=1 wi(t)
</p>
<p>(
1− δft(xi),yi
</p>
<p>)
(weighted error of ft on all data).
</p>
<p>6: αt =
1
2
</p>
<p>log 1−�t
�t
</p>
<p>7: For each i update weights using eq. (17.7):
</p>
<p>wi(t+ 1) =
w̃i(t+ 1)∑N
j=1 w̃j(t+ 1)
</p>
<p>, w̃i(t+ 1) =
</p>
<p>{
wi(t)e
</p>
<p>−αt if ft(xi) = yi
</p>
<p>wi(t)e
αt if ft(xi) 6= yi.
</p>
<p>8: end for
9: f∗(x) = arg maxy=1,2
</p>
<p>∑T
t=1 αtδft(x),y (Majority voting classifier)
</p>
<p>averaging the vote of each classifier with the importance parameters and selecting the most popular
output:
</p>
<p>f∗(x) = arg max
y=1,2
</p>
<p>T∑
t=1
</p>
<p>αtδft(x),y.
</p>
<p>The full AdaBoost procedure can be seen in algorithm 7 and in fig. 17.8 we have plotted importance-
weighted decision functions.
</p>
<p>When AdaBoost is applied to the N = 16-observations example considered previously for T = 10
or T = 500 boosting rounds the individual AdaBoost classifiers are much more extreme since they
are trained on fewer datapoints, however, when many AdaBoost classifiers are averaged the decision
boundary becomes highly non-linear and is able to separate the two classes.
</p>
<p>17.4.2 Properties of the AdaBoost algorithmF
</p>
<p>As mentioned, the peculiar form of the update rules for αt and w(t) in the AdaBoost is due to a
decision-theoretical analysis in [Freund and Schapire, 1997]. It can be shown the training error of
the ensemble classifier f∗ is bounded by
</p>
<p>�∗ ≤
T∏
t=1
</p>
<p>2
√
�t(1− �t),
</p>
<p>where �t are the error rates of each of the classifiers as described in algorithm 7. Suppose each
error rate is less than 50%, we can then write �t =
</p>
<p>1
2 − γt. Then γt measures how much better the
</p>
<p>classifier is than random guessing and by standard inequalities:
</p>
<p>�∗ ≤
T∏
t=1
</p>
<p>2
√
�t(1− �t) =
</p>
<p>T∏
t=1
</p>
<p>√
1− 4γ2t ≤ e−2
</p>
<p>∑T
t=1 γ
</p>
<p>2
t .
</p>
<p>Consequently, if all γt ≥ γ0 then the training error of the ensemble is bounded as �∗ ≤ e−2γ
2
0T and
</p>
<p>thus decreases exponentially in T . This may sound like great news, however, recall from chapter 10</p>
<p></p>
</div>, <div class="page"><p></p>
<p>17.4 Boosting 261
</p>
<p>a low training error is not in itself a good sign. Theoretical analysis of AdaBoost reveals that with
high probability: [Freund and Schapire, 1997]
</p>
<p>Test error ≤ Train error +O
(√
</p>
<p>dT
</p>
<p>N
</p>
<p>)
,
</p>
<p>where d is a term dependent of the complexity of our classification model and O(·) means a term
that scale no faster than what is in the parenthesis.
</p>
<p>So this is a slightly more negative picture, since when T increases the test error may go towards
zero, however, the second term will grow as
</p>
<p>√
T . In addition, we do not know the scaling factor
</p>
<p>of the second term so the above result should not be taken as predicting the test error is lower
than the training error which it will almost certainly never be. From an intuitive perspective, when
we only select a very small subset of training points in each round t (the difficult points), each
classifier is very prone to overfitting which is why the combined classifier can fit the training data
perfectly. When the classifiers are combined, this average out some of the overfitting, however, the
combined classifier may still be overfitting the data which plausibly is already happening in fig. 17.8.
In practice, AdaBoost often turns out to work very well and increases performance, however, as
always it is important to test if that is actually the case using for instance cross-validation.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>262 17 Ensemble methods
</p>
<p>Problems
</p>
<p>17.1. Fall 2014 question 26: Suppose Jane wishes to
apply a decision tree classifier to a binary classification
problem of only N = 4 observations. Training and apply-
ing the decicion tree to the full dataset X and y1, . . . , y4
gives predictions ŷ1, . . . , ŷ4 shown in table 17.1.
</p>
<p>y ŷ
</p>
<p>1 1
1 0
0 0
0 0
</p>
<p>Table 17.1: True values yj and predictions ŷj for a deci-
sion tree classifier trained on the full data set with ob-
served values y1, . . . , y4.
</p>
<p>To improve performance Jane decides to apply Ad-
aBoost, however Jane implements AdaBoost such that
instead of sampling the N elements of the training sets
Di with replacement, Jane samples the training sets with-
out replacement, i.e. the training set Di is simply the full
</p>
<p>dataset. Suppose Jane applies AdaBoost for k = 1 round
of boosting, what is the resulting (approximate) value for
the weights w?
</p>
<p>A w =
[
0.123 0.630 0.123 0.123
</p>
<p>]
B w =
</p>
<p>[
0.167 0.5 0.167 0.167
</p>
<p>]
C w =
</p>
<p>[
0.081 0.756 0.081 0.081
</p>
<p>]
D w =
</p>
<p>[
0.077 0.769 0.077 0.077
</p>
<p>]
E Don’t know.
</p>
<p>17.2. Fall 2013 question 24: Which one of the fol-
lowing statements pertaining to bagging or boosting is
correct?
</p>
<p>A In boosting miss-classified observations are given less
importance in the next round.
</p>
<p>B For each round of bagging it is expected that only a
subset of the observations are used for training.
</p>
<p>C Boosting uses leave-one-out cross-validation to learn
which observations to sample in the next round.
</p>
<p>D When combining multiple classifiers using bagging
the classifier with the best performance is selected.
</p>
<p>E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Part III
</p>
<p>Unsupervised learning</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>18
</p>
<p>Distance-based clustering techniques
</p>
<p>In this and the following chapters we will consider unsupervised learning techniques. In the previous
sections we considered the dataset as being composed of a data matrix X and a set of target values
y. In unsupervised learning we assume we only have X and our goal is to infer structure in X such
as a clustering (which observations naturally group together), outlier detection (which observations
are anomalous), density estimation (how typical is a given observation), and association mining
(what are prominent patterns of binary feature co-occurrence). All these tasks depend on how one
defines a clustering or an outlier and are thus not nearly as well defined as supervised learning
where we know what the target y is.
</p>
<p>In this chapter, we will consider the particular unsupervised learning problem of identifying
groups, or clusters, of data points in a space of arbitrary dimension. Recall a clustering of a set
of observations is simply a division of the set of observations into non-overlapping sets, often illus-
trated as a coloring of the observations. We will consider two methods, K-means and agglomerative
hierarchical clustering. Both of these methods are similar in that they are distance-based. How-
ever, they differ in that K-means attempts to identify K clusters, whereas hierarchical clustering
identifies a nested clustering.
</p>
<p>K-means clustering was first discovered by the polish mathematician Hugo Steinhaus in
1956 [Steinhaus, 1956] but given its name and popularized by MacQueen et al. [1967]. The ba-
sic hierarchical clustering algorithm was discovered by Johnson [1967].
</p>
<p>18.1 Types of clusters
</p>
<p>As already mentioned, what constitutes a clustering of a set of observations is somewhat in the eye
of the beholder, and different clustering methods are suitable for producing clusters with different
properties. We will therefore begin by discussing some general categories of clustering described in
Tan et al. [2013].
</p>
<p>18.1.1 The distance-based cluster types
</p>
<p>The simplest cluster types are the simple, distance-based types illustrated in fig. 18.1 and which
are all defined by the distance between observations and (possible) the center of clusters. They are:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>266 18 Distance-based clustering techniques
</p>
<p>Well-Separated
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Center-Based
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Contiguity-Based
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 18.1: Illustration of the three simple cluster types. The colors indicate the clusters.
</p>
<p>Well-separated Each point is closer to all points in its cluster than any point in another cluster.
As we will see, hierarchical clustering with max-linkage assumes clusters are well-separated
when identifying clusters.
</p>
<p>Center-based Each point is closer to the center of its cluster than to the center of any other cluster.
As we will se, K-means and Ward clustering (and arguable also average linkage hierarchical
clustering) takes a center-based approach to finding clusters.
</p>
<p>Contiguity-based Each point is closer to at least one point in its cluster than to any point in
another cluster. Hierarchical clustering with min-linkage takes a contiguity-based approach to
finding clusters.
</p>
<p>18.1.2 More elaborate cluster types
</p>
<p>The above three basic types of clustering are the simplest, but it is possible to consider methods
that rely on more elaborate (or specific) definitions of what constitutes the clustering. A particular
important example are density-based clustering (where a cluster is a group of observations that
lie unusually close to each other), however we have also included conceptual clusters as a separate
category for cluster-definitions that does not fit any of the other descriptions, see fig. 18.2.
</p>
<p>Density-based Clusters are regions of high density separated by regions of low density. The Gaus-
sian mixture-model, which we will consider in chapter 19 takes a density-based approach to
finding clusters.
</p>
<p>Conceptual clusters Points in a cluster share some general property that is derived from the
entire set of points.
</p>
<p>18.2 K-means clustering
</p>
<p>The goal of K-means clustering is to take as input an arbitrary data set X comprised of N observa-
tions x1, . . . ,xN in a D-dimensional space and then partition (or cluster) the data observations into
K groups. A natural way to represent such a partition is as a coloring, where each of the K groups</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.2 K-means clustering 267
</p>
<p>Density-Based
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Conceptual
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>Fig. 18.2: Illustration of two more elaborate cluster types. Note we do not have any general methods
for finding conceptual clusters.
</p>
<p>−1.5 −1 −0.5 0 0.5 1 1.5
</p>
<p>−1.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>Fig. 18.3: 2D K-means example dataset. Observations are indicated by gray points and the initial
location of the K-means cluster locations are indicated as the colored squares. In the example, the
location of the clusters are initialized at random.
</p>
<p>corresponds to one of K colors and the clustering then corresponds to coloring the observations.
In K-means clustering, a cluster is considered a group of observations where the distance between
observations within the group is small relative to the distance between observations outside the
group. This notation can be formalized by introducing a vector µk for each group k = 1, . . . ,K
which represents the typical location (or prototypical element) of the group. An observation xi then
belongs to the cluster k where the distance (typically based on the Euclidean distance ‖xi−µk‖) is
the smallest and as we will see in a moment the µk’s represent the centers of the clusters. However,
before explaining why the K-means algorithm is the way it is, it is easier to explain what it does
since it is such a simple algorithm.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>268 18 Distance-based clustering techniques
</p>
<p>−1.5 −1 −0.5 0 0.5 1 1.5
</p>
<p>−1.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>−1.5 −1 −0.5 0 0.5 1 1.5
</p>
<p>−1.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>−1.5 −1 −0.5 0 0.5 1 1.5
</p>
<p>−1.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>−1.5 −1 −0.5 0 0.5 1 1.5
</p>
<p>−1.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>−1.5 −1 −0.5 0 0.5 1 1.5
</p>
<p>−1.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>−1.5 −1 −0.5 0 0.5 1 1.5
</p>
<p>−1.5
</p>
<p>−1
</p>
<p>−0.5
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>Fig. 18.4: Example of running the K-means algorithm for three iterations and K = 4 clusters.
Starting with initial location of the cluster vectors indicated by the colored squares, the observations
are first assigned to the closest mean vector µk (top-left). Then in the next step (top-left) the cluster
location vectors µk are updated to correspond to the mean of the assigned points. This procedure
is repeated in the second row, however, in the third row (bottom-left) the cluster assignments do
not change and the method has therefore converged.
</p>
<p>Suppose we wish to apply the K-means algorithm (based on Euclidean distance) to the 2D
dataset shown in fig. 18.3 (gray circles) for K = 4 clusters. The location of each of the four µk
cluster locations are indicated by the colored squares. This is accomplished by the following steps:
</p>
<p>• First, initialize each µk at a random location as shown in fig. 18.3.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.2 K-means clustering 269
</p>
<p>• Assign each of the gray points to the nearest µk. It now belongs to this cluster. In fig. 18.4
(top-left pane) the region belonging to each cluster is indicated by the colors.
• Update the location of each µk to be the mean of the points assigned to it. In fig. 18.4 this is
</p>
<p>shown in the top-right pane.
• Repeat the two previous steps until the location of µk does not change. In fig. 18.4 this is shown
</p>
<p>in row two and three.
</p>
<p>18.2.1 A closer look at the K-means algorithm
</p>
<p>So why does the K-means algorithm look the way it does? The objective of the K-means algorithm
is to find a set of K vectors µ1, . . . ,µK as well as an assignment of observations to clusters such
that the sum-of-squares of each observation to the nearest vector µk is minimized. If we for each
point i introduce a binary variable zik describing which cluster k = 1, . . . ,K observation i belongs
to, such that if xi belongs to k then zik = 1 and zih = 0 for h 6= k. Considering Euclidean distance
we can define the sum-of-squares between each point to the cluster it is assigned to as:
</p>
<p>E =
</p>
<p>N∑
i=1
</p>
<p>K∑
k=1
</p>
<p>zik‖xi − µk‖22. (18.1)
</p>
<p>Our goal is then to find values of zik and µk to minimize E. The two steps in the K-means algorithm
accomplish exactly this. In the first step, the upper-left pane of fig. 18.4, we keep the µk’s fixed
and minimize zik. Since this expression is independent for each observation i, we can just for each
i choose zik to minimize
</p>
<p>∑K
k=1 zik‖xi − µk‖22. Obviously, this corresponds to selecting zik = 1 for
</p>
<p>the k where µk is the closest (in Euclidean distance) to xi. In other words:
</p>
<p>zik =
</p>
<p>{
1 if k = arg minh ‖xi − µh‖22
0 otherwise.
</p>
<p>(18.2)
</p>
<p>Now consider the second step, the upper-left pane of fig. 18.4, where the location µk are updated
and zik are kept fixed. If we consider the derivative of the objective function with respect to µk we
obtain:
</p>
<p>∇µkE = 2
N∑
i=1
</p>
<p>zik(xi − µk). (18.3)
</p>
<p>Setting this equal to zero and solving we obtain:
</p>
<p>µk =
</p>
<p>∑N
i=1 zikxi∑N
i=1 zik
</p>
<p>. (18.4)
</p>
<p>However, the nominator is simply the sum of those observations assigned to cluster k, and the
denominator is simply the number of observations assigned to k, so the expression is simply the
mean of the observations assigned to cluster k (notice, the update for the µk depends on the
distance measure and a change in distance measures may also lead to a change in the updates for
the cluster locations). We can then see the two steps in the K-means algorithm simply corresponds
to minimizing E with respect to zik or µk respectively while keeping the other quantity fixed. This
is also why the K-means algorithm converges; since each step makes the error E smaller, and E ≥ 0,
the algorithm must converge.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>270 18 Distance-based clustering techniques
</p>
<p>K
</p>
<p>S
u
m
-o
f-
sq
u
a
re
s
er
ro
r
E
</p>
<p>1 2 3 4 5 6
</p>
<p>0
</p>
<p>100
</p>
<p>200
</p>
<p>300
</p>
<p>400
</p>
<p>500
</p>
<p>600
</p>
<p>Fig. 18.5: Value of the sum-of-squares error function E in the converged state for the 2D dataset
shown in fig. 18.3 for different values of K. The error decreases when K is increased, however, a
suitable choice of K can potentially be found as where the drop in error levels off. In this case
K = 4, which visually also seems to be an appropriate choice.
</p>
<p>18.2.2 Practical issues with the K-means algorithm
</p>
<p>The K-means algorithm is a very simple and efficient clustering algorithm, however, it has some
drawbacks. Firstly, since we rely on the Euclidian distance, it prefers clusters that are “round” and
of roughly equal size. For this reason, it may be affected both by outliers but also by simple scaling
of one coordinate while keeping the others fixed so when applying the K-means algorithm it is
recommended to consider standardizing the data. Secondly, while the K-means algorithm converges
quickly, what clustering it converges to depends on how it was initialized. For this reason, it is
often useful to consider a particular strategy when initializing the K-means algorithm and consider
several restarts with different initialization. One popular (and simple) choice of initialization is the
farthest-first procedure Gonzalez [1985] according to which for k = 1, . . . ,K we initialize µk by:
</p>
<p>• Randomly assign one of the observations to be the location of the first cluster center, i.e. µ1.
• Initialize each subsequent µk as the observation xi which is the farthest away from the cluster
</p>
<p>it is currently assigned as being closest to of µ1, . . . ,µk−1.
</p>
<p>This initialization ensures the locations µk are well spread-out over the dataset and often gives
much faster convergence and better final positions.
</p>
<p>Thirdly, during the K-means algorithm, it is possible that a cluster µk has no observations
assigned to it. In this case one can either remove the cluster, let it stay at it’s current location or
assign µk to the observation which is the furthest away from it’s closest mean cluster location.
</p>
<p>Finally, K-means requires us to choose a suitable K. This is a difficult problem and there is
no single agreed-upon solution. One heuristic procedure is to run the K-means algorithm using
different choices of K and consider the K where the drop in error levels off. This is done for the
dataset in fig. 18.4 for K = 1, . . . , 6 and the sum-of-squares error can be seen in fig. 18.5, and the
figure suggests K = 4 where the drop in error levels off.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.3 Hierarchical agglomerative clustering 271
</p>
<p>Fig. 18.6: Hierarchical agglomerative clustering applied to the 2D dataset shown in the top right.
Each point is assigned to a singleton cluster (top middle), and the closest clusters are then merged
until all clusters have been merged. The dendrogram illustrates which clusters are merged in each
step and the height of each added clamp is the distance of the two merged clusters.
</p>
<p>18.3 Hierarchical agglomerative clustering
</p>
<p>A difficulty in K-means clustering was the requirement of finding a single agreed-upon value K.
Hierarchical agglomerative clustering overcomes this limitation by instead of finding a single K
arranging the data in a nested sequence of partitions organized as a hierarchy. The bottom of the
hierarchy correspond to the finest partition (each observation is in a unique (singleton) cluster)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>272 18 Distance-based clustering techniques
</p>
<p>Fig. 18.7: By cutting the dendrogram at different heights, a different number of clusters can be
obtained. As we will see, the shape of the dendrogram can be used as an indication of an appropriate
cut-height.
</p>
<p>whereas the top-level of the hierarchy corresponds to the coarsest possible partition corresponding
to putting every observation in the same cluster.
</p>
<p>Once again it is easier to show what hierarchical agglomerative clustering does than start with a
mathematical definition. Recall K-means required a measure of distance between observations (the
Euclidian distance). Hierarchical agglomerative clustering requires a measure of distance between
groups of observations. We will later show natural examples, however, for now assume we are given
such a measure. Hierarchical agglomerative clustering is then illustrated in fig. 18.6 when it is
applied to the dataset shown in the upper-left pane consisting of N = 11 observations and proceeds
by the following steps:
</p>
<p>• Start by placing each observation in a separate group to provide the coarsest possible partition
(top-middle pane). This correspond to the bottom-layer of the hierarchy shown as an insert.
</p>
<p>• Iteratively merge the two closest clusters. In the hierarchy, this is indicated by drawing a
“clamp” between the corresponding clusters. The y-location of the vertical bar in the clamp
corresponds to the distance of the two clusters.
</p>
<p>• Repeat until all observations are merged into a single cluster.
The hierarchy which is constructed is known as a dendrogram. The dendrogram is tree-structured
and by construction corresponds to a nested sequence of partitions. Since the y-location of the
vertical bars where clusters are merged indicate their location, the dendrogram can give a visual
summary of both the algorithm and the data and is part of why hierarchical clustering is popular.
Notice, the hierarchical agglomerative clustering algorithm is deterministic and converges in N − 1
steps; to obtain a particular clustering one can cut the dendrogram at a given height, see fig. 18.7,
and often visual inspection of the dendrogram (in particular where vertical lines are long) can give
a visual indication of what corresponds to a good cut, this will be indicated in a moment.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.3 Hierarchical agglomerative clustering 273
</p>
<p>Minimum Linkage Maximum Linkage Average Linkage
</p>
<p>min
x∈C1,y∈C2
</p>
<p>d(x,y) max
x∈C1,y∈C2
</p>
<p>d(x,y)
∑
</p>
<p>x∈C1,y∈C2
</p>
<p>d(x,y)
</p>
<p>|C1||C2|
</p>
<p>Fig. 18.8: Illustration of the three most popular linkage function, maximum (complete) linkage,
minimum (single) linkage and average linkage.
</p>
<p>18.3.1 Selecting linkage function
</p>
<p>Recall in hierarchical agglomerative clustering, we merged the closest clusters in each step. This
requires a distance function between groups of observations. If we assume we have a distance function
between individual observations, for instance just the Euclidian distance d(xi,xj) = ‖xi − xj‖2,
we can define such a distance function in three ways indicated in fig. 18.8. Consider two groups C1
and C2 of observations we can then define the three linkage functions as:
</p>
<p>Minimum (or single) linkage Here the distance between the groups is the distance between the
closest pair of observations
</p>
<p>d(C1, C2) = min
x∈C1,y∈C2
</p>
<p>d(x,y). (18.5)
</p>
<p>Maximum (or complete) linkage Here the distance between the groups is the distance between
the most distant pair of observations
</p>
<p>d(C1, C2) = max
x∈C1,y∈C2
</p>
<p>d(x,y). (18.6)
</p>
<p>Average linkage Here the distance between the groups is the average distance between all pairs
in the two groups
</p>
<p>d(C1, C2) =
</p>
<p>∑
x∈C1,y∈C2 d(x,y)
</p>
<p>|C1||C2|
, (18.7)
</p>
<p>where |C1| and |C2| is the number of observations in the two groups.
</p>
<p>Ward’s method
</p>
<p>Another popular choice of linkage function is Ward’s method (or simply Ward linkage) which is
inspired by the K-means algorithm. Suppose at a given step of the clustering algorithm there are
K clusters. We then compute the K centroid vectors µ1, · · · ,µK as the mean of each cluster and
compute the K-means error already introduced in eq. (18.1):
</p>
<p>E =
</p>
<p>N∑
i=1
</p>
<p>K∑
k=1
</p>
<p>zik‖xi − µk‖22.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>274 18 Distance-based clustering techniques
</p>
<p>Ward’s Method
</p>
<p>Fig. 18.9: Ward’s method for linkage. At each step, the sum-of-squares error of the distance from
each observation to its cluster center is computed, and the clusters, which provides the smallest
increase in the error, is merged.
</p>
<p>The two clusters whose merger provides the smallest increase in the above error are then merged,
see also fig. 18.9.
</p>
<p>Thus, hierarchical agglomerative clustering requires that we select the linkage function from the
outset. The choice of linkage function has an important effect on the type of clusters hierarchical
agglomerative clustering is good at finding and, correspondingly, what type of clustering hierarchical
agglomerative clustering is unsuited for identifying. We will illustrate this with three examples.
</p>
<p>In fig. 18.10 we consider a dataset consisting of two half-moons. We apply hierarchical agglomer-
ative clustering, cut the dendrogram at a height corresponding to two clusters, and color the dataset
accordingly. Each row shows a linkage function, at the top maximum linkage, in the middle average
linkage and at the bottom minimum linkage. As indicated, both average and maximum linkage
cannot find the right clusters, whereas minimum linkage does. Why? Minimum linkage (bottom)
only cares about the nearest set of observations, thus, it will chain together the two moons since
each point in any of the moons is closer to another point in the same moon. On the other hand,
maximum linkage (top) cares about the furthest distance. Thus, it favors clusters which are round
and very compact. Average linkage is somewhere in between the two methods and produce clusters
which are (roughly) comparable to K-means. If we notice the dendrograms, we can see that (visu-
ally) the two clusters in the single-linkage (bottom) case stands out, whereas for complete linkage
(top) four clusters might be more appropriate. Notice also the relative scale of the dendrogram
y-axis. As expected, single linkage merge everything at a much lower height than complete linkage.
</p>
<p>This also gives an indication of where minimum linkage may get into problems. Since minimum
linkage only cares about the closest pairs of observations, if there is a chain of observations between
two clusters minimum linkage will use these to chain together the two clusters. This is indicated in
fig. 18.11. In the top-row, complete linkage (which is focused on compactness) finds the four clusters,
whereas in the bottom-row, single linkage fails as there is a slight “chain” of points merging the two
right-most clusters. Furthermore, a small group in the bottom-right is slightly further away from
the other clusters and is assigned by single linkage its own cluster at this level of the dendrogram.
Notice in addition, that the dendrogram for the complete linkage function quite clearly indicates
there are four clusters in the dataset (the large vertical gap) whereas for the single linkage function
the picture is not so clear.
</p>
<p>Finally, consider the dataset comprised of two differently-sized clusters shown in fig. 18.12.
For clusters of different size, complete linkage fails because complete linkage, when for instance
determining where a point in the middle belongs, cares about the distance to the edges of the two
point-clouds. Thus, it will try to roughly divide the point-clouds along the middle which in this</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.3 Hierarchical agglomerative clustering 275
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>12
</p>
<p>14
</p>
<p>16
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>Fig. 18.10: Each row corresponds to hierarchical agglomerative clustering applied to the 2D dataset
with different linkage functions. The choices are maximum linkage, average linkage, and minimum
linkage. The colors indicate a cut-off corresponding to two clusters. Notice, only minimum linkage
solves the problem due to its ability to chain together nearby clusters favoring connected compo-
nents. Notice also the qualitative difference of the three dendrograms.
</p>
<p>case is wrong. Single-linkage on the other hand is ideally suited because there are no outliers and
a clear gap between the two point-clouds, this is also indicated by the dendrograms.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>276 18 Distance-based clustering techniques
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>2.5
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>Fig. 18.11: Hierarchical agglomerative clustering applied to 2D dataset with complete/maximum
(top) and single/minimum (bottom) linkage. Notice, single linkage is confused by the observations
lying between the two right-most clusters, and the outliers, complete linkage is more robust and
produce more compact clusters.
</p>
<p>In conclusion, complete linkage works well when all clusters are roughly round, of equal size or
there are outliers in the dataset. It fails when clusters have very different size, are shaped oddly or
they are defined by being connected.
</p>
<p>Single linkage on the other hand works well for the case where the clusters are internally con-
nected and separated by gaps. It fails when there is outliers in the data or the dataset is otherwise
very noisy.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.4 Comparing partitions 277
</p>
<p>0
</p>
<p>0.5
</p>
<p>1
</p>
<p>1.5
</p>
<p>2
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>Fig. 18.12: Hierarchical agglomerative clustering applied to 2D dataset with maximum (top) and
minimum (bottom) linkage. In this case maximum linkage (top row) tries to produce compact
clusters of roughly equal size and thereby incorrectly mixes up the two blobs. Minimum linkage
easily solves the problem since the two clusters are spatially separated.
</p>
<p>18.4 Comparing partitions
</p>
<p>How do we evaluate a partition-based model? We have previously considered this question in a loose
fashion when we discussed how to select K in the K-means algorithm, however, without a definite
recommendation. It is plausibly the case there is no definite way to evaluate a clustering. After
all, different people might have different preferences. Consider for instance how different people
might group the set of all animals, some may group them according to their utility (pets, domestic
animals, dangerous animals, etc.) whereas others might cluster them based on their species and
others again based on their behaviour (flying, swimming, crawling, burrowing). In this section, we</p>
<p></p>
</div>, <div class="page"><p></p>
<p>278 18 Distance-based clustering techniques
</p>
<p>QZ
</p>
<p>Fig. 18.13: A dataset of N = 9 observations are clustered into two partitions Z and Q indicated by
the colors. In the case of Z there are K = 3 clusters and for Q there are M = 4 clusters.
</p>
<p>will not attempt to cut this Gordian knot, but rather suppose we have access to side information
(i.e. a true clustering) and determine how we might use this to evaluate our clustering method. The
first step in any such procedure is to consider how different two clusterings are. This is a necessary
component to any supervised evaluation of a clustering method and so this will be our focus of this
section: To produce a proper measure of the difference between a clustering Z and Q. We will use
the running example in fig. 18.13 where the N = 9 observations are clustered into two partitions Z
and Q indicated by the colors.
</p>
<p>The example illustrates two problems when comparing partitions. Firstly, that the number of
clusters in the two clusterings may be different (consider for instance one clustering obtained by the
K-means algorithm with K = 4 clusters compared to a true clustering with 3 clusters) and secondly
that we have to figure out which cluster in one partition corresponds to which cluster in the other
partition. Suppose the observations are labeled by i = 1, · · · , N and the cluster assignments for
partition Z is z1, · · · , zN such that zi = k means observation i is in cluster k. Similarly we denote
q1, · · · , qN as the cluster assignments for Q. We will denote the total number of clusters in Z and
Q as K and M respectively.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.4 Comparing partitions 279
</p>
<p>Example 18.4.1: Encoding partitions
</p>
<p>To completely de-mystify the notation, consider the two partition-example in fig. 18.13.
Suppose we label the colors 1 (yellow), 2 (blue), 3 (red) and 4 (green), we then have:
</p>
<p>Z =
[
1 1 1 1 2 2 3 3 3
</p>
<p>]
Q =
</p>
<p>[
4 4 1 1 2 2 2 3 3
</p>
<p>]
An in this case, K = 3 and M = 4. The particular numbers would refer to the ground-truth
class or partition number as obtained by a clustering method. Note whatever method we
use to compare partitions should be invariant to labeling, meaning that comparing Z and Q
should yield the same result as comparing Z and Q′ defined as
</p>
<p>Q′ =
[
10 10 3 3 8 8 8 1 1
</p>
<p>]
It is, however, convenient to assume the clusters are labeled successively 1, 2, 3, . . . , that is,
the highest value in Z is K and the largest value in Q is M .
</p>
<p>Before continuing, we will introduce a few results which can be defined purely from Z and Q.
First, recall the delta function is defined as
</p>
<p>δhk =
</p>
<p>{
1 if h = k
</p>
<p>0 otherwise
</p>
<p>Therefore, the number of observations in Z which belongs to cluster k can be computed as
</p>
<p>{Number of observations in cluster k in Z} =
N∑
i=1
</p>
<p>δzi,k
</p>
<p>More fundamentally, we will define the joint count matrix n as the K ×M matrix defined as:
</p>
<p>nkm = {Number of observations assigned to cluster k in Z and m in Q} (18.8)
</p>
<p>=
</p>
<p>N∑
i=1
</p>
<p>N∑
j=1
</p>
<p>δzi,kδzj ,m (18.9)
</p>
<p>Based on this matrix, we can count the number of observations assigned to cluster k in Z (and
similarly, m in Q) as:
</p>
<p>nZ = {Number of observations assigned to cluster k in Z} =
M∑
m=1
</p>
<p>nkm (18.10)
</p>
<p>nQ = {Number of observations assigned to cluster m in Q} =
K∑
k=1
</p>
<p>nkm (18.11)
</p>
<p>In the following, all measures we introduce will be expressed using the joint count matrix n. That
is, if we wish to compute two partitions, this matrix should be what we compute first.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>280 18 Distance-based clustering techniques
</p>
<p>Finally, a surprise counting exercise which will prove very useful. Suppose we wish to count the
possible (distinct) pairs we can make out of n observations. To count this, the first observation can
be paired to all n− 1 other observations, the second to all n− 2 (but excluding the first, as we have
counted this pair), the third can be paired to n− 3 and so on. All in all:
</p>
<p>{Distinct pairs of n observations} = (n− 1) + (n− 2) + · · ·+ 2 + 1 + 0
</p>
<p>=
n(n− 1)
</p>
<p>2
(18.12)
</p>
<p>Example 18.4.2: Counting matrix, continued
</p>
<p>To continue the example in fig. 18.13, for instance n14 = 2 as observations 1 and 2 are
assigned to cluster 1 (yellow) and 4 (green) in Z and Q respectively. Generally, the reader
is encouraged to verify:
</p>
<p>n =
</p>
<p>2 0 0 20 2 0 0
0 1 2 0
</p>
<p>
Note the horizontal/vertical sums of n:
</p>
<p>nZ =
</p>
<p>42
3
</p>
<p> , nQ = [2 3 2 2]
Agree with the number of observations assigned to each cluster.
Finally, we can test our counting result. The number of distinct pairs of blue balls in Z are
2× (2− 1)× 12 = 1 (which is true, because one unique pair can be made between two balls)
and for the yellow balls: 4× (4−1)× 12 = 6, which the reader can verify by counting. A very
patient reader can verify the total number of pairs is N(N−1)2 = 9× (9− 1)× 12 = 36.
</p>
<p>18.4.1 Rand index
</p>
<p>Consider two distinct observations i, j. To say that partition Z is similar to Q is to say that when i
and j are placed in the same cluster in partition Z, they will also most often be together in partition
Q and vice versa.
</p>
<p>To make this more concrete, i, j are both in the same cluster in Z and Q if and only if δzizj = 1
and δqiqj = 1. Therefore, both partition Z and Q agree that i, j are in the same cluster only if
</p>
<p>1 = δzizjδqiqj = Sij ,
</p>
<p>(which is otherwise 0). Similarly, both partition Z and Q agree that i, j are not in the same cluster
only if
</p>
<p>1 = (1− δzizj )(1− δqiqj ) = Dij .
We can then count the total number of times Z and Q agrees two observations are or are not in
the same cluster by taking the sum over all distinct observations i, j:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.4 Comparing partitions 281
</p>
<p>QZ
</p>
<p>Fig. 18.14: Counting the pairs of observations contributing to S, i.e., pairs of observations the two
partitions agree are in the same clusters.
</p>
<p>D =
</p>
<p>N−1∑
i=1
</p>
<p>N∑
j=i+1
</p>
<p>Dij and S =
</p>
<p>N−1∑
i=1
</p>
<p>N∑
j=i+1
</p>
<p>Sij
</p>
<p>There are a total of 12N(N − 1) distinct pairs of observations to compare, and so we can define the
Rand similarity between Z and Q as the relative number of times Z and Q agree on the assignment
of observations:
</p>
<p>R(Q,P ) =
S +D
</p>
<p>1
2N(N − 1)
</p>
<p>. (18.13)
</p>
<p>Notice that the way the Rand index both counts matches and non-matches (S and D) makes it
somewhat comparable to the SMC.
</p>
<p>Expressing the Rand index using the counting matrix
</p>
<p>We can re-express the Rand index using the counting matrix. First, focus on S, pairs of observations
assigned to the same cluster in both partitions. Each number nkm represent observations assigned
</p>
<p>to cluster k in Z and m in Q. We can form nkm(nkm−1)2 distinct pairs of these and therefore we can
conclude:
</p>
<p>S =
</p>
<p>K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>nkm(nkm − 1)
2
</p>
<p>.
</p>
<p>K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>nkm(nkm − 1)
2
</p>
<p>.
</p>
<p>To find D, we compute:
</p>
<p>D =
</p>
<p>N−1∑
i=1
</p>
<p>N∑
j=i+1
</p>
<p>(1− δzizj )(1− δqiqj ) (18.14)
</p>
<p>=
</p>
<p>N−1∑
i=1
</p>
<p>N∑
j=i+1
</p>
<p>1
</p>
<p>−
N−1∑
i=1
</p>
<p>N∑
j=i+1
</p>
<p>δzizj
</p>
<p>−
N−1∑
i=1
</p>
<p>N∑
j=i+1
</p>
<p>δqiqj
</p>
<p>+
N−1∑
i=1
</p>
<p>N∑
j=i+1
</p>
<p>δzizjδqiqj
</p>
<p> (18.15)
=
</p>
<p>[
N(N − 1)
</p>
<p>2
</p>
<p>]
−
[
Pairs of observations
in same cluster in Z
</p>
<p>]
−
[
Pairs of observations
in same cluster in Q
</p>
<p>]
+ [S] (18.16)
</p>
<p>=
N(N − 1)
</p>
<p>2
−
</p>
<p>K∑
k=1
</p>
<p>nZk (n
Z
k − 1)
2
</p>
<p>−
M∑
m=1
</p>
<p>nQm(n
Q
m − 1)
2
</p>
<p>+ S (18.17)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>282 18 Distance-based clustering techniques
</p>
<p>QZ
</p>
<p>Fig. 18.15: Counting the pairs of observations contributing to D, pairs of observations the two
partitions agree are in different clusters (here only shown for partition Z for simplicity).
</p>
<p>Notice, R(Q,Q) = R(Z,Z) = 1 and in general 0 ≤ R(Q,P ) ≤ 1. In fig. 18.13, if we focus on
the Q-partition, the green, yellow and red observations are all in the same blocks in the Z-partition
as is one pair of blue observations (see fig. 18.14). Similarly we can count the pairs of observation
both partitions agree are not in the same cluster which is illustrated in fig. 18.15. For more details
see example 18.4.3
</p>
<p>Example 18.4.3: Rand index, continued
</p>
<p>To get the Rand index from Z and Q and the counting matrix, we first compute S to be
</p>
<p>S =
2(2− 1)
</p>
<p>2
+
</p>
<p>2(2− 1)
2
</p>
<p>+
2(2− 1)
</p>
<p>2
+
</p>
<p>1(1− 1)
2
</p>
<p>+
2(2− 1)
</p>
<p>2
= 4
</p>
<p>Next, to compute D, we compute the two terms involving nZ and nZ to be:
</p>
<p>K∑
k=1
</p>
<p>nZk (n
Z
k − 1)
2
</p>
<p>= 6 + 1 + 3 = 10 (18.18)
</p>
<p>M∑
m=1
</p>
<p>nQm(n
Q
m − 1)
2
</p>
<p>= 1 + 3 + 1 + 1 = 6 (18.19)
</p>
<p>This allow us to compute
D = 36− 10− 6 + 4 = 24
</p>
<p>Finally, we obtain a Rand index of:
</p>
<p>R(Z,Q) =
4 + 24
1
28 · 9
</p>
<p>=
7
</p>
<p>9
.
</p>
<p>18.4.2 Jaccard similarity
</p>
<p>A problem with the Rand index is that if there are many clusters, there will typically be many more
pairs of observations in different clusters than in the same cluster and so in general we can expect</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.4 Comparing partitions 283
</p>
<p>D � S which means the Rand index is often close to 1. The reader might notice this problem, and
indeed the definition of the Rand index, is very similar to the definition of the simple matching
coefficient where we also counted the number of times two vectors agreed on the negative and
positive matches. We can therefore considered the Jaccard similarity where we disregard the trivial
00 matches:
</p>
<p>J(Q,P ) =
S
</p>
<p>1
2N(N − 1)−D
</p>
<p>(18.20)
</p>
<p>Notice it is still the case that 0 ≤ J(Q,P ) ≤ 1.
</p>
<p>Example 18.4.4: Jaccard similarity
</p>
<p>We can easily compute the Jaccard similarity as all quantities are known. We get:
</p>
<p>J(Q,P ) =
4
</p>
<p>1
29 · 8− 24
</p>
<p>=
1
</p>
<p>3
.
</p>
<p>18.4.3 Comparing partitions using normalized mutual information
</p>
<p>Our third measure of cluster similarity is based on the normalized mutual information. It is similar
to Jaccard similarity and Rand index but theoretically better motivated. Normalized mutual infor-
mation is based on the idea of quantifying how much information one partition provides about the
other partition. Recall from our earlier discussion of information theory in section 5.5 all we have
to specify to compute the mutual information is a joint distribution pkm(k,m) of two variables k
and m, and then we can compute the mutual information mechanically (see method 5.5.1). The
two events we are interested in is simply that an observation is assigned to a given cluster, and the
joint density corresponds to the event a particular observation is assigned to one cluster in k in Z
and at the same time m in Q. In other words, we simply define
</p>
<p>pkm(k,m) =
nkm
N
</p>
<p>where nkm is the familiar counting matrix. From here, it is all a matter of standard definitions,
which have been re-produced in method 18.4.1 for ease</p>
<p></p>
</div>, <div class="page"><p></p>
<p>284 18 Distance-based clustering techniques
</p>
<p>Method 18.4.1: Information theory
</p>
<p>We wish to compare the mutual information of two clusters assignments Z and Q. To do
this, we define the probability an observation is assigned in k and m as:
</p>
<p>pkm(k,m) =
nkm
N
</p>
<p>, for k = 1, . . . ,K and m = 1, . . . ,M
</p>
<p>Based on this matrix, we can define the marginal distributions as the K and M -dimensional
vectors:
</p>
<p>pk(k) =
</p>
<p>M∑
m=1
</p>
<p>pkm(k,m), pm(m) =
</p>
<p>K∑
k=1
</p>
<p>pkm(k,m)
</p>
<p>The Entropy in the 1 and 2d-case is then defines as:
</p>
<p>H[Z] ≡ H[pk] = −
K∑
k=1
</p>
<p>pk(k) log pk(k). H[ZQ] ≡ H[pkm] = −
K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>pkm(k,m) log pkm(k,m).
</p>
<p>In both cases, it measures the complexity of pk and pkm in bits. In addition, the mutual
information and normalized mutual information is defined as:
</p>
<p>MI[Z,Q] = MI[pkm] = H[Z] +H[Q]−H[ZQ]
</p>
<p>NMI[Z,Q] = NMI[pkm] =
MI[Z,Q]√
H[Z]
</p>
<p>√
H[Q]
</p>
<p>.
</p>
<p>Where the NMI[Z,Q] is understood as measuring the overlap of the two partitions.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.4 Comparing partitions 285
</p>
<p>Example 18.4.5: Mutual information, example
</p>
<p>To continue our example fig. 18.13 we can compute the entropy of each partition as:
</p>
<p>Entropy of Z: H[Z] = −4
9
</p>
<p>log
4
</p>
<p>9
− 1
</p>
<p>3
log
</p>
<p>1
</p>
<p>3
− 2
</p>
<p>9
log
</p>
<p>2
</p>
<p>9
≈ 1.06
</p>
<p>Entropy of Q: H[Q] = −2
9
</p>
<p>log
2
</p>
<p>9
− 2
</p>
<p>9
log
</p>
<p>2
</p>
<p>9
− 2
</p>
<p>9
log
</p>
<p>2
</p>
<p>9
− 1
</p>
<p>3
log
</p>
<p>1
</p>
<p>3
≈ 1.37.
</p>
<p>Similarly, the entropy of both partitions is:
</p>
<p>H[pZQ] = H[ZQ] = −
K∑
k=1
</p>
<p>M∑
m=1
</p>
<p>pZQ(k,m) log pZQ(k,m)
</p>
<p>= −4× 2
9
</p>
<p>log
2
</p>
<p>9
− 1
</p>
<p>9
log
</p>
<p>1
</p>
<p>9
= 1.58.
</p>
<p>From this, we can easily compute the Mutual information and Normalized mutual informa-
tion:
</p>
<p>MI[Z,Q] = H[Z] +H[Q]−H[Z,Q] ≈ 1.06 + 1.37− 1.58 ≈ 0.85.
and
</p>
<p>NMI[Z,Q] =
MI[Z,Q]√
H[Z]
</p>
<p>√
H[Q]
</p>
<p>≈ 0.85√
1.06
√
</p>
<p>1.37
≈ 0.70.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>286 18 Distance-based clustering techniques
</p>
<p>Problems
</p>
<p>18.1. Fall 2013 question 8: In Table 18.1 is given
the pairwise distances between the four smallest and four
largest islands in the Galápagos data. A hierarchical clus-
tering is used to cluster these eight observations using
single (i.e., minimum) linkage. Which one of the dendro-
grams given in Figure 18.16 corresponds to the cluster-
ing?
</p>
<p>O1 O2 O3 O4 O5 O6 O7 O8
</p>
<p>O1 0 2.39 1.73 0.96 3.46 4.07 4.27 5.11
O2 2.39 0 1.15 1.76 2.66 5.36 3.54 4.79
O3 1.73 1.15 0 1.52 3.01 4.66 3.77 4.90
O4 0.96 1.76 1.52 0 2.84 4.25 3.80 4.74
O5 3.46 2.66 3.01 2.84 0 4.88 1.41 2.96
O6 4.07 5.36 4.66 4.25 4.88 0 5.47 5.16
O7 4.27 3.54 3.77 3.80 1.41 5.47 0 2.88
O8 5.11 4.79 4.90 4.74 2.96 5.16 2.88 0
</p>
<p>Table 18.1: Pairwise Euclidean distance, i.e d(Oa,Ob) =
||xa − xb||2 =
</p>
<p>√∑
m(xam − xbm)2, between eight ob-
</p>
<p>servations of the Galápagos data. Red observations (i.e.,
O1, O2, O3, and O4) correspond to the four smallest is-
lands whereas blue observations (i.e., O5, O6, O7, and
O8) correspond to the four largest islands.
</p>
<p>Fig. 18.16: Hierarchical clustering of the eight observa-
tions considered in Table 18.1.
</p>
<p>A Dendrogram 1.
B Dendrogram 2.
C Dendrogram 3.
D Dendrogram 4.
E Don’t know.
</p>
<p>18.2. Fall 2014 question 20: Consider the simple 1-
dimensional data set comprised of N = 7 observations as
shown in table 18.2. Suppose we wish to apply K-means
clustering to the dataset and the K = 3 one-dimensional
cluster centers are initialized in µ1 = 4, µ2 = 7 and
µ3 = 14. After terminating of the K-means clustering
algorithm, what are the final (rounded) cluster centers
µ1, µ2, µ3?
</p>
<p>X 3 6 7 9 10 11 14
</p>
<p>Table 18.2: Simple 1-dimensional dataset comprised of
N = 7 observations.
</p>
<p>A µ1 = 3.00, µ2 = 8.00, µ3 = 12.50
B µ1 = 3.00, µ2 = 7.33, µ3 = 11.67
C µ1 = 4.50, µ2 = 9.25, µ3 = 14.00
D µ1 = 5.33, µ2 = 10.00, µ3 = 14.00
E Don’t know.
</p>
<p>18.3. Fall 2014 question 11: In table 18.3 is given
the pairwise cityblock distances between 8 observations.
A hierarchical clustering is used to cluster these nine ob-
servations using group average linkage. Which of the den-
drograms shown in fig. 18.17 corresponds to the cluster-
ing?
</p>
<p>Dendrogram 4Dendrogram 3
</p>
<p>Dendrogram 2Dendrogram 1
</p>
<p>o2 o6 o1 o3 o7 o5 o8 o4o2 o6 o1 o3 o7 o5 o4 o8
</p>
<p>o3 o6 o1 o2 o7 o5 o4 o8o3 o6 o1 o2 o7 o5 o8 o4
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>8
</p>
<p>10
</p>
<p>Fig. 18.17: Hierarchical clustering of the 8 observations
considereded in table 18.3</p>
<p></p>
</div>, <div class="page"><p></p>
<p>18.4 Comparing partitions 287
</p>
<p>o1 o2 o3 o4 o5 o6 o7 o8
o1 0 4 7 9 5 5 5 6
o2 4 0 7 7 7 3 7 8
o3 7 7 0 10 6 6 4 9
o4 9 7 10 0 8 6 10 9
o5 5 7 6 8 0 8 6 7
o6 5 3 6 6 8 0 8 11
o7 5 7 4 10 6 8 0 7
o8 6 8 9 9 7 11 7 0
</p>
<p>Table 18.3: Pairwise Cityblock distance, i.e d(oi, oi) =
</p>
<p>‖xi − xj‖1 =
∑M
k=1 |xik − xjk|, between 8 observations.
</p>
<p>Each observation oi corresponds to a M = 15 dimen-
sional binary vector, xik ∈ {0, 1}. The blue observations
{o1, o2, o3, o4} belong to class C1 and the black observa-
tions {o5, o6, o7, o8} belong to class C2.
</p>
<p>A Dendrogram 1.
B Dendrogram 2.
C Dendrogram 3.
D Dendrogram 4.
E Don’t know.
</p>
<p>18.4. Fall 2012 question 12: Figure 18.18 contains
four dendrograms generated according to the distance
matrix given in Table 18.4. By thresholding dendrogram
2 we obtain the following clusters
</p>
<p>Cluster 1: A4, B1, B2, B3
Cluster 2: A1, B4
Cluster 3: A2
Cluster 4: A3
</p>
<p>Let mij denote the number of observations of class j in
cluster i, mi =
</p>
<p>∑
jmij denote the number of observa-
</p>
<p>tions in cluster i, and m =
∑
imi denote the total num-
</p>
<p>ber of observations. Let further pij =
mij
mi
</p>
<p>denote the
probability that a member of cluster i belongs to class
j. The purity of cluster i is given as pi = maxj pij and
the overall purity of a clustering is given as purity =∑K
i=1
</p>
<p>mi
m pi. Let the class an observation belongs to be
</p>
<p>defined in terms of whether the person considered has a
liver disease (i.e., B1, B2, B3 and B4) or not (i.e., A1, A2,
A3, and A4). What is the purity of the above clustering?
</p>
<p>Fig. 18.18: Four dendrograms generated according to the
distance matrix given in Table 18.4.
</p>
<p>A1 A2 A3 A4 B1 B2 B3 B4
</p>
<p>A1 0 3.33 3.73 5.06 4.05 3.76 4.79 2.63
A2 3.33 0 4.77 4.68 3.89 3.72 3.59 3.28
A3 3.73 4.77 0 3.67 3.93 3.86 5.15 4.35
A4 5.06 4.68 3.67 0 1.52 3.64 3.73 3.73
B1 4.05 3.89 3.93 1.52 0 3.21 3.21 2.45
B2 3.76 3.72 3.86 3.64 3.21 0 2.54 3.94
B3 4.79 3.59 5.15 3.73 3.21 2.54 0 4.44
B4 2.63 3.28 4.35 3.73 2.45 3.94 4.44 0
</p>
<p>Table 18.4: Euclidean distances between four selected
subjects without a liver disease (denoted A1, A2, A3,
and A4) and four selected subjects with a liver disease
(denoted B1, B2, B3 and B4).
</p>
<p>A purity = 18
B purity = 12
C purity = 23
D purity = 34
E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>19
</p>
<p>Mixture models for unsupervised clustering
</p>
<p>The goal of density estimation is to describe the probability distribution a given set of observation
X have originated from. Learning probability distributions is relevant in a number of contexts.
Consider for instance a standard application of Bayes’ theorem
</p>
<p>p(y = c|x) = p(x|y = c)p(y = c)∑C
c′=1 p(x|y = c′)p(y = c′)
</p>
<p>Applying this to a practical problem involves estimating the C densities p(x|y). However represent-
ing the density can be useful in many other contexts, for instance if we estimate the density of all
credit card transactions, a credit card transaction x having low value p(x) is then equivalent to an
unusual credit card transaction which may warrant further investigation. In this chapter we will
focus on probabilistic estimation of densities using the Gaussian mixture-model and a particular
simple way to train the Gaussian mixture-model known as the Expectation maximization (EM)
algorithm.
</p>
<p>Mixture models were first considered around the middle of the 19th century and their explicit
statement is usually attributed to the biostatistician Karl Pearson who used mixture models to
analyse the length of crabs [Pearson, 1894]. The EM algorithm was first named by Dempster et al.
[1977], however, ideas reminiscent of the EM algorithm has been used in different contexts before
this.
</p>
<p>19.1 The Gaussian mixture model
</p>
<p>The goal of the Gaussian mixture-model (GMM) is to derive a distribution for an M -dimensional
vector x ∈ RM which we will write as p(x). We wish this distribution to be potentially very flexible
and a common strategy for obtaining this in a tractable manner is to make p(x) be a combination of
simpler, more tractable elements. First recall the definition of the multivariate normal distribution
introduced earlier in chapter 13. The multivariate normal distribution for an M -dimensional vector
is defined as the density:
</p>
<p>N (x|µ,Σ) = 1√
(2π)d|Σ|
</p>
<p>e−
1
2 (x−µ)
</p>
<p>TΣ−1(x−µ),</p>
<p></p>
</div>, <div class="page"><p></p>
<p>290 19 Mixture models for unsupervised clustering
</p>
<p>x
</p>
<p>y
</p>
<p>P
ro
b
a
b
il
it
y
D
en
si
ty
</p>
<p>−3
−2
</p>
<p>−1
0
</p>
<p>1
2
</p>
<p>3
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>0.02
</p>
<p>0.04
</p>
<p>0.06
</p>
<p>0.08
</p>
<p>0.1
</p>
<p>0.12
</p>
<p>0.14
</p>
<p>x
</p>
<p>y
</p>
<p>−2 0 2
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>Fig. 19.1: Example of the probability density function of a 2-dimensional multivariate normal dis-
tribution. In left it is plotted as a function of x = [x y]T , i.e. N (x|µ,Σ), whereas on the right the
same distribution is shown as a contour plot.
</p>
<p>where µ ∈ RM is the mean and Σ is the M ×M covariance matrix. An example is given in fig. 19.1
corresponding to the multivariate normal distribution
</p>
<p>Σ =
</p>
<p>[
1 0.8
</p>
<p>0.8 1
</p>
<p>]
and µ =
</p>
<p>[
0
0
</p>
<p>]
.
</p>
<p>In the Gaussian mixture-model (GMM) we want to use the multivariate normal distribution as
a building block to create a more flexible distribution. Suppose K is an integer for instance K = 4.
Let’s imagine we select an integer 1, . . . ,K at random and we denote the event we selected k with
the binary variable zk = 1 and the event we did not select k as zk = 0. For instance
</p>
<p>z =
[
0 1 0 0
</p>
<p>]T
,
</p>
<p>is the event we selected k = 2. Then z is a binary vector where only one entry can be non-zero at
a time. Suppose the probability we select k is πk:
</p>
<p>P (We select option k) = πk,
</p>
<p>then a little thought reveals that
</p>
<p>p(z) =
</p>
<p>K∏
k=1
</p>
<p>πzkk . (19.1)
</p>
<p>Why? Well if we take the above example, we get:
</p>
<p>p(z =
[
0 1 0 0
</p>
<p>]T
) =
</p>
<p>K∏
k=1
</p>
<p>πzkk = π
0
1π
</p>
<p>1
2π
</p>
<p>0
3π
</p>
<p>0
4 = π2,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>19.1 The Gaussian mixture model 291
</p>
<p>p(x)
</p>
<p>x
</p>
<p>−3 −2 −1 0 1 2 3
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>p(x|z1 = 1)
</p>
<p>p(x|z2 = 1)
</p>
<p>p(x|z3 = 1)
</p>
<p>p(x|z4 = 1)
</p>
<p>x
</p>
<p>−3 −2 −1 0 1 2 3
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>x2
</p>
<p>p
(x
)
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>−5
</p>
<p>0
</p>
<p>5
0
</p>
<p>0.02
</p>
<p>0.04
</p>
<p>0.06
</p>
<p>0.08
</p>
<p>0.1
</p>
<p>0.12
</p>
<p>x
</p>
<p>y
</p>
<p>−5 0 5
</p>
<p>−6
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>6
</p>
<p>Fig. 19.2: Top row: One-dimensional Gaussian mixture model example with K = 4 mixture com-
ponents. In the left column is shown the density, p(x), and in the right pane the K = 4 individual
mixture components. Notice the weights scale the mixture components in the density. In the lower
pane is shown a 2D Gaussian mixture model, also with K = 4, both as a 3D surface plot and as a
contour plot.
</p>
<p>as we should expect. We now imagine that when we know what k is, for instance k = 2, then we
know what distribution x has, specifically we assume it is a multivariate normal distribution with
parameters µk and Σk. To put this in symbols
</p>
<p>p(x|zk = 1) = N (x|µk,Σk).
We can once again write this in the simpler form
</p>
<p>p(x|z) =
K∏
k=1
</p>
<p>N(x|µk,Σk)zk , (19.2)
</p>
<p>since for instance,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>292 19 Mixture models for unsupervised clustering
</p>
<p>Algorithm 8: Expectation-Maximization algorithm
</p>
<p>1: Initialize µk,Σk and πk for k = 1, . . . ,K
2: while The likelihood L changes do
3: Update γik =
</p>
<p>N (x|µkΣk)πk∑K
k′=1N (x|µk′Σk′ )πk′
</p>
<p>(E-step)
</p>
<p>4: Update the parameter values in this order (where Nk =
∑N
i=1 γik): (M-step)
</p>
<p>5: µk =
1
Nk
</p>
<p>∑N
i=1 γikxi
</p>
<p>6: Σk =
1
Nk
</p>
<p>∑N
i=1 γik(xi − µk)(xi − µk)
</p>
<p>T
</p>
<p>7: πk =
Nk
N
</p>
<p>8: Compute the likelihood L =
∑N
i=1 log
</p>
<p>[∑K
k=1 πkN (xi|µk,Σk)
</p>
<p>]
9: end while
</p>
<p>p(z =
[
0 1 0 0
</p>
<p>]T
) = N (x|µ1,Σ1)0N (x|µ2,Σ2)1N (x|µ3,Σ3)0N (x|µ4,Σ4)0
</p>
<p>= N (x|µ2,Σ2).
</p>
<p>We are actually done! Distribution p(x) can then be found by the sum and product rule of proba-
bility theory. In particular, using eq. (19.1) and eq. (19.2), it must be the case
</p>
<p>p(x) =
∑
z
</p>
<p>p(x, z) =
∑
z
</p>
<p>p(x|z)p(z)
</p>
<p>=
</p>
<p>K∑
k=1
</p>
<p>πkN (x|µk,Σk). (19.3)
</p>
<p>The normal distributions in the GMM is known as the mixture components and the values πk are
known as the weights. In fig. 19.2 is shown two examples of a Gaussian mixture model. The top
row is an M = 4 mixture component example used to represent the density of a single real number
x. In the right-pane the individual mixture components are plotted; notice the height does not
correspond to their height in the GMM since they are scaled with πk. In the bottom row is shown
the same M = 4 GMM as a surface and contour plot.
</p>
<p>19.2 The EM algorithm
</p>
<p>The GMM is a general and flexible way to represent continuous densities, but without a useful
way to train the GMM it is not very useful. The Expectation Maximization (EM) algorithm provides
an elegant method for finding the parameters of a GMM which approximates a dataset X of N
observations. To state what the EM algorithm tries to accomplish it is convenient to introduce the
following symbols for the parameters:
</p>
<p>π =
[
π1 · · · πk
</p>
<p>]
µ = {µ1, . . . ,µK}
Σ = {Σ1, . . . ,ΣK}
</p>
<p>the objective of the EM algorithm is then to find the values of the parameters π,µ,Σ which
maximizes the log of the likelihood of the data</p>
<p></p>
</div>, <div class="page"><p></p>
<p>19.2 The EM algorithm 293
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−4
</p>
<p>−2
</p>
<p>0
</p>
<p>2
</p>
<p>4
</p>
<p>x1
</p>
<p>x2
</p>
<p>p
(x
)
</p>
<p>−6
−4
</p>
<p>−2
0
</p>
<p>2
4
</p>
<p>6
</p>
<p>−6
−4
</p>
<p>−2
0
</p>
<p>2
4
</p>
<p>6
</p>
<p>0.01
</p>
<p>0.02
</p>
<p>0.03
</p>
<p>0.04
</p>
<p>0.05
</p>
<p>0.06
</p>
<p>Fig. 19.3: The initialization step of the EM algorithm when applied to the 2D dataset shown as the
gray points. The K = 3 mixture components are shown as a contour plot in the left pane, and in
the right pane as a surface plot. The colored circles represent the area capturing twice the standard
deviation of each mixture component.
</p>
<p>L(π,µ,Σ) = log p(X|µ,Σ,π) =
N∑
i=1
</p>
<p>log p(xi|µ,Σ,π) =
N∑
i=1
</p>
<p>log
</p>
<p>[
K∑
k=1
</p>
<p>πkN (xi|µk,Σk)
]
. (19.4)
</p>
<p>This could be accomplished using gradient descent, which we encountered earlier in chapter 15,
however, the EM algorithm takes advantage of the particular form of the problem to provide a
much more effective method. We will first state what the EM algorithm does and later provide an
argument for why the EM algorithm works.
</p>
<p>First some notation: For a given data point xi, the probability xi belongs to component k can
be computed with (as usual) Bayes theorem:
</p>
<p>p(zk = 1|xi) =
p(xi|zk = 1)p(zk = 1)∑K
</p>
<p>k′=1 p(xi|zk′ = 1)p(zk′ = 1)
</p>
<p>=
N (xi|µkΣk)πk∑K
</p>
<p>k′=1N (xi|µk′Σk′)πk′
= γik (19.5)
</p>
<p>We can then define the “total mass” of a component k as Nk =
∑N
i=1 γik. Notice N =
</p>
<p>∑K
k=1Nk
</p>
<p>because
K∑
k=1
</p>
<p>Nk =
</p>
<p>K∑
k=1
</p>
<p>N∑
i=1
</p>
<p>p(zk = 1|xi) =
N∑
i=1
</p>
<p>[
K∑
k=1
</p>
<p>p(zk = 1|xi)
]
</p>
<p>=
</p>
<p>N∑
i=1
</p>
<p>1 = N.
</p>
<p>Since γik denotes the probability observation i belongs to cluster k, we can define the empirical
mean, the empirical covariance and the empirical mass of the clusters as
</p>
<p>µk =
1
</p>
<p>Nk
</p>
<p>N∑
i=1
</p>
<p>γikxi, Σk =
1
</p>
<p>Nk
</p>
<p>N∑
i=1
</p>
<p>γik(xi − µk)(xi − µk)T , and πk =
Nk
N
. (19.6)
</p>
<p>The idea behind the EM algorithm can thus be summarized as:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>294 19 Mixture models for unsupervised clustering
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>Fig. 19.4: Each row corresponds to the first three steps of the EM algorithm and each column to the
E-step and M-step. In the top-left pane, the observations are assigned to the three clusters in the
E-step. The top-left right pane shows the M-step where the parameters πk,Σk and µk are updated
based on the assignments. This continues for two additional steps.
</p>
<p>Initialize: First we initialize µ,Σ and π
Expectation step: Compute γik for all i, k
Maximization step: Update µk,Σk and πk
Iterate: Repeat the two previous steps until the likelihood eq. (19.4) does not change.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>19.2 The EM algorithm 295
</p>
<p>x1
</p>
<p>x
2
</p>
<p>−4 −2 0 2 4 6 8
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>Iterations t
</p>
<p>L
ik
e
li
h
o
o
d
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
</p>
<p>1500
</p>
<p>2000
</p>
<p>2500
</p>
<p>3000
</p>
<p>3500
</p>
<p>4000
</p>
<p>4500
</p>
<p>Fig. 19.5: Running the EM algorithm for 20 iterations produces the above partitioning. The like-
lihood is plotted in the right-pane. As can be seen, the likelihood continues to increase but at a
diminishing rate.
</p>
<p>Putting these steps together we obtain the EM algorithm as given in algorithm 8. To illustrate the
EM algorithm, consider the 2d dataset shown in fig. 19.3 with the given initialization of clusters.
In fig. 19.4 is shown the first three steps of the EM algorithm. The left-most column corresponds
to the E-step and the right-most column to the M-step with the assignments to clusters γik being
indicated by the colors. Furthermore, the EM algorithm maximizes the likelihood and in fig. 19.5
is shown the 20th step of the EM algorithm and the log likelihood.
</p>
<p>19.2.1 Why the EM algorithm worksF
</p>
<p>The above presentation leaves two important questions unanswered. Firstly, the steps of the EM
algorithm might appear as arising from nothing and secondly, why should we believe the EM
algorithm works? To begin with the later question, what the EM algorithm tries to do is to maximize
the log of the likelihood of the data L(π,µ,Σ), and the way we will show this is simply by showing
that both the M -step and the E-step increases the log-likelihood. Secondly, we will see the EM
algorithm can be derived from more general considerations which also applies to other models.
</p>
<p>To begin, suppose we collect the parameters π,µ,Σ into the symbol θ = {π,µ,Σ}. Recall
according to eq. (19.3) each observation xi comes with a latent (binary) vector zi that indicates
which mixture component xi belongs to. That is, if zik = 1 then xi belongs to component k and
we write:
</p>
<p>p(xi|θ) =
∑
zi
</p>
<p>p(xi, zi|θ) =
∑
zi
</p>
<p>p(xi|zi,θ)p(zi) =
K∑
k=1
</p>
<p>πkN (xi|µk,Σk). (19.7)
</p>
<p>If we collect all the zi’s in an N ×K matrix Z we can therefore write:
</p>
<p>p(X|θ) =
∑
Z
</p>
<p>p(X,Z|θ),</p>
<p></p>
</div>, <div class="page"><p></p>
<p>296 19 Mixture models for unsupervised clustering
</p>
<p>where p(X,Z|θ) = ∏Ni=1 p(xi, zi|θ). We can now proceed with a little algebra. Recall that the
log of the likelihood, which we wish to maximize, is simply L(X|θ) = logP (X|θ) and the later
probability can be re-written using the basic rules of probability:
</p>
<p>log p(X|θ) = log p(X|θ)P (Z|X,θ)
P (Z|X,θ) = log p(X,Z|θ)− log p(Z|X,θ) (19.8)
</p>
<p>Suppose we consider any other setting of the parameters θold. We can then (at least symbolically)
write up the distribution p(Z|X,θold) and taking the expectation of both sides of eq. (19.8) gives
(the left-hand side is not affected by the expectation because it is independent of Z):
</p>
<p>log p(X|θ) = Ep(Z|X,θold) [log p(X,Z|θ)]− Ep(Z|X,θold) [log p(Z|X,θ)] . (19.9)
</p>
<p>Now to the quite amazing thing. First, it follows from Jensen’s inequality1 that
</p>
<p>Ep(Z|X,θold) [log p(Z|X,θ)] ≤ Ep(Z|X,θold)
[
log p(Z|X,θold)
</p>
<p>]
. (19.10)
</p>
<p>In other words, considering only the last term the log-likelihood becomes as small as possible if
θ = θold. Let’s connect this to the EM algorithm. Suppose θold is the value of θ at a given step of
the algorithm. Suppose then we select θ as the value that maximize the first term in eq. (19.9):
</p>
<p>θ = arg max
θ
</p>
<p>Ep(Z|X,θold) [log p(X,Z|θ)] (19.11)
</p>
<p>Using eq. (19.9) we then have that for this θ:
</p>
<p>By eq. (19.11) : Ep(Z|X,θold) [log p(X,Z|θ)] ≥ Ep(Z|X,θold)
[
log p(X,Z|θold)
</p>
<p>]
(19.12)
</p>
<p>By eq. (19.10) : −Ep(Z|X,θold) [log p(Z|X,θ)] ≥ −Ep(Z|X,θold)
[
log p(Z|X,θold)
</p>
<p>]
(19.13)
</p>
<p>Using these two expression on each term in eq. (19.9) we have now shown
</p>
<p>log p(X|θ) = Ep(Z|X,θold) [log p(X,Z|θ)]− Ep(Z|X,θold) [log p(Z|X,θ)] (19.14)
</p>
<p>≥ Ep(Z|X,θold)
[
log p(X,Z|θold)
</p>
<p>]
− Ep(Z|X,θold)
</p>
<p>[
log p(Z|X,θold)
</p>
<p>]
(19.15)
</p>
<p>= log p(X|θold) (19.16)
1 Recall that Jensen’s inequality says that for any concave function φ (and the logarithm is a concave
</p>
<p>function) and densities p it holds that: Ep(x) [φ(f(x))] ≤ φ
(
Ep(x)[f(x)]
</p>
<p>)
. Then, for any density r(x) it
</p>
<p>holds
</p>
<p>Ep(x) [log p(x)] = Ep(x)
</p>
<p>[
log
</p>
<p>p(x)
</p>
<p>r(x)
</p>
<p>]
+ Ep(x) [log r(x)] = −Ep(x)
</p>
<p>[
log
</p>
<p>r(x)
</p>
<p>p(x)
</p>
<p>]
+ Ep(x) [log r(x)] .
</p>
<p>By Jensen’s inequality the first term is always less than 0 because−Ep(x)
[
log r(x)
</p>
<p>p(x)
</p>
<p>]
≥ − logEp(x)
</p>
<p>[
r(x)
p(x)
</p>
<p>]
=
</p>
<p>− log
∑
x r(x) = − log 1 = 0. Applying this to the right-hand side of the above equation we get:
</p>
<p>Ep(x) [log p(x)] = −Ep(x)
[
log
</p>
<p>r(x)
</p>
<p>p(x)
</p>
<p>]
+ Ep(x) [log r(x)] ≥ Ep(x) [log r(x)] .
</p>
<p>The result now follows by replacing x with Z, p with p(Z|X,θold) and r with p(Z|X,θ).</p>
<p></p>
</div>, <div class="page"><p></p>
<p>19.2 The EM algorithm 297
</p>
<p>In other words, choosing θ to maximize Ep(Z|X,θold) [log p(X,Z|θ)] in eq. (19.11) also maximize
the log-likelihood L(X|θ). How is this connected with the EM algorithm? Firstly, the posterior
p(Z|X,θold) exactly corresponds to γik computed in the E-step using eq. (19.5). We can then
examine what happens in the maximization-step eq. (19.11) more closely by noticing:
</p>
<p>Ep(Z|X,θold) [log p(X,Z|θ)] = Ep(Z|X,θold)
</p>
<p>[
N∑
i=1
</p>
<p>log p(xi, zi|θ)
]
</p>
<p>=
</p>
<p>N∑
i=1
</p>
<p>K∑
k=1
</p>
<p>γik log p(xi, zik = 1|θ)
</p>
<p>=
</p>
<p>N∑
i=1
</p>
<p>K∑
k=1
</p>
<p>γik log [πkN (xi|µk,Σk)]
</p>
<p>=
</p>
<p>K∑
k=1
</p>
<p>Nk log πk +
</p>
<p>N∑
i=1
</p>
<p>K∑
k=1
</p>
<p>γik logN (xi|µk,Σk)
</p>
<p>We leave it to the reader to show that differentiating these expressions by the parameter we are
interested in maximizing and setting the derivative equal to zero results in exactly the M -step
updates given in section 19.2.
</p>
<p>19.2.2 Some problems with the EM algorithm
</p>
<p>The EM algorithm is guaranteed to always increase the log likelihood L(π,µ,Σ), however, this
does not mean the EM algorithm is guaranteed to be well-behaved. Firstly, what values of π,µ
and Σ the EM algorithm converges to depends upon the initialization; this is similar to K-means
clustering but in general the increased flexibility of the EM algorithm for GMMs increases this
problem. Secondly, the EM algorithm may exhibit divergent behaviour. If a mixture component k
is centered upon a single observation, and this is the only observation for which γik is large, the
EM algorithm may diverge in the sense the cluster becomes more and more peaked around this
observation; in other words, the EM algorithm diverges. To compensate for this one can add a
regularization term to Σk as
</p>
<p>Σk =
1
</p>
<p>Nk
</p>
<p>N∑
i=1
</p>
<p>γik(xi − µk)(xi − µk)T + λI
</p>
<p>where λ &gt; 0 is the regularization term. This difficulty increases with poor initialization and it is
therefore recommended to initialize the EM algorithm to the output of the K-means clustering
algorithm. Third, the EM algorithm in its present form requires
</p>
<p>(K − 1)︸ ︷︷ ︸
π
</p>
<p>+ KM︸︷︷︸
µ1,...,µK
</p>
<p>+K(M + 1)M/2︸ ︷︷ ︸
Σ1,...,ΣK
</p>
<p>parameters; for high-dimensional datasets the number K(M + 1)M/2 can be brought down by
considering a diagonal covariance matrix to KM . There is however also goods news with regards
to the EM algorithm for GMMs. Asides accomplishing the primary objective, a general density
estimator which can be fitted efficiently, an advantage of the EM algorithm over K-means is that
one can select K using cross-validation.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>298 19 Mixture models for unsupervised clustering
</p>
<p>Training
</p>
<p>Test
</p>
<p>−2 −1 0 1 2 3 −2 −1 0 1 2 3
</p>
<p>−2 −1 0 1 2 3 −2 −1 0 1 2 3
</p>
<p>Fig. 19.6: Example of four GMMs fitted to a small dataset comprised ofN = 13 training observations
and N test = 4 test-observations indicated by the red crosses. The figure illustrates how the GMM
begins to overfit the data as the number of mixture components is increased. The test log-likelihood
is shown in fig. 19.7.
</p>
<p>19.2.3 Selecting K for the GMM using Cross-validation
</p>
<p>As opposed to the K-means algorithm, the GMM provides a natural way to select K. Since the
goal of fitting a GMM using for instance the EM algorithm is to maximize the log-likelihood, it is
natural to quantify the predictive performance in terms of the log-likelihood measured on a test set
Xtest:
</p>
<p>Ltest(π,µ,Σ) = log p(Xtest|µ,Σ,π) (19.17)
</p>
<p>=
</p>
<p>Ntest∑
i=1
</p>
<p>log
</p>
<p>[
K∑
k=1
</p>
<p>πkN (xtesti |µk,Σk)
]
. (19.18)
</p>
<p>One can then apply cross-validation using −Ltest(π,µ,Σ) as an error measure to select the number
of mixture components K. This procedure is illustrated in fig. 19.6 where four different GMMs
corresponding to K = 1, 2, 3, 4 is fitted until convergence on a small 1d dataset comprised of
N = 13 training observations and N test = 4 test-observations indicated by the red crosses. As seen,
the GMM begins to overfit as K becomes large leading to reduced test log-likelihood (Ltest) plotted
in fig. 19.7. In a similar fashion, cross-validation could also be used to select the regularization
parameter λ.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>19.2 The EM algorithm 299
</p>
<p>Number of mixture components
</p>
<p>T
es
t
lo
g
li
k
el
ih
o
o
d
</p>
<p>1 2 3 4
</p>
<p>−11
</p>
<p>−10
</p>
<p>−9
</p>
<p>−8
</p>
<p>−7
</p>
<p>−6
</p>
<p>Fig. 19.7: Test log-likelihood as evaluated on the four test observations and four values of K,
K = 1, 2, 3, 4 shown in fig. 19.6.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>300 19 Mixture models for unsupervised clustering
</p>
<p>Problems
</p>
<p>19.1. Fall 2013 question 22: Let
</p>
<p>N (x|µ,Σ) = 1√
2π|Σ|
</p>
<p>exp
</p>
<p>(
−1
</p>
<p>2
(x− µ)&gt;Σ−1(x− µ)
</p>
<p>)
define the multivariate normal distribution with mean
µ and covariance matrix Σ. In Figure 19.8 is given 5000
observations drawn from a density defined by a Gaussian
Mixture Model (GMM) with three clusters.
</p>
<p>Fig. 19.8: 5000 data observations drawn from a Gaussian
Mixture Model (GMM) with three clusters.
</p>
<p>Which one of the following GMM densities was used
to generate the data?
</p>
<p>A
</p>
<p>p(x) =
1
</p>
<p>3
· N (x|
</p>
<p>[
15
15
</p>
<p>]
,
</p>
<p>[
0.5 0
0 15
</p>
<p>]
)
</p>
<p>+
1
</p>
<p>3
· N (x|
</p>
<p>[
30
0
</p>
<p>]
,
</p>
<p>[
2 −1.4
−1.4 2
</p>
<p>]
)
</p>
<p>+
1
</p>
<p>3
· N (x|
</p>
<p>[
0
0
</p>
<p>]
,
</p>
<p>[
10 7
7 10
</p>
<p>]
)
</p>
<p>B
</p>
<p>p(x) =
1
</p>
<p>3
· N (x|
</p>
<p>[
15
15
</p>
<p>]
,
</p>
<p>[
0.5 0
0 15
</p>
<p>]
)
</p>
<p>+
1
</p>
<p>3
· N (x|
</p>
<p>[
30
0
</p>
<p>]
,
</p>
<p>[
2 1.4
</p>
<p>1.4 2
</p>
<p>]
)
</p>
<p>+
1
</p>
<p>3
· N (x|
</p>
<p>[
0
0
</p>
<p>]
,
</p>
<p>[
10 −7
−7 10
</p>
<p>]
)
</p>
<p>C
</p>
<p>p(x) =
1
</p>
<p>3
· N (x|
</p>
<p>[
15
15
</p>
<p>]
,
</p>
<p>[
0.5 0
0 15
</p>
<p>]
)
</p>
<p>+
1
</p>
<p>3
· N (x|
</p>
<p>[
30
0
</p>
<p>]
,
</p>
<p>[
10 −7
−7 10
</p>
<p>]
)
</p>
<p>+
1
</p>
<p>3
· N (x|
</p>
<p>[
0
0
</p>
<p>]
,
</p>
<p>[
2 1.4
</p>
<p>1.4 2
</p>
<p>]
)
</p>
<p>D
</p>
<p>p(x) =
1
</p>
<p>3
· N (x|
</p>
<p>[
15
15
</p>
<p>]
,
</p>
<p>[
15 0
0 0.5
</p>
<p>]
)
</p>
<p>+
1
</p>
<p>3
· N (x|
</p>
<p>[
30
0
</p>
<p>]
,
</p>
<p>[
2 1.4
</p>
<p>1.4 2
</p>
<p>]
)
</p>
<p>+
1
</p>
<p>3
· N (x|
</p>
<p>[
0
0
</p>
<p>]
,
</p>
<p>[
10 −7
−7 10
</p>
<p>]
)
</p>
<p>E Don’t know.
</p>
<p>19.2. Fall 2013 question 26: Which one of the follow-
ing statements pertaining to clustering is correct?
</p>
<p>A k-means and Gaussian Mixture Models are guaran-
teed to find the same solutions regardless of initial-
ization.
</p>
<p>B The level at which clusters merge in the dendrogram
in hierarchical clustering using minimum/single-,
maximum/complete- or group average linkage can be
determined by the proximities between all the obser-
vations.
</p>
<p>C In k-means the cluster centers are updated as the
average of the observations belonging to the cluster
regardless of the distance measure used.
</p>
<p>D A Gaussian Mixture Model with diagonal covariance
matrix has the same number of free parameters as
k-means.
</p>
<p>E Don’t know.
</p>
<p>19.3. Fall 2014 question 7: Suppose the points in the
scatter plot fig. 19.9 was generated from a Gaussian
mixture-model (GMM) of the form</p>
<p></p>
</div>, <div class="page"><p></p>
<p>19.2 The EM algorithm 301
</p>
<p>y
</p>
<p>x
</p>
<p>−10 −5 0 5 10
</p>
<p>−5
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>Fig. 19.9: Scatter plot of observations
</p>
<p>p(x, y) =
</p>
<p>2∑
i=1
</p>
<p>wiN
([
x
y
</p>
<p>]
;
</p>
<p>[
0
µi
</p>
<p>]
,
</p>
<p>[
σ2i 0
0 δ2i
</p>
<p>])
.
</p>
<p>and suppose µ1 = 7, µ2 = 1. Which of the following is
most likely to be true?
</p>
<p>A w1 = 0.5, σ
2
1 = 2σ
</p>
<p>2
2 , δ
</p>
<p>2
1 = 2δ
</p>
<p>2
2
</p>
<p>B w1 = 0.7, δ
2
1 &gt; σ
</p>
<p>2
2
</p>
<p>C w1 = 0.7, σ
2
1 = 20, δ
</p>
<p>2
2 = 1
</p>
<p>D w1 = 0.5, p(0, 0) &lt; p(0, 7)
E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>20
</p>
<p>Density estimation
</p>
<p>Anomaly detection attempts to find observations that can be regarded as different from the other
observations. A tempting way to put this is we should consider an observation anomalous when it
lies in a low density region of the data, i.e. a region where we would consider it unexpected to find
an observation. We will therefore mainly regard anomaly detection as a problem of estimating the
density of a dataset and then obtaining the (candidate) outliers is simply a matter of finding the
lowest-density observations.
</p>
<p>The GMM can be regarded as the primary density-estimation tool for our disposal, however, for
very large datasets the GMM might be too expensive to fit. An additional problem with the GMM
is that it is affected by initialization and, as we will see in a moment, can have difficulties treating
regions of different density leading to potentially spurious results. It is therefore useful to consider
approximate, deterministic methods for density estimation which are more robust. In this section,
we will consider two such approaches: kernel density estimation, which is an approximation to the
GMM, and Average relative density which is an entirely separate method not based on probabilities.
</p>
<p>Kernel density estimation was separately discovered by Murray Rosenblatt and Emanuel
Parzen [Rosenblatt et al., 1956, Parzen, 1962], meanwhile the section on the average relative density
is based on Tan et al. [2013].
</p>
<p>20.1 The kernel density estimator
</p>
<p>A problem with the Gaussian mixture-model is that it simply contains many parameters to be fitted
and selecting different values of these parameters (or re-starting the EM algorithm from different
initial configurations) will lead to different assignment of density. A kernel density estimator (KDE)
is best seen as a deterministic approximation to the Gaussian mixture model which tries to overcome
some of these limitations. Recall the density of a GMM with K components can be written as:
</p>
<p>p(x) =
</p>
<p>K∑
k=1
</p>
<p>πkN (x|µk,Σk). (20.1)
</p>
<p>where π,µ and Σ are all parameters to be tuned. When we apply a KDE to a dataset X of N
observations we simply assume the GMM consists of K = N components centered on top of each
data point and with diagonal covariance matrix. In other words, we select</p>
<p></p>
</div>, <div class="page"><p></p>
<p>304 20 Density estimation
</p>
<p>Training
</p>
<p>Test
</p>
<p>−2 −1 0 1 2 3 −2 −1 0 1 2 3
</p>
<p>−2 −1 0 1 2 3 Bandwidth
</p>
<p>T
es
t
lo
g
li
k
el
ih
o
o
d
</p>
<p>0.5 1 1.5 2 2.5 3
</p>
<p>−9
</p>
<p>−8
</p>
<p>−7
</p>
<p>−6
</p>
<p>Fig. 20.1: Example of a KDE fitted to a small dataset comprised of N = 13 training observations and
N test = 4 test-observations indicated by the red crosses. The figure illustrates how the KDE overfits
as the kernel parameter λ is varied as λ = 2, 0.5, 0.15. The last pane shows the test log-likelihood.
</p>
<p>πk =
1
</p>
<p>N
, µk = xk and Σk = λ
</p>
<p>2I
</p>
<p>where λ is known as the kernel width. This gives a density of the form:
</p>
<p>pλ(x) =
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>N (x|xi, λ2I). (20.2)
</p>
<p>20.1.1 Selecting the kernel width λ
</p>
<p>We can select λ similar to how we selected K for the GMM namely by using cross-validation.
Consider a test set Xtest, we can then similar to the GMM consider the log of the likelihood of the
test data:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>20.1 The kernel density estimator 305
</p>
<p>−3 −2 −1 0 1 2 3 Bandwidth
</p>
<p>L
O
O
</p>
<p>lo
g
li
k
el
ih
o
o
d
</p>
<p>0.5 1 1.5 2 2.5 3
</p>
<p>−6
</p>
<p>−5
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>Fig. 20.2: LOO estimation of the kernel width parameter λ for the full dataset comprised N = 17
observations along with the KDE corresponding to the best bandwidth.
</p>
<p>L(λ) = log p(Xtest|λ)
</p>
<p>=
</p>
<p>Ntest∑
j=1
</p>
<p>log p(xtestj |λ)
</p>
<p>=
</p>
<p>Ntest∑
j=1
</p>
<p>log
</p>
<p>[
1
</p>
<p>N
</p>
<p>N∑
i=1
</p>
<p>N (xtestj |xi, λ2I)
]
</p>
<p>A simple example using a dataset of N = 13 observations and a test set of N test = 4 observations
can be found in fig. 20.1. Another advantage of the KDE over the GMM is that leave-one-out
cross-validation can be carried out very quickly: For each pair of observations we can pre-compute
Mij = N (xi|xj , λ2I) once and re-use them in the computation of the leave-one-out estimate of the
log of the likelihood as:
</p>
<p>L(λ) = 1
N
</p>
<p>N∑
i=1
</p>
<p>log
</p>
<p>∑
j 6=i
</p>
<p>1
</p>
<p>N − 1Mij
</p>
<p> .
The leave-one-out estimate of the log of the likelihood for the full dataset comprised of N = 17
observations can be found in fig. 20.2 along with KDE corresponding to the highest log likelihood
according to the LOO estimator. LOO estimation is only one of several ways of selecting the kernel
widths, an interested reader can consult Raykar and Duraiswami [2006] for other approaches.
</p>
<p>Uneven densities and the GMM
</p>
<p>Let us turn to a problem for which the KDE or GMM may not be suitable. In fig. 20.3 we have shown
a simple 2d dataset comprised of two clusters of data of very uneven density and two candidate
outliers indicated by the red circles. Comparing the two outliers, the right-most candidate outlier
is clearly further away from its nearest neighbours, however, it also lies in a region of relatively
lower density. The KDE is unable to make use of this difference in density as it use the same kernel
width for the entire dataset and therefore tends to consider the right-most point a better candidate
outlier as shown in fig. 20.4 (top row) for different choices of the kernel width (λ = 0.2, 1, 4). In the
bottom row we have shown the density obtained by applying the GMM for K = 1, 2, 4. As shown,</p>
<p></p>
</div>, <div class="page"><p></p>
<p>306 20 Density estimation
</p>
<p>−4 −2 0 2
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>Fig. 20.3: A simple 2d dataset comprised of two clusters of data of very uneven density and two
candidate outliers indicated by the red circles. The right-most candidate outlier is clearly further
away from its nearest neighbours, however, it also lies in a region of relatively lower density. Which
of the two should we suspect are outliers?
</p>
<p>the GMM rapidly begins to overfit the data and we should therefore cross-validate to select K. In
addition, the use of specific cluster centers may lead to artificially high-density regions as shown by
the elongated oval shape in the plot for K = 4. As a rule, this makes the GMM more flexible for
fitting densities and good at handling densities which are elongated along one or more directions (i.e.
elliptical densities), but also somewhat prone to spurious behaviour due to the particulars of how
the EM algorithm decided to place the cluster centers during a particular run. Thus, when using
GMMs for outlier detection it is important the number of components be carefully determined (i.e.,
using cross-validation) and naturally, the observations which we wish to examine as being potential
outliers should not be part of the data used for training the GMM density.
</p>
<p>20.2 Average relative density
</p>
<p>The average relative density (ARD) tries to overcome the difficulty we saw in the earlier section
where the KDE or GMM was unable to handle clusters of different densities well. However, the
ARD is also different from the KDE or GMM in that it does not rely on probabilities.
</p>
<p>Recall the definition of the K nearest neighbourhood of a point x given in eq. (12.1) from
chapter 12, i.e. the K observations in the dataset X which are closest to x:
</p>
<p>NX(x,K) = {The K observations in X which are nearest to x} . (20.3)
</p>
<p>The average distance to the K nearest neighbours is given by
</p>
<p>1
</p>
<p>K
</p>
<p>∑
x′∈NX(x,K)
</p>
<p>d(x,x′),
</p>
<p>where d is the relevant distance measure for our dataset, for instance the Euclidian distance
d(x,y) = ‖x − y‖2. Intuitively, if the average distance to the nearest neighbours is low, that</p>
<p></p>
</div>, <div class="page"><p></p>
<p>20.2 Average relative density 307
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>Fig. 20.4: (top row:) The kernel density estimator applied to a 2d dataset for different settings of the
kernel width. From left to right we have plotted λ = 0.2, 1, 4. (bottom row:) Density as estimated
by the GMM for K = 1, 2, 4 components and initialized using the K-means algorithm. The second
component for K = 2 is quite faint and the GMM rapidly begins to overfit for K &gt; 2
</p>
<p>means there are many observations close to x and so the density at x is high and contrary, if the
average distance is high, the density is low. It thus makes sense to define the density around x
computed by K neighbours as the inverse of the average distance
</p>
<p>densityX(x,K) =
1
</p>
<p>1
K
</p>
<p>∑
x′∈NX(x,K) d(x,x
</p>
<p>′)
. (20.4)
</p>
<p>Suppose for a dataset X we wish to evaluate the density of observation i, xi, of the dataset.
Obviously, we don’t want to include xi as a member of the K-neighbourhood because that would
bias the density upwards. Imagine if K = 1, then obviously NX(xi,K) = {xi} because xi is in X
and so densityX(xi,K) =
</p>
<p>1
1
1d(xi,xi)
</p>
<p>= 10 = ∞. Rather in the case where we wish to compute the
density of an observation xi from X we therefore use:
</p>
<p>densityX\i(xi,K) =
1
</p>
<p>1
K
</p>
<p>∑
x′∈NX\i (xi,K)
</p>
<p>d(xi,x′)
, (20.5)
</p>
<p>where, as in eq. (12.6), X\i is simply X with observation i removed:
</p>
<p>XT\i =
[
x1 x2 · · ·xi−2 xi−1 xi+1 xi+2 · · · xN
</p>
<p>]
.
</p>
<p>One could use the (estimated) density directly, however, if we are looking for outliers it is perhaps
more relevant still to look for those points where the density is lower than what it typically is for
surrounding points. This is exactly what the average relative density (ard) attempts to accomplish</p>
<p></p>
</div>, <div class="page"><p></p>
<p>308 20 Density estimation
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>−4 −3 −2 −1 0 1
</p>
<p>−4
</p>
<p>−3
</p>
<p>−2
</p>
<p>−1
</p>
<p>0
</p>
<p>1
</p>
<p>Fig. 20.5: (top row:) Density plotted for the 2d dataset shown in fig. 20.3 for K = 2, 4, 6. The
density relies only on the average distances and so considers all the top-right points to be anomalous.
(bottom row:) The ard takes the relative density into account and therefore do not consider the
low-density cluster to be anomalous but rather considers the left-most candidate outlier to be far
more suspicious.
</p>
<p>by considering the density of a given point x relative to the average of the density of the K nearest
neighbours xj ∈ NX(x,K) of x where we use eq. (20.5) to estimate the density of each xj :
</p>
<p>ardX(x,K) =
densityX(x,K)
</p>
<p>1
K
</p>
<p>∑
xj∈NX(x,K) densityX\j (xj ,K)
</p>
<p>. (20.6)
</p>
<p>It is instructive to consider what this definition means for K = 1. In this case we first find the one
observation in X closest to x namely NX(x,K) = {xj} and then we simply compute the relative
density:
</p>
<p>ardX(x, 1) =
densityX(x, 1)
</p>
<p>densityX\j (xj , 1)
.
</p>
<p>Suppose further that the observation in X closest to xj (but which is not xj itself) is NX\j (xj , 1) =
{xk}. In this case the above becomes:
</p>
<p>ardX(x, 1) =
</p>
<p>1
d(x,xj)
</p>
<p>1
d(xj ,xk)
</p>
<p>=
d(xj ,xk)
</p>
<p>d(x,xj)
</p>
<p>That is, if x is closer to its nearest neighbour xj than it’s nearest neighbour xj is to it’s nearest
neighbour xk then the ard is high and vice versa.
</p>
<p>Finally, if we wish to compute the ard for an already existing observation xi in X then similar
to eq. (20.5) we have to exclude that observation from X to not bias the ard upwards. That is, we
should use:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>20.2 Average relative density 309
</p>
<p>ardX(xi,K) =
densityX\i(xi,K)
</p>
<p>1
K
</p>
<p>∑
xj∈NX\i (xi,K)
</p>
<p>densityX\j (xj ,K)
. (20.7)
</p>
<p>The result of plotting the density and the ard can be seen in the top and bottom rows of fig. 20.5.
The top row illustrates the density for K = 2, 4, 6 and the bottom row the ard for the same choices
of K. We see how the density marks all the points in the low-density region as outliers, however,
the ard is able to take into account they are in a low-density region and consider the left-most
candidate outlier far more likely to be anomalous.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>310 20 Density estimation
</p>
<p>Problems
</p>
<p>20.1. Fall 2013 question 11:
</p>
<p>O1 O2 O3 O4 O5 O6 O7
</p>
<p>O8 5.11 4.79 4.90 4.74 2.96 5.16 2.88
</p>
<p>Table 20.1: Pairwise Euclidean distance, i.e d(Oa,Ob) =
||xa − xb||2 =
</p>
<p>√∑
m(xam − xbm)2, between observation
</p>
<p>O8 and observation O1–O7 given in Table 20.2.
</p>
<p>We would like to quantify if O8 is an outlier using a
Gaussian kernel density estimator where we use the seven
observations O1, O2, . . ., O7 to estimate the density at
observation O8 based on the Euclidean distances given
in Table 20.2 and reproduced in terms of observation O8
in Table 20.1. The Gaussian kernel density estimator is
given by
</p>
<p>p(x) =
1
</p>
<p>N
</p>
<p>N∑
n=1
</p>
<p>1
</p>
<p>(2π)M/2
√
|σ2I|
</p>
<p>exp
</p>
<p>(
− 1
</p>
<p>2
(x− xn)&gt;(
</p>
<p>1
</p>
<p>σ2
I)(x− xn)
</p>
<p>)
Thus, N = 7 and in our analysis we will use σ = 1 and as
the dataset is 7-dimensional we have that M=7. We note
that |σ2I| is the determinant of the diagonal matrix with
σ2 in the diagonal. For σ = 1 we have |σ2I| = 1. What
is the density at observation O8 using only observations
O1-O7 in the above Gaussian kernel density estimator
</p>
<p>O1 O2 O3 O4 O5 O6 O7 O8
</p>
<p>O1 0 2.39 1.73 0.96 3.46 4.07 4.27 5.11
O2 2.39 0 1.15 1.76 2.66 5.36 3.54 4.79
O3 1.73 1.15 0 1.52 3.01 4.66 3.77 4.90
O4 0.96 1.76 1.52 0 2.84 4.25 3.80 4.74
O5 3.46 2.66 3.01 2.84 0 4.88 1.41 2.96
O6 4.07 5.36 4.66 4.25 4.88 0 5.47 5.16
O7 4.27 3.54 3.77 3.80 1.41 5.47 0 2.88
O8 5.11 4.79 4.90 4.74 2.96 5.16 2.88 0
</p>
<p>Table 20.2: Pairwise Euclidean distance, i.e d(Oa,Ob) =
||xa − xb||2 =
</p>
<p>√∑
m(xam − xbm)2, between eight ob-
</p>
<p>servations of the Galápagos data. Red observations (i.e.,
O1, O2, O3, and O4) correspond to the four smallest is-
lands whereas blue observations (i.e., O5, O6, O7, and
O8) correspond to the four largest islands.
</p>
<p>A 3.4 · 10−32
B 1.3 · 10−8
C 6.5 · 10−6
D 5.1 · 10−2
E Don’t know.
</p>
<p>20.2. Spring 2013 question 9: We suspect that ob-
servation O1 may be an outlier. In order to assess if this
is the case we would like to calculate the average rel-
ative KNN density based on the observations given in
</p>
<p>Table 20.3 only. We recall that the KNN density and av-
erage relative density for the observation x are given by:
density(x, K) =
</p>
<p>(
1
K
</p>
<p>∑
y∈N(x,K) distance(x,y)
</p>
<p>)−1
,
</p>
<p>a.r.d.(x, K) =
density(x,K)
</p>
<p>1
</p>
<p>K
</p>
<p>∑
y∈N(x,K)
</p>
<p>density(y, K)
,
</p>
<p>where N(x,K) is the set of K nearest neighbors of obser-
vation x and a.r.d(x,K) is the average relative density
of x using K nearest neighbors. Based on the data in
Table 20.3, what is the average relative density for ob-
servation O1 for K = 2 nearest neighbors?
</p>
<p>O1 O2 O3 O4 O5 O6 O7 O8 O9 O10
O1 0 393.5 68.1 165.4 271.8 200.6 210.9 206.1 166.3 365.0
O2 393.5 0 411.3 361.8 478.6 490.9 409.2 382.3 391.1 37.4
O3 68.1 411.3 0 119.8 208.4 136.6 152.8 154.3 111.1 387.1
O4 165.4 361.8 119.8 0 137.5 130.8 62.1 44.7 32.5 346.2
O5 271.8 478.6 208.4 137.5 0 99.0 76.8 101.0 116.4 468.5
O6 200.6 490.9 136.6 130.8 99.0 0 100.1 124.0 100.5 473.8
O7 210.9 409.2 152.8 62.1 76.8 100.1 0 29.5 45.2 396.8
O8 206.1 382.3 154.3 44.7 101.0 124.0 29.5 0 44.6 370.1
O9 166.3 391.1 111.1 32.5 116.4 100.5 45.2 44.6 0 375.1
</p>
<p>O10 365.0 37.4 387.1 346.2 468.5 473.8 396.8 370.1 375.1 0
</p>
<p>Table 20.3: Pairwise Euclidean distance between the 10
first observations in the PM10 data. Red observations
(i.e., O1, O3, O5, O6, O7 and O8) are observations where
the pollution levels are above the median value and dark
green observations (i.e., O2, O4, O9 and O10) correspond
to observations where the pollution level is below the me-
dian value.
</p>
<p>A 0.01
B 0.02
C 0.23
D 0.46
E Don’t know.
</p>
<p>20.3. Fall 2011 question 27: One definition of an out-
lier is the following:
</p>
<p>An outlier is an object that has a low probability
with respect to a probability distribution model of
the data.
</p>
<p>Consider the data set in Table 20.4. To detect outliers,
we standardize the data
</p>
<p>zn =
xn −mean(x)
</p>
<p>std(x)
, mean(x) = 6, std(x) = 28,
</p>
<p>and model them with a standard univariate Normal dis-
tribution N(0, 1). We define an outlier as a data object
with an attribute
</p>
<p>|zn| &gt; c,
</p>
<p>where c is a constant such that the probability that
|zn| &gt; c is equal to α, i.e., p(|zn| &gt; c) = α. We choose
α = 0.0027 corresponding to c = 3.00. According to this
definition, which data objects are judged to be outliers?</p>
<p></p>
</div>, <div class="page"><p></p>
<p>20.2 Average relative density 311
</p>
<p>n 1 2 3 4 5 6 7 8
</p>
<p>xn −3 −5 −8 0 75 −4 −1 −6
</p>
<p>Table 20.4: A simple data set with eight data objects
each with one attribute.
</p>
<p>A There are no outliers.
B x5 = 75 is an outlier.
C x3 = −8 and x5 = 75 are outliers.
D x4 = 0, x3 = −8, x5 = 75 are outliers.
E Don’t know.
</p>
<p>20.4. Fall 2013 question 10: We suspect that obser-
vation O8 may be an outlier. In order to assess if this
is the case we would like to calculate the average rel-
ative KNN density based on the observations given in
Table 20.2 only. We recall that the KNN density and av-
erage relative density for the observation x are given by:
density(x, K) =
</p>
<p>(
1
K
</p>
<p>∑
y∈N(x,K) distance(x,y)
</p>
<p>)−1
,
</p>
<p>a.r.d.(x, K) =
density(x,K)
</p>
<p>1
</p>
<p>K
</p>
<p>∑
y∈N(x,K)
</p>
<p>density(y, K)
,
</p>
<p>where N(x,K) is the set of K nearest neighbors of obser-
vation x and a.r.d(x,K) is the average relative density
of x using K nearest neighbors. Based on the data in
Table 20.2, what is the average relative density for ob-
servation O8 for K = 3 nearest neighbors?
</p>
<p>A 0.19
B 0.28
C 0.56
D 1.79
E Don’t know.
</p>
<p>20.5. Fall 2014 question 13: Consider again the dis-
tances in table 20.5. We wish to compute the average
relative KNN density (a.r.d) of the observations in ta-
ble 20.5 using the cityblock distance indicated by the ta-
ble. Letting d(x,y) denote the cityblock distance metric
this is defined as
</p>
<p>density(x,K) =
1
</p>
<p>1
K
</p>
<p>∑
y∈N(x,K) d(x,y)
</p>
<p>a.r.d(x,K) =
density(x,K)
</p>
<p>1
K
</p>
<p>∑
z∈N(x,K) density(z,K)
</p>
<p>,
</p>
<p>N(x,K) : Set of K-nearest neighbours of x.
</p>
<p>What is the a.r.d. of observation o1 using K = 1 nearest
neighbours?
</p>
<p>o1 o2 o3 o4 o5 o6 o7 o8
o1 0 4 7 9 5 5 5 6
o2 4 0 7 7 7 3 7 8
o3 7 7 0 10 6 6 4 9
o4 9 7 10 0 8 6 10 9
o5 5 7 6 8 0 8 6 7
o6 5 3 6 6 8 0 8 11
o7 5 7 4 10 6 8 0 7
o8 6 8 9 9 7 11 7 0
</p>
<p>Table 20.5: Pairwise Cityblock distance, i.e d(oi, oi) =
</p>
<p>‖xi − xj‖1 =
∑M
k=1 |xik − xjk|, between 8 observations.
</p>
<p>Each observation oi corresponds to a M = 15 dimen-
sional binary vector, xik ∈ {0, 1}. The blue observations
{o1, o2, o3, o4} belong to class C1 and the black observa-
tions {o5, o6, o7, o8} belong to class C2.
</p>
<p>A a.r.d(x = o1,K = 1) =
1
4
</p>
<p>B a.r.d(x = o1,K = 1) =
1
3
</p>
<p>C a.r.d(x = o1,K = 1) =
1
2
</p>
<p>D a.r.d(x = o1,K = 1) =
3
4
</p>
<p>E Don’t know.
</p>
<p>20.6. Fall 2014 question 14: Consider again the dis-
tances in table 20.5. Suppose we wish to perform mixture
modelling (see chapter 9.2 of “Introduction to data min-
ing”) and we consider mixture distributions of the form
(λ &gt; 0):
</p>
<p>p(x|θ) = 1
2λ
</p>
<p>exp(−d(x,θ)/λ)
</p>
<p>Suppose we consider K = 8 mixture components, the pa-
rameter θ1, . . . ,θ8 of each mixture component is taken
to be the position of the observations o1, . . . , o8 and each
mixture component is weighted equally in the full mix-
ture distribution. Suppose we set λ = 4 and let d denote
the cityblock distance metric. What is the probability
density at observation o1?
</p>
<p>A Probability density at o1 ≈ 0.043
B Probability density at o1 ≈ 0.031
C Probability density at o1 ≈ 0.013
D Probability density at o1 ≈ 0.341
E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>21
</p>
<p>Association rule learning
</p>
<p>Association rule learning is the discovery of interesting relations between features in a dataset.
Suppose for instance we record the items that a large number of customers buy in a database, then
we might discover that people who often buy onions, tomatoes, and burger dressing are also likely
to buy hamburger meat. Automatic discovery of such rules is known as association mining and
is of large commercial interest since the invention of the efficient Apriori method for discovering
association rules in the early 90s [Agrawal et al., 1993, 1994] that, at the time of writing, has more
than 38 000 citations. In this chapter, we will discuss basic concepts relating to association rule
learning and introduce this popular method, the Apriori algorithm, for automatic rule discovery.
</p>
<p>21.1 Basic concepts
</p>
<p>We will still denote our dataset X as an N × M matrix and assume X is binary. Each row
(observation) of the matrix corresponds to a transaction and each column (attribute) to an item.
An example is shown in table 21.1 where we consider N = 5 transactions corresponding to M = 4
items. The canonical interpretation ofX in association rule learning is that each column corresponds
to a set of things people can buy and each of the N observations corresponds to a “shopping cart”
defining which items a given customer has bought. For instance if Xij = 1 then person i bought
</p>
<p>item j. Because of this interpretation, it is common to use set notation. Suppose xi =
[
0 1 1 0
</p>
<p>]T
.
</p>
<p>This will be written as the set:
ti = {I2, I3}
</p>
<p>to denote that transaction i corresponded to a person buying item I2 and I3 (butter and beer)
and we will write t1, . . . , tN for all transactions. Since we will use set notation somewhat often it is
perhaps a good idea to review some basic concepts. Suppose r = {I1, I2, I4} and s = {I2, I3, I4, I5}.
Then we denote the size by vertical bars such that |r| = 3 and |s| = 4. The intersection, i.e. the
elements in both sets, and the union, i.e. the elements in either of the sets are written as
</p>
<p>r ∩ s = {I2, I4}, and r ∪ s = {I1, I2, I3, I4, I5}.
</p>
<p>A special set is the empty set ∅ = { } and two sets r, u are disjoint if r ∩ u = ∅. For instance
{I1, I2, I4} ∩ {I3, I5} = ∅. Set membership (i.e. if an element is in the set) is written as x ∈ r. In</p>
<p></p>
</div>, <div class="page"><p></p>
<p>314 21 Association rule learning
</p>
<p>Table 21.1: Small example dataset for association mining.
</p>
<p>milk butter beer diapers
</p>
<p>1 0 1 1
0 1 0 1
0 1 1 1
0 0 1 0
1 0 1 1
</p>
<p>our case I3 ∈ r is false but I3 ∈ s is true. If all elements in a set c is contained in a set r this will
be written as c ⊆ t, for instance:
</p>
<p>{I2, I4} ⊆ r.
Finally, set difference, that is the operation where we remove the elements in one set from another,
is written as:
</p>
<p>r \ s = {I1}, and s \ r = {I3, I5}.
</p>
<p>21.1.1 Itemsets and association rules
</p>
<p>Returning to association mining, the set of all items will be written as
</p>
<p>I = {I1, I2, . . . , IM}. (21.1)
</p>
<p>Each transaction is then a subset of I. We will write T = {t1, . . . , tN} for the set of all transactions.
Notice, in set-notation, ti ⊆ I. With this notation in place, we can make our first real definition:
an itemset is simply a subset of I. Each transaction is an itemset, however, also c = {I3, I5} is an
itemset even though it is not a member of all transactions T . An association rule is written as
</p>
<p>X → Y, (21.2)
</p>
<p>where X,Y ⊆ I and X ∩ Y = ∅. The rule says that people who buy X will also tend to buy Y . For
instance, we can consider the rule
</p>
<p>{butter, beer} → {milk},
</p>
<p>which is saying that people who buy butter and beer will also tend to buy milk. What is a useful
rule? One definition is to say it should satisfy two criteria:
</p>
<p>High support It should be invoked fairly often, i.e. X ∪ Y should form a set of items many buy.
High confidence When the rule is triggered, i.e. someone buys X, then the person should be very
</p>
<p>likely to also buy Y .
</p>
<p>These quantities are measured by the support and confidence which, as we will see, are nothing
more than dressed-up probabilities.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>21.1 Basic concepts 315
</p>
<p>21.1.2 Support
</p>
<p>For an itemset X we can define a number, the support, which is the fraction of transactions which
contains the itemset X. Formally:
</p>
<p>supp(X) =
{Number of transactions containing X}
</p>
<p>N
</p>
<p>=
|{t ∈ T |X ⊆ t}|
</p>
<p>N
. (21.3)
</p>
<p>Notice the use of set-notation: {t ∈ T |X ⊆ t} is simply the set of transactions t in T such that X
is contained as a subset of t and provides a more formal way of defining sets which we will return
to later.
</p>
<p>When we talk about the support of an association rule X → Y , we mean the support of both
X and Y . In other words:
</p>
<p>supp(X → Y ) = supp(X ∪ Y ). (21.4)
</p>
<p>Let’s try a few examples based on the market basket data X having five transactions of four
items given in Table table 21.1. For instance:
</p>
<p>supp({I2, I3}) =
1
</p>
<p>5
, supp({I1, I3, I4}) =
</p>
<p>2
</p>
<p>5
, supp({I1, I2, I3, I4}) = 0, supp({I3, I4} → {I2}) =
</p>
<p>1
</p>
<p>5
.
</p>
<p>Support and probabilities
</p>
<p>It is important to stress the support is nothing but the empirical probability of the itemset. If for
instance X = {I3, I5} we can just as easily write:
</p>
<p>supp(X) = p(X) = p(I3 = 1, I5 = 1),
</p>
<p>which is just the probability that the third and fifth coordinate is 1.
</p>
<p>21.1.3 Confidence
</p>
<p>To quantify confidence we introduce the confidence of an association rule X → Y as the fraction
of transactions which contain X that also contains Y . Formally:
</p>
<p>conf(X → Y ) = supp(X ∪ Y )
supp(X)
</p>
<p>. (21.5)
</p>
<p>Once again, this is closely linked to probabilities, namely the conditional probability of Y given X:
</p>
<p>conf(X → Y ) = p(X ∪ Y )
p(X)
</p>
<p>= p(Y |X).
</p>
<p>To give a single example:
</p>
<p>conf({I3, I4} → {I2}) =
1
5
3
5
</p>
<p>=
1
</p>
<p>3
.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>316 21 Association rule learning
</p>
<p>Returning to our original problem, we are interested in association rules X → Y where the
support of the rule is higher than some pre-specified value and where the confidence too is higher
than another pre-specified value (or, equivalently, find sets Z = {X ∪ Y } which occur with high
probability and pairs of sets, X,Y , such that p(Y |X) is large). This ensures the rules are relevant
(high support) and that they are mostly true (confidence). However, finding rules with a high
support pose a difficult challenge. If for instance we suppose M = 1000 (which is a fairly low
number considering the number of items in an ordinary supermarket) and we are interested in all
itemsets involving k = 5 items there are
</p>
<p>M !
</p>
<p>(M − k)!k! ≈ 8.25× 10
12
</p>
<p>potential itemsets to check. In the next section, we will look at a method, the Apriori algorithm,
which makes this search much faster.
</p>
<p>21.2 The Apriori algorithm
</p>
<p>The Apriori algorithm is a way to discover association rules with high support. We assume we are
given N transactions of M items and a minimum support value 0 &lt; � ≤ 1. When a particular
itemset X has support higher than �, supp(X) ≥ �, we say X is frequent. A word of warning: the
Apriori algorithm may seem quite complicated especially on a first glance; however, it builds on a
very simple principles which we will discuss first.
</p>
<p>Itemsets have what is known as the downwards closure property. Downwards closure simply says
that if X is frequent, then so is any of its subsets. This is very easy to prove: If a transaction
contains X, then it also contains any subset A ⊂ X, and so
</p>
<p>A ⊂ X implies supp(A) = |{t ∈ T |A ⊂ t}|
N
</p>
<p>≥ |{t ∈ T |A ⊂ X ⊂ t}|
N
</p>
<p>= supp(X). (21.6)
</p>
<p>This implies that if A is not frequent (infrequent), then so is any set containing X. So how can this
principle allow us to find all frequent itemsets? Suppose we first compute the support of all itemsets
which only contain a single item {Ii} for i = 1, . . . ,M . If any of these are not frequent, then we
can discard any itemset which contains that element since (by the downwards closure principle)
it cannot be frequent. The idea is then to start with all (frequent) itemsets and iteratively find
frequent itemsets with k = 1, 2, . . . items. We let Lk denote the set of all frequent itemsets with k
elements. For instance
</p>
<p>L1 = {{i}|supp({i}) ≥ �} (21.7)
</p>
<p>The algorithm then at step k compute Lk from Lk−1 as follows:
</p>
<p>• First generate a set of candidate itemsets of size k, Ck, by adding all items to the itemsets in
Lk. Formally first define
</p>
<p>C ′k = {s ∪ {j}|s ∈ Lk−1, j /∈ s} (21.8)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>21.2 The Apriori algorithm 317
</p>
<p>Algorithm 9: Apriori algorithm
</p>
<p>1: Given N transactions and let � &gt; 0 be the minimum support count
2: L1 = {{j}|supp({j}) ≥ �}
3: for k = 2, . . . ,M and Lk 6= ∅ do
4: C′k = {s ∪ {j}|s ∈ Lk−1, j /∈ s}
5: Set Ck = C
</p>
<p>′
k
</p>
<p>6: for each c ∈ C′k do
7: for each s ⊂ c such that |s| = k − 1 do
8: if s is not frequent, i.e. s /∈ Lk−1 then
9: Ck = Ck \ {c} (Remove c from Ck)
</p>
<p>10: end if
11: end for
12: end for
13: Lk = {c|c ∈ Ck, supp(c) ≥ �} (compute support)
14: end for
15: L1 ∪ L2 ∪ · · · ∪ Lk are then all frequent itemsets
</p>
<p>Some of these itemsets can be known to be infrequent since they contain a subset of size k − 1
which is infrequent, i.e. is not in Lk−1. That an itemset c contains a subset not in Lk−1 can be
written as:
</p>
<p>{s|s ⊂ c, |s| = k − 1} ∩ Lk−1 = ∅. (21.9)
</p>
<p>Removing all these itemsets from Ck can therefore be written as:
</p>
<p>Ck = C
′
k \ {c|{s|s ⊂ c, |s| = k − 1} ∩ Lk−1 = ∅}. (21.10)
</p>
<p>This is quite a daunting expression and it is somewhat easier to comprehend what it does by
an example (which we will provide in a moment) or by breaking it into smaller steps. In line 5
to line 12 of algorithm 9 the expression eq. (21.10) is computed in a sequence of simpler steps
which the reader is invited to consult.
</p>
<p>• We then compute the support for each of the itemsets c ∈ Ck and let Lk be those candidates
in Ck with support greater than �.
</p>
<p>• The method terminates when Lk = ∅. All frequent itemsets are then L1 ∪ L2 ∪ · · · ∪ Lk
More explicitly stated the Apriori method is given in algorithm 9
</p>
<p>21.2.1 An example of the Apriori algorithm
</p>
<p>The Apriori algorithm may appear quite daunting, but it is much easier to understand the steps
by considering a concrete example. Suppose we set � = 0.15 and apply the Apriori algorithm to the
problem in table 21.1 to find all frequent itemsets. To be frequent if � = 0.15 means the itemset
must be found in at least 1 transactions (why? because 15 = 0.2 &gt; �).
</p>
<p>Initialization
</p>
<p>We first observe that L1 must contain all items because all items occur in at least one transaction.
We write this as</p>
<p></p>
</div>, <div class="page"><p></p>
<p>318 21 Association rule learning
</p>
<p>L1 =
</p>
<p>
1 · · ·
· 1 · ·
· · 1 ·
· · · 1
</p>
<p> .
First iteration, k = 2:
</p>
<p>The next step is to form C ′2, which is done by taking each element in L1 (for instance {I3}) and
then add each item which is not I3 to this element to get {I1, I3}, {I2, I3} and {I3, I4}. Doing this
we obtain:
</p>
<p>C ′2 =
</p>
<p>
1 1 · ·
1 · 1 ·
1 · · 1
· 1 1 ·
· 1 · 1
· · 1 1
</p>
<p> .
</p>
<p>So far so good. Now, to form C2 from C
′
2 involve the following steps: Start with the first element
</p>
<p>in C ′2, {I1, I2}. Then take each subset which has size k − 1 = 1, namely {I1} and {I2}. We next
check that these are in L1 (which they are) and therefore, because both {I1} and {I2} are in L1,
c = {I1, I2} remains in Ck. Proceeding this way we see that in fact C2 = C ′2 as shown above.
</p>
<p>Finally, we go over all itemsets in C2 and compute their support. We see that supp({I1, I2} = 0
and so this itemset is not included in L2, however, all other itemsets occur in at least 1 transaction
and are therefore accepted, therefore
</p>
<p>L2 =
</p>
<p>
1 · 1 ·
1 · · 1
· 1 1 ·
· 1 · 1
· · 1 1
</p>
<p> .
Second iteration, k = 3:
</p>
<p>In the second iteration, we first add all singleton sets to L2 to get the candidate sets C
′
3. For instance,
</p>
<p>starting with {I2, I3} ∈ L2 we obtain the candidate transactions {I1, I2, I3}, {I2, I3, I4} ∈ C ′3.
Ignoring duplicates this list is:
</p>
<p>C ′3 =
</p>
<p>
1 1 1 ·
1 · 1 1
1 1 · 1
· 1 1 1
</p>
<p> .
Next we proceed by the difficult rule in eq. (21.10). We first set C3 = C
</p>
<p>′
3 and start with the
</p>
<p>first transaction c = {I1, I2, I3}. We consider all subsets of c with one element removed, s =
{I1, I2}, {I2, I3}, {I1, I3} and notice that {I1, I2} is not in L2. Since this itemset is infrequent, we
know by the downwards closure property that c cannot be frequent and can therefore be dismissed
– which is why c = {I1, I2, I3} does not occur in C3. Doing this for all itemsets leave us with:
</p>
<p>C3 =
</p>
<p>[
1 · 1 1
· 1 1 1
</p>
<p>]
.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>21.3 Using the Apriori algorithm to find itemsets with high confidence 319
</p>
<p>Computing their support we notice:
</p>
<p>supp({I1, I3, I4}) =
2
</p>
<p>5
, and supp({I2, I3, I4}) =
</p>
<p>1
</p>
<p>5
</p>
<p>and therefore L3 =
</p>
<p>[
1 · 1 1
· 1 1 1
</p>
<p>]
.
</p>
<p>Third iteration, k = 4:
</p>
<p>Since L3 =
</p>
<p>[
1 · 1 1
· 1 1 1
</p>
<p>]
the third iteration is very simple. We form the itemset C ′4 =
</p>
<p>[
1 1 1 1
</p>
<p>]
and
</p>
<p>obtain in the next step that s = {I1, I2, I3}, {I1, I3, I4}, {I1, I2, I4}, {I2, I3, I4} needs to be checked.
Since only {I1, I3, I4}, {I2, I3, I4} ∈ L3 we get C4 = ∅ and therefore L4 = ∅. The algorithm therefore
terminates and we finally have:
</p>
<p>L =
</p>
<p>
</p>
<p>1 · · ·
· 1 · ·
· · 1 ·
· · · 1
1 · 1 ·
1 · · 1
· 1 1 ·
· 1 · 1
· · 1 1
1 · 1 1
· 1 1 1
</p>
<p>
</p>
<p>21.3 Using the Apriori algorithm to find itemsets with high confidence
</p>
<p>Finding association rules X → Y with high confidence can easily be solved using the Apriori
algorithm. In general, we want to find rules X → Y such that
</p>
<p>supp(X → Y ) = supp(X ∪ Y ) ≥ �
</p>
<p>conf(X → Y ) = supp(X ∪ Y )
supp(X)
</p>
<p>≥ δ
</p>
<p>From the last equation we obtain: 1δ supp(X ∪ Y ) ≥ supp(X). This suggests the following strategy:
We first use the Apriori method to generate all itemsets Z with support greater than �: supp(Z) ≥ �.
If � is chosen reasonably large, these itemsets will typically be fairly small, perhaps on the order of
about 5− 10 items.
</p>
<p>Given these itemsets, we can then search over all subsets X of Z and find those subsets where
supp(X) ≤ 1δ supp(Z). Then defining Y = Z \ X we are guaranteed the association rule X → Y
satisfy both the above requirements.
</p>
<p>This procedure can be speed up by noticing that if W ⊂ X, then supp(W ) ≥ supp(X). Thus, if
we have already seen that a particular set X has supp(X) &gt; 1δ supp(Z), there is no need to check
the subsets X ′ of X because they are guaranteed to have low confidence too. Accordingly, we should
consider an algorithm that starts by considering X = Z \ {i} for each i ∈ Z and only proceed to
remove further elements from each X if the corresponding association rule X → Y has confidence
greater than δ.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>320 21 Association rule learning
</p>
<p>21.4 Some limitations
</p>
<p>The Apriori algorithm is very efficient at identifying joint occurrences, i.e. joint probabilities that
are frequent by having high support. However, it is only possible to find associations with high confi-
dence within these itemsets identified to have high support, whereas there may be many other associ-
ation rules of high confidence. As such, if you were to buy lobster you are likely to also buy lemon and
white wine, however, as lobster is bought very infrequently the itemset {lobster, lemon,white wine}
will in general be below the support threshold � and such high-confidence rule missed by the algo-
rithm.
</p>
<p>Customers may also have different preferences and can typically be segmented into customer
types. It can therefore be useful to consider preferences within segments of consumers rather than in
terms of all transactions. The basic association mining framework presented here only considers all
transactions and therefore mix different costumer segments thereby potentially missing important
segment specific associations if these segments are not somehow identified prior to the analysis (for
instance using some clustering approach).
</p>
<p>While our initial motivation for association mining was the analysis of market baskets, i.e. the
transactions of customers and their items purchased, association mining has very general applicabil-
ity. For instance association mining can be used within medicine (i.e., identifying patterns such as if
you have these and these symptoms it is likely you also have these symptoms), bioinformatics (i.e.,
identifying association between genes), and general questionnaire data (i.e., if you answered this
and this it is likely you will also answer this) to mention but a few potential domains of application.
The Apriori algorithm assumes binary features whereas in many real applications, datasets may
not be binary and therefore some type of binarization must be invoked prior to the analyses. How
to binarize the data can be unclear whereas the binarization may influence results. For instance
if we binarize an attribute such as age we could simply threshold by the median value (i.e. 50th
percentile) or use one-out-of-K coding to include multiple binary features each denoting different
age interval. In this case, the binarization will heavily influence the support, i.e. using the median
we cannot have support � &gt; 50% and if we for instance split according to 10th percentiles using
1-out-of-10 coding we will obtain 10 new binary attributes from the original attribute age and in
general not be able to get support � &gt; 10%.
</p>
<p>Finally, care should be taken interpreting association rules as support and confidence are nothing
but joint and conditional probabilities respectively. Thus, similar to our discussion of Bayesian
networks in chapter 13 the arrows in association mining does not imply causality. As such, we have
for the example given in Table 21.1 that the confidence of the rule {beer} → {diapers} is 75%
whereas the confidence of the rule {diapers} → {beer} is also 75%.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>21.4 Some limitations 321
</p>
<p>Problems
</p>
<p>21.1. Spring 2014 question 11: We consider the
twelve costumers given in Table 21.2. We will consider
this data set a market basket problem in which the twelve
customers have various combinations of the six items de-
noted MH , ML, PH , PL, DH , DL. Which one of the pro-
posed solutions below includes all the frequent itemsets
with support of more than 40 %?
</p>
<p>MH ML PH PL DH DL
LIS1 1 0 0 1 1 0
LIS2 1 0 1 0 1 0
LIS3 0 1 0 1 0 1
LIS4 0 1 0 1 0 1
LIS5 1 0 1 0 0 1
LIS6 1 0 1 0 1 0
</p>
<p>OPO1 1 0 1 0 0 1
OPO2 0 1 0 1 1 0
OPO3 0 1 1 0 0 1
OPO4 0 1 0 1 0 1
OPO5 0 1 1 0 0 1
OPO6 1 0 1 0 1 0
</p>
<p>Table 21.2: Given are the six first costumers of Lisbon
and Oporto including whether these costumers spent
more or less than the median consumption of MILK
(MH , ML), PAPER (PH , PL), and DELI (DH , DL).
Subscript H and L are thus used to respectively denote a
relatively high and low level of consumption (i.e., above
or below the median consumption).
</p>
<p>A {ML}, {MH}, {PH}, {PL}, {DH}, {DL}.
B {ML}, {MH}, {PH}, {PL}, {DH}, {DL}, {MH , PH}.
C {ML}, {MH}, {PH}, {PL}, {DH}, {DL}, {MH , PH},
{ML, DL}.
</p>
<p>D {ML}, {MH}, {PH}, {PL}, {DH}, {DL}, {MH , PH},
{MH , DH}, {ML, PL}, {ML, DL}, {PL, DL}.
</p>
<p>E Don’t know.
</p>
<p>21.2. Spring 2014 question 12: Consider again the
dataset in Table 21.2. What is the confidence of the as-
sociation rule {ML, DL} → {PL}?
</p>
<p>A 25%
B 40%
C 60%
D 80%
E Don’t know.
</p>
<p>21.3. Fall 2013 question 20: We again consider the ten
subjects given in Table 21.3. We will consider this data
set a market basket problem where the customers are the
ten subjects and they have various combinations of the
six items denoted Y AY , Y AN , OAY , OAN , PAY , PAN .
Which one of the proposed solutions below includes all
the frequent itemsets with support of more than 52 %?
</p>
<p>Y AY Y AN OAY OAN PAY PAN
S1 1 0 1 0 1 0
S2 1 0 1 0 0 1
S3 0 1 0 1 1 0
S4 0 1 1 0 1 0
S5 0 1 1 0 1 0
</p>
<p>NS1 0 1 1 0 1 0
NS2 0 1 0 1 1 0
NS3 1 0 0 1 0 1
NS4 0 1 1 0 1 0
NS5 0 1 1 0 1 0
</p>
<p>Table 21.3: Given are five subjects that survived in
Haberman’s study (denoted S1, S2, . . ., S5) as well as the
five subjects that did not survive in Haberman’s study
(denoted NS1, NS2, . . ., NS5) including whether these
subjects are young or old (Y AY , Y AN ), were operated
after 1960 or not (OAY , OAN ), and had positive axillary
nodes or not (PAY , PAN ).
</p>
<p>A {Y AN},{OAY }, {PAY }.
B {Y AN},{OAY }, {PAY },
{Y AN , PAY }, {OAY , PAY }.
</p>
<p>C {Y AN},{OAY }, {PAY },
{Y AN , OAY }, {Y AN , PAY }, {OAY , PAY }.
</p>
<p>D {Y AN},{OAY }, {PAY },
{Y AN , OAY }, {Y AN , PAY }, {OAY , PAY },
{Y AN , OAY , PAY }.
</p>
<p>E Don’t know.</p>
<p></p>
</div>, <div class="page"><p></p>
</div>, <div class="page"><p></p>
<p>Solutions
</p>
<p>Problems of Chapter 2
</p>
<p>2.1 The correct answer is B: For both Age and PV there are a natural zero and we can apply
all the operators &lt;,&gt;,=, 6=, ∗, / thus these attributes are ratio. As Race, HT and UI are binary
categorical, i.e. =, 6= can be applied to them but not &lt;,&gt; these attributes are not ordinal. Age is
ratio but not continuous as the attribute is measured in whole years. Furthermore, MW is continuous
and PV is discrete.
</p>
<p>2.2 The correct answer is C: All the attributes are ratio since 0 means absence of what is
being measured. As the Plants and E-Plants both are based on counts they are discrete whereas
all remaining attributes are continuous and greater than zero as they quantify distances and areas
(which are non-negative quantities).
</p>
<p>2.3 The correct answer is D: x1, x2 are ratio, x3 is ordinal, x4 is nominal and x5 is interval.
Accordingly only option four is correct.
</p>
<p>Problems of Chapter 3
</p>
<p>3.1 The correct answer is A: CAL, PROT, FAT, FIB, SUG, POT, VIT, SHELF WEIGHT have
negative coefficients of PCA1 whereas TYPE, SOD, CARB, CUPS have positive coefficients result-
ing in a negative projection onto the first principal component. The magnitude of the coefficients
for PROT and SHELF are very small hence PCA2 does not primarily discriminate between low
values of PROT and high values of SHELF and high values of PROT and low values of SHELF
even though PROT has a small negative coefficient and SHELF a small positive coefficient. From
Figure 3.14 it can be seen that the projection of the data onto PCA2 is negatively correlated with
RAT as there is a strong tendency that small values of the projection onto PCA2 corresponds to
relatively high values of RAT whereas large values in the projection onto PCA2 results in relatively
low value of RAT. Finally, PCA1 and PCA2 will by definition always be orthogonal to each other
despite the preprocessing of the data.
</p>
<p>3.2 The correct answer is A: The first three principal components account for
σ21+σ
</p>
<p>2
2+σ
</p>
<p>2
3
</p>
<p>σ21+σ
2
2+σ
</p>
<p>2
3+σ
</p>
<p>2
4+σ
</p>
<p>2
5+σ
</p>
<p>2
6
</p>
<p>=
</p>
<p>61.3% of the variation. Thus, A is incorrect.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>324 Solutions
</p>
<p>3.3 The correct answer is D: The variation explained by each principal component is given by
σ2i∑
i′ σ
</p>
<p>2
i′
</p>
<p>. As such we find:
</p>
<p>V arExpPC1 = 9.7
2
</p>
<p>9.72+6.72+5.72+3.72+3.02+1.32+0.72
= 0.4792
</p>
<p>V arExpPC1− 3 = 9.7
2+6.72+5.72
</p>
<p>39.72+6.72+5.72+3.72+3.02+1.32+0.72
= 0.8733
</p>
<p>V arExpPC7 = 0.7
9.72+6.72+5.72+3.72+3.02+1.32+0.72
</p>
<p>= 0.0025
</p>
<p>V arExpPC1− 4 = 9.7
2+6.72+5.72+3.72
</p>
<p>9.72+6.72+5.72+3.72+3.02+1.32+0.72
= 0.9431
</p>
<p>V arExpPC1− 5 = 9.7
2+6.72+5.72+3.72+3.02
</p>
<p>9.72+6.72+5.72+3.72+3.02+1.32+0.72
= 0.9889
</p>
<p>As such the first PC accounts for less than 50% of the variance, the first three principal components
accounts for less than 90% whereas the last component accounts for less than 1%. As four compo-
nents are insufficient but five components sufficient to account for 95% of the variance the correct
answer is that five principal components are required in order to account for more than 95% of the
variation in the data.
</p>
<p>3.4 The correct answer is D: The terminology is taken from the appendix of Tan et. al. The
principal directions must be normalized and orthogonal leaving only C and D. It is however visually
clear the shape is both elongated along and symmetrical about the diagonal direction leaving option
D as the correct choice.
</p>
<p>3.5 The correct answer is B: The projection can be found by substracting the mean from X
and projecting onto the first two columns of V (see appendix B of Tan et al). The first point with
the mean subtracted has coordinates[
</p>
<p>3− 7/3 2− 4/3 1− 5/3
]
</p>
<p>This should be (left) multiplied with the first two columns of V :3− 7/32− 4/3
1− 5/3
</p>
<p>&gt; −0.99 −0.13−0.09 0.7
0.09 −0.7
</p>
<p> = [−0.78 0.85]
corresponding to option B.
</p>
<p>3.6 The correct answer is C: The variance explained by each principal component is given by
σ2i∑
i′ σ
</p>
<p>2
i′
</p>
<p>. As such we find:
</p>
<p>V arExpPC1 = 2.69
2
</p>
<p>2.692+2.532+1.052+0.832+0.492+0.312
= 0.459
</p>
<p>V arExpPC1− 3 = 2.69
2+2.532+1.052
</p>
<p>2.692+2.532+1.052+0.832+0.492+0.312
= 0.934
</p>
<p>V arExpPC5− 6 = 0.49
2+0.312
</p>
<p>2.692+2.532+1.052+0.832+0.492+0.312
= 0.021
</p>
<p>V arExpPC4 = 0.83
2
</p>
<p>2.692+2.532+1.052+0.832+0.492+0.312
= 0.0436
</p>
<p>As such the first PC accounts for more than 40% of the variance, the first three principal components
accounts for less than 95% whereas the last two component accounts for more than 2%, thus, this is
correct. As the fourth component accounts for 4.36% of the variance the last statement is incorrect.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Solutions 325
</p>
<p>Problems of Chapter 4
</p>
<p>4.1 The correct answer is C: Notice the definition of the cityblock distance corresponds to the
number of 01 and 10 matches between the binary vectors:
</p>
<p>f01 + f10 =
</p>
<p>M∑
k=1
</p>
<p>|xik − xjk|
</p>
<p>Then since f01 + f10 + f11 + f00 = M we can compute
</p>
<p>SMC(o1, o3) =
f11 + f00
</p>
<p>M
=
M − f01 − f10
</p>
<p>M
=
</p>
<p>8
</p>
<p>15
= 0.53
</p>
<p>The other options are not true, on the simulated dataset the correct options are: COS(o1, o3) =
0.565685424949, J(o1, o3) = 0.363636363636
</p>
<p>4.2 The correct answer is D: The three measures of similarity is given by
</p>
<p>J(x,y) =
f11
</p>
<p>f01 + f10 + f11
</p>
<p>SMC(x,y) =
f00 + f11
</p>
<p>f01 + f10 + f11 + f00
</p>
<p>cos(x,y) =
x · y
‖x‖‖y‖
</p>
<p>We now have
</p>
<p>J(NS1,NS2) =
1
</p>
<p>7
</p>
<p>SMC(NS1,NS2) =
2
</p>
<p>8
=
</p>
<p>1
</p>
<p>4
</p>
<p>cos(NS4,NS5) =
2
</p>
<p>2 · 2 =
1
</p>
<p>2
</p>
<p>SMC(NS5,AS5) =
6
</p>
<p>8
=
</p>
<p>3
</p>
<p>4
</p>
<p>J(NS5,AS5) =
3
</p>
<p>5
</p>
<p>cos(NS5,AS5) =
3
</p>
<p>4
</p>
<p>Hence the correct answer is cos(NS5,AS5) = 34 .
</p>
<p>4.3 The correct answer is D: The three measures of similarity is given by
</p>
<p>J(x,y) =
f11
</p>
<p>f01 + f10 + f11
</p>
<p>SMC(x,y) =
f + f11
</p>
<p>f01 + f10 + f11 + f
</p>
<p>cos(x,y) =
x · y
‖x‖‖y‖</p>
<p></p>
</div>, <div class="page"><p></p>
<p>326 Solutions
</p>
<p>We now have
</p>
<p>J(S1,S2) =
2
</p>
<p>4
=
</p>
<p>1
</p>
<p>2
,
</p>
<p>J(S1,NS1) =
2
</p>
<p>4
=
</p>
<p>1
</p>
<p>2
,
</p>
<p>SMC(S1,S2) =
4
</p>
<p>6
=
</p>
<p>2
</p>
<p>3
,
</p>
<p>SMC(S1,NS1) =
4
</p>
<p>6
=
</p>
<p>2
</p>
<p>3
,
</p>
<p>cos(S1,S2) =
2√
3
√
</p>
<p>3
=
</p>
<p>2
</p>
<p>3
,
</p>
<p>cos(S1,NS1) =
2√
3
√
</p>
<p>3
=
</p>
<p>2
</p>
<p>3
.
</p>
<p>Hence, the correct answer is SMC(S1, S2) = cos(S1, S2).
</p>
<p>Problems of Chapter ??
</p>
<p>6.1 The correct answer is C: The trick to solving the problem is to not use Bayes theorem. If
we introducing appropriate shorthand the following relations hold from basic probability theory:
</p>
<p>p(B|H) = 1− p(R|H)
</p>
<p>p(R|H) = p(H,R)
p(H)
</p>
<p>p(H) = p(H|4)p(4) + p(H|2)(1− p(4))
</p>
<p>Plugging in the information we then have
</p>
<p>p(4) = p(2) =
1
</p>
<p>2
</p>
<p>p(H) =
1
</p>
<p>2
(0.8 + 0.2) = 0.5
</p>
<p>p(R|H) = 0.1
0.5
</p>
<p>= 0.2
</p>
<p>From which it follows p(B|H) = 1− 0.2 = 0.8
6.2 The correct answer is C: We will let NS denote normal semen and AS denote abnormal
semen whereas CDY and CDN will indicate having had a childhood disease and not having had a
childhood disease respectively. We now find using Bayes theorem:
</p>
<p>P (NS|CDY ) = P (CDY |NS)P (NS)P (CDY )
= P (CDY |NS)P (NS)P (CDY |NS)P (NS)+P (CDY |AS)P (AS)
</p>
<p>= 0.125·0.880.125·0.88+0.167·0.12
</p>
<p>= 0.8459</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Solutions 327
</p>
<p>6.3 The correct answer is D: We will let PA denote positive axillary nodes detected and NA
denote no positive axillary nodes detected whereas SY and SN will indicate having survived and
not having survived respectively. We now find using Bayes theorem:
</p>
<p>P (PA|SY ) = P (SY |PA)P (PA)P (SY )
= P (SY |PA)P (PA)P (SY |PA)P (PA)+P (SY |NA)P (NA)
</p>
<p>= 0.36·0.560.36·0.56+0.14·0.44
</p>
<p>= 0.766 ≈ 76.6%
</p>
<p>Problems of Chapter 7
</p>
<p>7.1 The correct answer is B: The 25th and 50th percentile but not the 50th and 75th percentiles
of the attribute DB coincides. AlA and AsA will not necessarily be highly correlated even though
their distributions may have a similar shape (hence, this is correct). For attributes to be correlated
it is important they take on high or low values systematically, however, this can not be inspected
in a boxplot. TB is not likely to be normal distribution as this attribute does not have a symmetric
but highly right skewed distribution. The attribute GDR does not have a clear outlier, in fact the
outlier corresponds to the females in the dataset and all we can deduce from the plot is that more
than 75 % of the observations are males.
</p>
<p>7.2 The correct answer is A: The solution can be obtained by first observing the median of
the dataset is 1, leaving option A and B, then noticing the 10 observations taking the value 3 is
10/60 ≈ 17% of the dataset and since the top-most whisker must be at the 90th percentile according
to Tan fig. 3.12 this leave option A.
</p>
<p>7.3 The correct answer is D: Even though there are observations marked as outliers these should
not necessarily be removed. None of the attributes appear to be normal distribution but have a
skewed distribution. As a result, for all the attribute the mean value will be larger and somewhat
different from the median value of the data. Only if we standardize the data each attribute is
expected to be given equal importance in the PCA.
</p>
<p>Problems of Chapter 8
</p>
<p>8.1 The correct answer is C: WINDDIR and HOUR are not part of the model and thus irrelevant.
As logCAR has a positive coefficient of 0.36 and WIND a negative coefficient of -0.19 fewer cars and
more wind will decrease the pollution level. As x7 has a positive coefficient pollution is according to
the model increasing over time and not decreasing. As x2 has a negative coefficient of -0.01 higher
temperatures will result in lower pollution levels according to the model.
</p>
<p>8.2 The correct answer is A: Since both x1, x2 and x6 have negative coefficients large values of
these attributes will in general make the model predict the consumer is from Lisbon. Since the offset
is negative this implies that if a costumer after the standardization has x1 = x2 = x3 = x4 = x5 =
x6 = 0 the customer is more likely to come from Lisbon than Oporto as the offset x0 = −0.51. As
y = 1 denotes the consumer is from Oporto the logit function will return the probability a person</p>
<p></p>
</div>, <div class="page"><p></p>
<p>328 Solutions
</p>
<p>is from Oporto. Even though the coefficients of FRESH and PAPER are the smallest they may
still contribute in the predictions and we can not from this analysis conclude that they should be
removed from the modeling. This would require comparing the test performance including FRESH
and PAPER to not including these two attributes.
</p>
<p>8.3 The correct answer is B: It is not reasonable to say AreaNI is irrelevant as it is included in
the model with a non-zero coefficient and as seen from the boxplot in Figure 8.10 the attribute has a
large range compared to the other attributes. However, as the coefficient of x5 pertaining to DistNI
is negative this implies that short distances to neighboring island would result in a prediction of
larger area than large distances to the neighboring island would and this is hence correct. Even
though the coefficient for x2 number of endemic plants has the largest magnitude coefficient the
range of this attribute is rather limited as seen from Figure 8.10 and it is thus not reasonable to
say this is the most important attribute for predicting the Area. As the coefficients in front of x4
is positive and x6 negative an island that is highly elevated and close to Santa Cruz Island will in
general be predicted to be relatively large.
</p>
<p>8.4 The correct answer is A: First observe the value of the linear regression function at (x1, x2) =
(0, 0) is
</p>
<p>1
</p>
<p>1 + e−w0
</p>
<p>This gives for option A,D and B,C respectively:
</p>
<p>A,D :
1
</p>
<p>1 + e−w0
= (1 + e−2)−1 = 0.88
</p>
<p>B,C :
1
</p>
<p>1 + e−w0
= (1 + e−(−2))−1 = 0.12
</p>
<p>Inspecting any of the two figures clearly indicate we can rule out B,C leaving A,D. However
consider option D and the point (1, 1). Then we have a density estimate of
</p>
<p>1
</p>
<p>1 + e−(2+1+1−10)
=
</p>
<p>1
</p>
<p>1 + e6
≈ 0.0025.
</p>
<p>Since this should correspond to the “high-density” corner we are left with option A.
</p>
<p>Problems of Chapter 9
</p>
<p>9.1 The correct answer is C: The impurity gain is given by
</p>
<p>∆ = I(parent)−
2∑
j=1
</p>
<p>N(vj)
</p>
<p>N
I(vj),
</p>
<p>where
</p>
<p>I(t) = 1−
C−1∑
i=0
</p>
<p>p(i|t)2.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Solutions 329
</p>
<p>Inserting for the split defined by the PAN attribute we obtain:
</p>
<p>∆ =(1− (( 81306 )2 + ( 225306 )2))
−[ 170306 (1− (( 62170 )2 + ( 108170 )2))
+ 136306 (1− (( 19136 )2 + ( 117136 )2))] = 0.025.
</p>
<p>9.2 The correct answer is A: The two classes would be well separagted by the decisions
</p>
<p>A=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.25
</p>
<p>]
‖∞ &lt; 0.25
</p>
<p>B=‖
[
x1
x2
</p>
<p>]
−
[
</p>
<p>0.5
0.5
</p>
<p>]
‖2 &lt; 0.25.
</p>
<p>Decision A would capture the red crosses close to (0.25,0.5). Decision B will separate the circular
shape of red crosses in the upper middle from all the remaining observations that are black circles.
</p>
<p>9.3 The correct answer is B: The relevant definitions can be found in section 4.3 of the textbook.
The split at 2.5 divide the set X into two parts, the lower and upper part. We also need the
frequencies for all the data. For each of these we can compute the frequencies of the three classes:
</p>
<p>all :p(0|a) = 3/7, p(1|a) = 1/7, p(2|a) = 3/7
lower :p(0|l) = 3/5, p(1|l) = 0/5, p(2|l) = 2/5
upper :p(0|u) = 1/2, p(1|u) = 0/2, p(2|u) = 1/2
</p>
<p>From this we can compute the impurity: I(x) = 1−maxi p(i|x)
</p>
<p>all :I(a) = 1− 3/7 = 4/7.
lower :I(l) = 1− 3/5 = 2/5.
upper :I(u) = 1− 1/2 = 1/2.
</p>
<p>Then combining these we have
</p>
<p>∆ = I(a)− (5/7)I(l)− (2/7)I(u)
= 4/7− (5/7)(2/5)− (2/7)(1/2)
</p>
<p>=
1
</p>
<p>7
(4− 2− 1) = 1
</p>
<p>7
</p>
<p>Which is roughly ∆ ≈ 0.143.
9.4 The correct answer is A: Trying to evaluate the trees in the corners (−1, 1) and (1,−1)
rule out all but option A and C. Then considering the corner of the upper-left square must be “cut
of” by the circle allow one to rule out C leaving the correct choice A; alternatively evaluate the
classifiers at (−0.2, 0.2) also show C cannot be correct.
</p>
<p>Problems of Chapter 10
</p>
<p>10.1 The correct answer is D: The dataset is first split into two datasets of size</p>
<p></p>
</div>, <div class="page"><p></p>
<p>330 Solutions
</p>
<p>Dcv = 800, Dval = 200.
</p>
<p>Focusing on the cross validation, for each of the L = 6 values of λ we need to evaluate K = 10
cross-validation splits into datasets of size
</p>
<p>Dcv−train = 720, Dcv−test = 80
</p>
<p>The time taken for this is
Tcv = LK(D
</p>
<p>2
cv−train + 1/2D
</p>
<p>2
cv−test)
</p>
<p>This gives us the optimal value of λ. To estimate the performance we need to test and train one
final model. This takes:
</p>
<p>Tval = (D
2
cv + 1/2D
</p>
<p>2
val)
</p>
<p>The total time elapsed is then
</p>
<p>T = Tcv + Tval = 31.956 · 106.
</p>
<p>10.2 The correct answer is B: Using forward and backward selection we would like to minimize
the test error. Thus, the forward selection would first select x3 having lowest test error and subse-
quently x1 decreasing the test error the most in combination with x3. Subsequently, x2 is selected
as this has minimal test error in combination with x1 and x3. Selecting also x4 does not improve
the test error hence the selection procedure will terminate when x1, x2, and x3 are selected. The
backward feature selection will remove feature x3 to minimize the test error but removing additional
features will increase the test error hence the backward selection strategy will terminate with the
features x1, x2 and x4 being selected which is also the optimal feature combination for this data.
Thus, backward selection will indeed result in a better model being selected than using forward
selection.
</p>
<p>10.3 The correct answer is D: For least squares linear regression the test error will not always
decrease as we include more attributes in the model. We may indeed overfit to the data. However
the training error will monotonically decrease.
</p>
<p>10.4 The correct answer is D: Using forward and backward selection we would like to minimize
the test error. Thus, the forward selection would first select x5 having lowest test error and sub-
sequently x2 decreasing the test error the most in combination with x5. Further selecting features
in combination with x2 and x5 does not improve the test error thus the model will terminate. The
backward feature selection will remove feature x5 to minimize the test error and subsequently x1
which constitutes the optimal feature configuration for the problem. Thus, removing additional
features will not increase the test error hence the backward selection strategy will terminate with
the features x2 and x6.
</p>
<p>10.5 The correct answer is B: The error rate and accuracy for each of the classifiers is given by:
Logistic regression:
error rate= 10+69306 =
</p>
<p>79
306 ,
</p>
<p>accuracy= 215+12306 =
227
306
</p>
<p>Decision tree:
error rate= 34+55306 =
</p>
<p>89
306 ,
</p>
<p>accuracy= 191+26306 =
217
306 .
</p>
<p>Thus, the error rate for logistic regression is smaller and the accuracy higher than the decision tree</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Solutions 331
</p>
<p>classifier. As there are 225 subjects that died and only 81 subjects that survived the classes are
imbalanced. Thus, predicting everything to be in the largest class (died) would give an accuracy of
225
306 which is larger than the accuracy obtained by the decision tree classifier.
</p>
<p>10.6 The correct answer is B: Recall the accuracy is defined as (Tan 4.2)
</p>
<p>acc =
f00 + f11
</p>
<p>N
</p>
<p>i.e. the sum of true negatives and positives divided by the total number of observations. Since We
can count the number of true negatives and positives as a function of θ to get
</p>
<p>θ = 0.35 : f00 = 1, f11 = 2
</p>
<p>θ = 0.45 : f00 = 2, f11 = 2
</p>
<p>θ = 0.55 : f00 = 2, f11 = 1
</p>
<p>θ = 0.55 : f00 = 2, f11 = 0
</p>
<p>So the highest accuracy of The highest accuracy is (2 + 2)/5 = 4/5 = 0.8 is obtained at θ = 0.45
</p>
<p>10.7 The correct answer is B: 2-fold cross-validation is not the same as the hold method
where 50% is hold out as two models are trained and evaluated on all the data by two-fold cross-
validation whereas hold-out 50% only trains one model and evaluate the performance of this model
on half the data. For a very small dataset it is better to use leave-one-out cross validation as this
will keep as much data for training as possible. Only one level of cross-validation is needed for
tuning model parameters. Two levels are used when quantifying performance of the model with
parameters selected. Leave-one-out cross-validation is computationally expensive since as many
models as observations needs to be trained.
</p>
<p>10.8 The correct answer is B: Firstly notice the training error column can be disregarded.
Forward selection then first selects x2, then x2, x4, then x2, x3, x4 and finally x1, x2, x3, x4. Backward
selection however start with x1, x2, x3, x4, then disregards x2 to form x1, x3, x4 and terminates.
</p>
<p>Since the test error for forward selection is 25 and for backward selection 15 only option B is
correct.
</p>
<p>Problems of Chapter 12
</p>
<p>12.1 The correct answer is B: O1-04 are all closest to each other than to O5-O8 and will thus
be correctly classified. O5 is closest to O7, O2 and O4 and will thus be misclassified. O6 is closest
to O1, O4 and O3 and will thus be misclassified. O7 is closest to O5, O8 and O2 and will thus be
correctly classified and O8 is closest to O7, O5 and O4 and is thereby also correctly classified. As
two observations will be misclassified the error rate will be 2/8=1/4.
</p>
<p>12.2 The correct answer is D: Since there are 7 observations, K = 7 must classify everything to
the largest class and so k2 = 7. Next, for K = 1 the ticks must be colored correctly and so k3 = 1,
however by checking the left-most part of the k4-pane it is easy to see k4 = 5. This leaves option 4.
</p>
<p>12.3 The correct answer is A: The true accuracy is 0.125 or 1/8. This is easy to see by going
through table 12.2 and observing all observations except o1 is misclassified</p>
<p></p>
</div>, <div class="page"><p></p>
<p>332 Solutions
</p>
<p>Problems of Chapter 13
</p>
<p>13.1 The correct answer is B: According to the Näıve Bayes classifier we have
</p>
<p>P (CKD = 1|RBC = 1, PC = 1, DM = 1, CAD = 1) =
P (RBC = 1|CKD = 1)×
P (PC = 1|CKD = 1)×
P (DM = 1|CKD = 1)×
P (CAD = 1|CKD = 1)×
</p>
<p>P (CKD = 1)
</p>
<p>

P (RBC = 1|CKD = 1)×
P (PC = 1|CKD = 1)×
P (DM = 1|CKD = 1)×
P (CAD = 1|CKD = 1)×
</p>
<p>P (CKD = 1)
</p>
<p>+

P (RBC = 1|CKD = 0)×
P (PC = 1|CKD = 0)×
P (DM = 1|CKD = 0)×
P (CAD = 1|CKD = 0)×
</p>
<p>P (CKD = 0)
</p>
<p>
=
</p>
<p>2/9 · 7/9 · 6/9 · 1/9 · 9/15
2/9 · 7/9 · 6/9 · 1/9 · 9/15 + 1/6 · 1/6 · 1/6 · 1/6 · 6/15 = 0.9614.
</p>
<p>13.2 The correct answer is D: According to the Bayes classifier we have
</p>
<p>P (CKD = 1|RBC = 1, PC = 1, DM = 1) =(
P (RBC = 1, PC = 1, DM = 1|CKD = 1)×
</p>
<p>P (CKD = 1)
</p>
<p>)
(
P (PRBC = 1, PC = 1, DM = 1|CKD = 1)×
</p>
<p>P (CKD = 1)
</p>
<p>)
+
</p>
<p>(
P (RBC = 1, PC = 1, DM = 1|CKD = 0)×
</p>
<p>P (CKD = 0)
</p>
<p>)
=
</p>
<p>2/9 · 9/15
2/9 · 9/15 + 0/6 · 6/15 = 1
</p>
<p>13.3 The correct answer is D: According to the Näıve Bayes classifier we have
</p>
<p>P (S|Y AY = 1, OAY = 1, PAY = 1) =
P (Y AY = 1 = 1|S)×
</p>
<p>P (OAY = 1|S)×
P (PAY = 1|S)×
</p>
<p>P (S)
</p>
<p>

P (Y AY = 1 = 1|S)×
</p>
<p>P (OAY = 1|S)×
P (PAY = 1|S)×
</p>
<p>P (S)
</p>
<p>+

P (Y AY = 1 = 1|NS)×
</p>
<p>P (OAY = 1|NS)×
P (PAY = 1|NS)×
</p>
<p>P (NS)
</p>
<p>
=
</p>
<p>2/5 · 4/5 · 4/5 · 5/10
2/5 · 4/5 · 4/5 · 5/10 + 1/5 · 3/5 · 4/5 · 5/10 =
</p>
<p>8
</p>
<p>11
.
</p>
<p>13.4 The correct answer is A: True answer is: 0.83. This can be found by computing the per-class
probabilities</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Solutions 333
</p>
<p>p(f1 = 0|C1) = 3/5, p(f2 = 1|C1) = 1
p(f1 = 0|C2) = 1/5, p(f2 = 1|C2) = 3/5
</p>
<p>The class label priors is p(C1) = p(C2) =
1
2 and so the Naive-Bayes estimate is
</p>
<p>pNB(C1|f1 = 0, f2 = 1) =
p(f1 = 0|C1)p(f2 = 1|C1)p(C1)
</p>
<p>p(f1 = 0|C1)p(f2 = 1|C1)p(C1) + p(f1 = 0|C2)p(f2 = 1|C2)p(C2)
</p>
<p>=
3/5
</p>
<p>3/5 + (1/5)(3/5)
=
</p>
<p>5
</p>
<p>6
</p>
<p>Problems of Chapter 14
</p>
<p>14.1 The correct answer is B: The aim of regularized least squares regression is to reduce the
model’s variance without introducing too much bias and not the reverse. Linear regression with
transformed inputs can indeed model nonlinear relations, as the inputs may by the transformation
be non-linearly transformed. It is useful to plot attributes vs. residuals to investigate non-linear
relationships between each attribute and the output that is not presently accounted for by the
model and forward selection can both be used for regression and classification problems.
</p>
<p>Problems of Chapter 15
</p>
<p>15.1 The correct answer is C: The aim of regularized leats squares regression is to reduce the
model’s variance without introducing too much bias and not the reverse. The regularization strength
is normally chosen to be the value that minimize the error on the test set using cross-validation.
Artificial neural networks with linear transfer functions can indeed be written in terms of a linear
regression model. However, forward and backward selection uses the test-error and not the training
error to determine which attributes to remove or select.
</p>
<p>15.2 The correct answer is A: To compute the output, initialize n1 = f(1) = 1, n2 = f(2) = 2.
Then we can compute:
</p>
<p>n3 = f(1 ∗ 0.5 + 2 ∗ (−0.4)) = f(−3/10) = 0
n4 = f(1 ∗ 0.4 + 2 ∗ 0) = f(0.4) = 0.4
</p>
<p>Then for the output we have
</p>
<p>ŷ = n5 = f(n3 ∗ (−0.4) + n4 ∗ 0.1) = f(0.04) = 0.04.
</p>
<p>And so the correct output is ŷ = 0.04.
</p>
<p>15.3 The correct answer is D: Classifier 1 has a decision boundary defined by one linear boundary
and is thus based on logistic regression whereas classifier 2 has a very complicated decision boundary
that is defined in terms of the observation that is the closest and is therefore based on the 1-nearest
neighbor classifier. The decision boundary of classifier 3 is based on horizontal and vertical lines</p>
<p></p>
</div>, <div class="page"><p></p>
<p>334 Solutions
</p>
<p>and thus is based on the decision tree classifier. Classifier four has smooth but non-linear decision
boundaries and is thus based on the artificial neural network.
</p>
<p>15.4 The correct answer is C: It is apparent the decision boundary which best match a 1NN
classifier is P3; this rule out all but option C.
</p>
<p>Problems of Chapter 16
</p>
<p>16.1 The correct answer is C: By coarse inspection, the point (0.1, 0.95) lies on the ROC curve.
This corresponds to a FPR of 0.1 and a TPR of 0.95. Automatically this rules out the instances
where the red curve is higher than the black, and by inspection it can be seen to roughly correspond
to θ = 0.1 in figure C (see Tan et. al. chapter 5.7).
</p>
<p>16.2 The correct answer is B: The false positive rate is
</p>
<p>FPR =
FP
</p>
<p>TN + FP
</p>
<p>Thus we can find the number of false positives as
</p>
<p>FP =
FPR× TN
</p>
<p>1− FPR ≈ 96.0
</p>
<p>Notice that
</p>
<p>TPR =
TP
</p>
<p>TP + FN
</p>
<p>N = (TP + FN) + (TN + FP )
</p>
<p>Then
</p>
<p>TPR =
TP
</p>
<p>N − (TN + FP )
</p>
<p>which implies TP = TPR× (N − TN − FP ) ≈ 171.0
16.3 The correct answer is A: The precision (p) and recall (r) as well as true negative rate
(TNR) and false positive rate (FPR) is given by: Logistic regression:
</p>
<p>p =
12
</p>
<p>12 + 10
=
</p>
<p>6
</p>
<p>11
, (21.11)
</p>
<p>r =
12
</p>
<p>12 + 69
=
</p>
<p>12
</p>
<p>81
, (21.12)
</p>
<p>TNR =
215
</p>
<p>215 + 10
=
</p>
<p>43
</p>
<p>45
, (21.13)
</p>
<p>FPR =
10
</p>
<p>215 + 10
=
</p>
<p>2
</p>
<p>45
. (21.14)
</p>
<p>Decision tree:</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Solutions 335
</p>
<p>p =
26
</p>
<p>26 + 34
=
</p>
<p>13
</p>
<p>30
, (21.15)
</p>
<p>r =
26
</p>
<p>26 + 55
=
</p>
<p>26
</p>
<p>81
, (21.16)
</p>
<p>TNR =
191
</p>
<p>191 + 34
=
</p>
<p>191
</p>
<p>225
, (21.17)
</p>
<p>FPR =
34
</p>
<p>191 + 34
=
</p>
<p>34
</p>
<p>225
. (21.18)
</p>
<p>Thus, the precision of the logistic regression classifier is indeed higher than the precision of the
decision tree classifier whereas the remaining statements are incorrect.
</p>
<p>Problems of Chapter 17
</p>
<p>17.1 The correct answer is B: There is one misclassified observation and so ε1 = 1/4 and
α1 =
</p>
<p>1
2 log
</p>
<p>1−ε1
ε1
</p>
<p>= 0.5493. Then the un-normalized weights become:
</p>
<p>w =
[
e−α1 eα1 e−α1 e−α1
</p>
<p>]
=
[
</p>
<p>1√
3
</p>
<p>√
3 1√
</p>
<p>3
1√
3
</p>
<p>]
normalizing gives:
</p>
<p>w =
1
</p>
<p>6
</p>
<p>[
1 3 1 1
</p>
<p>]
and so option B is the correct.
</p>
<p>17.2 The correct answer is B: In boosting miss-classified observations are indeed given more
importance in the next round. Bagging sample with replacement such that the same observation can
be included multiple times within a round and hence some observations are not included. Boosting
does not use leave-one-out cross-validation to learn which observation to sample in the next round.
When combining classifiers in bagging this is attained by majority voting.
</p>
<p>Problems of Chapter 18
</p>
<p>18.1 The correct answer is D: In single linkage the observations that is the closest between the
clusters define the level in which they merge. As such O1 and O4 are the closest to each other with a
distance of 0.96. Then O2 and O3 merge at 1.15 and subsequently 05 and 07 at 1.41. 01 and 04 next
merge with 02 and 03 at 1.52. As the minimum distance between clusters now become the distance
between 05 and 07 the cluster O1, O2, O3, O4 merge with 05, 07 at 2.66. Thus dendrogram 1 and
2 are incorrect. As 08 merge having a distance to O7 of 2.88 whereas 06 has a minimal distance to
O1 of 4.07 dendrogram 4 is correct and dendrogram 3 incorrect.
</p>
<p>18.2 The correct answer is A: Running the K-means method gives that at first iteration we
have clusters
</p>
<p>(3) (6, 7, 9, 10) (11, 14)
</p>
<p>Cluster centers are then
µ1 = 3, µ2 = 8, µ3 = 12.5</p>
<p></p>
</div>, <div class="page"><p></p>
<p>336 Solutions
</p>
<p>New clusters are then
(3) (6, 7, 9, 10) (11, 14)
</p>
<p>and so the method has converged.
</p>
<p>18.3 The correct answer is D: The true answer is D, dendrogram 4. This is easy to see by for
instance comparing the height at which observations 2 and 6 are linked to the true answer in the
table.
</p>
<p>18.4 The correct answer is D: purity = 48 · 34 + 28 · 12 + 18 · 1 + 18 · 1 = 34
</p>
<p>Problems of Chapter 19
</p>
<p>19.1 The correct answer is B: Answer option B is the correct answer as: Cluster located with
center at (15,15) has much more spread in the x2 direction (i.e., variance 15) than the x1 direction
(i.e., variance 0.5). Furthermore, the cluster located at (30,0) has positive covariance and the cluster
located at (0,0) negative covariance. These properties only holds for answer option B.
</p>
<p>19.2 The correct answer is B: k-means and Gaussian Mixture Models (GMM) are dependent
on initialization. The dendrogram height is indeed determined by proximity and linkage function.
The update of centers in k-means depends on distance measure, i.e. Euclidean distance results in
updating centers to be the average observation whereas the 1-norm results in centers updated to be
the median value. A Gaussian Mixture Model with diagonal covariance matrix does not have the
same number of free parameters as k-means as each center has M parameters and the GMM also
includes M parameters to define the covariance of each cluster as well as a parameter defining the
size, thus resulting in 2KM + K − 1 parameters in total (-1 due to the sum to one constraint of
the parameter defining the relative size) whereas k-means would have KM parameters.
</p>
<p>19.3 The correct answer is C: The two mixture components have similar shape but differ in
their weight. This leaves options B and C. Since they are elongated in the horizontal direction then
σ21 = σ
</p>
<p>2
2 &gt; δ
</p>
<p>2
1 = δ
</p>
<p>2
2 , ruling out option B. This leaves the last option. The correct values are in fact:
</p>
<p>w1 = 0.7, σ
2
i = 20, δ
</p>
<p>2
i = 1
</p>
<p>Problems of Chapter 20
</p>
<p>20.1 The correct answer is C: According to the Gaussian kernel density estimator:
</p>
<p>p(x) =
1
</p>
<p>N
</p>
<p>N∑
n=1
</p>
<p>1
</p>
<p>(2π)M/2
√
|σ2I|
</p>
<p>exp(− 12 (x− xn)
&gt;
(
1
</p>
<p>σ2
I)(x− xn))
</p>
<p>We find for σ2 = 1
</p>
<p>p(x) =
1
</p>
<p>7
</p>
<p>7∑
n=1
</p>
<p>1
</p>
<p>(2π)M/2
√
|σ2I|
</p>
<p>exp(− 12‖x− xn)‖
2
2)
</p>
<p>We thus find</p>
<p></p>
</div>, <div class="page"><p></p>
<p>Solutions 337
</p>
<p>p(O8) =
1
</p>
<p>7
</p>
<p>7∑
n=1
</p>
<p>1
</p>
<p>(2π)M/2
√
σ2M
</p>
<p>exp(− 12‖xO8 − xOn)‖22)
</p>
<p>=
1
</p>
<p>7 ∗ (2π)M/2 (exp(−
5.112
</p>
<p>2 ) + exp(− 4.79
2
</p>
<p>2 )
</p>
<p>+ exp(− 4.9022 ) + exp(− 4.74
2
</p>
<p>2 ) + exp(− 2.96
2
</p>
<p>2 )
</p>
<p>+ exp(− 5.1622 ) + exp(− 2.88
2
</p>
<p>2 )) = 6.5 · 10−6
</p>
<p>20.2 The correct answer is D:
</p>
<p>density(xO1, 2) = (
1
</p>
<p>2
· (68.1 + 165.4))−1 = 0.0086
</p>
<p>density(xO3, 2) = (
1
</p>
<p>2
· (68.1 + 111.1))−1 = 0.0112
</p>
<p>density(xO4, 2) = (
1
</p>
<p>2
· (32.5 + 44.7))−1 = 0.0259
</p>
<p>a.r.d.(x,K) =
density(xO1, 2)
</p>
<p>1
2 (density(xO3, 2) + density(xO4, 2))
</p>
<p>=
0.0086
</p>
<p>1
2 · (0.0112 + 0.0259)
</p>
<p>= 0.46
</p>
<p>20.3 The correct answer is A: The z-score is defined as
</p>
<p>zn =
xn −mean(x)
</p>
<p>std(x)
=
xn − 6
</p>
<p>28
.
</p>
<p>The z-scores for all data objects are given by (rounded to one decimal)
</p>
<p>n 1 2 3 4 5 6 7 8
|zn| 0.3 0.4 0.5 0.2 2.5 0.4 0.3 0.4 .
</p>
<p>Thus, according to the definition, no objects are judged to be outliers, since no z-score has an
absolute value greater than 3.00.
</p>
<p>20.4 The correct answer is C:
</p>
<p>density(xO8, 3) = (
1
</p>
<p>3
· (2.88 + 2.96 + 4.74))−1 = 0.2836
</p>
<p>density(xO7, 3) = (
1
</p>
<p>3
· (1.41 + 2.88 + 3.54))−1 = 0.3831
</p>
<p>density(xO5, 3) = (
1
</p>
<p>3
· (1.41 + 2.66 + 2.84))−1 = 0.4342
</p>
<p>density(xO4, 3) = (
1
</p>
<p>3
· (0.96 + 1.76 + 1.52))−1 = 0.7075
</p>
<p>a.r.d.(x, K) =
density(xO8, 3)
</p>
<p>1
3 (density(xO7, 3) + density(xO6, 3) + density(xO4, 3))
</p>
<p>=
0.2836
</p>
<p>1
3 · (0.3831 + 0.4342 + 0.7075)
</p>
<p>= 0.56
</p>
<p>20.5 The correct answer is D: The true ARD is 0.75. The nearest neighbour of o1 is o2 and the
density at each of these points is respectively 14 and
</p>
<p>1
3 from which it follows the a.a.d is</p>
<p></p>
</div>, <div class="page"><p></p>
<p>338 Solutions
</p>
<p>a.r.d.(x,K) =
density(x,K)
</p>
<p>1
K
</p>
<p>∑
z∈N(x,K) density(z,K)
</p>
<p>=
1/4
</p>
<p>1/3
=
</p>
<p>3
</p>
<p>4
</p>
<p>20.6 The correct answer is A: The true density is 0.043. This is easiest to compute as
</p>
<p>p(x = o1) =
</p>
<p>8∑
i=1
</p>
<p>1
</p>
<p>8
</p>
<p>1
</p>
<p>2λ
exp(−d(o1, oi)/λ)
</p>
<p>Problems of Chapter 21
</p>
<p>21.1 The correct answer is C: For a set to have support more than 40% the set must occur at
least 5 out of the 12 times. All the itemsets that have this property are {ML},{MH}, {PH}, {PL},
{DH}, {DL}, {MH , PH}, {ML, DL}.
21.2 The correct answer is C: The confidence is given as
</p>
<p>P (PL = 1|ML = 1, DL = 1) =
P (PL = 1,ML = 1, DL = 1)
</p>
<p>P (ML = 1, DL = 1)
</p>
<p>=
3/12
</p>
<p>5/12
= 3/5(=
</p>
<p>σPL,ML,DL
σML,DL
</p>
<p>)
</p>
<p>21.3 The correct answer is B: For a set to have support more than 52% the set must occur at
least 6 out of the 10 times. All the itemsets that have this property are {Y AN},{OAY }, {PAY },
{Y AN , PAY }, {OAY , PAY }.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>A
</p>
<p>Mathematical Notation</p>
<p></p>
</div>, <div class="page"><p></p>
<p>340 A Mathematical Notation
</p>
<p>We have tried to keep the mathematical content of the book to the minimum necessary to
achieve a proper understanding. However, this minimum level is nonzero, and the reader should
have a basic grasp of linear algebra, calculus, analysis and probability theory.
</p>
<p>Elementary notation
</p>
<p>Numbers are denoted by variables such as x and the set of all real numbers (0, 10, 13 ,−4,
√
</p>
<p>2, π, etc.)
will be denoted as R. The notation Rd will denote the product space, and so if we consider a vector:
</p>
<p>x =
</p>
<p>−14
1
</p>
<p>
we will write this as x ∈ R3 and the notation: R3 is used to indicate all three-dimensional vectors.
Notice vectors are bolded and are typically lower-case roman letters x,y, · · · . We use R+ = [0,∞[
for the set of all non-negative numbers. The symbol T is used to indicate the transpose of a vector
or matrix. For instance
</p>
<p>xT =
[
−1 4 1
</p>
<p>]
.
</p>
<p>Uppercase bold roman letters W ,A,B, · · · indicate matrices, for instance
</p>
<p>A =
</p>
<p>[
−1 0 2
1 1 −2
</p>
<p>]
,
</p>
<p>in which case we say A is a 2 × 3 matrix and we write A ∈ R2×3. The ith element of a vector is
written as xi and the i, j’th element of a matrix as Aij (or Ai,j). For instance x2 = 4 and A2,3 = −2.
</p>
<p>If we have N observations x1, · · · ,xN of a M -dimensional vector
</p>
<p>x =
[
x1 . . . , xM
</p>
<p>]T
,
</p>
<p>we can combine the observations into an N ×M data matrix X
</p>
<p>X =
</p>
<p>x
T
1
...
xTN
</p>
<p> , (A.1)
in which the ith row of X corresponds to the row vector xTi . We will use this notation for our
data matrix and the rows of X will correspond to N observations and the M columns of X will
correspond to M attributes.
</p>
<p>Finally the expectation of a function f , for instance f(x, y) = sin(x)e−y−x, with respect to a
random variable x having the density function p(x) will be denoted
</p>
<p>Ex[f ] =
∫ ∞
−∞
</p>
<p>f(x, y)p(x) dx.
</p>
<p>In cases where it is clear from the context which variable will be averaged over we will omit the
suffix and simply write E[x2].</p>
<p></p>
</div>, <div class="page"><p></p>
<p>A Mathematical Notation 341
</p>
<p>Linear Algebra
</p>
<p>The matrix product is written as Ax. Recall for two matrices A,B: (AB)T = BTAT and that the
identity matrix IM is the M ×M matrix such that for instance
</p>
<p>I4 =
</p>
<p>
1 0 0 0
0 1 0 0
0 0 1 0
0 0 0 1
</p>
<p> .
We may suppress M and simply write I. Remember AI = IA = A assuming the dimensions
match.
</p>
<p>An N×M matrixA is said to be symmetric ifAT = A and quadratic if N = M . For a quadratic
matrix A, if there exists a matrix A−1 such that
</p>
<p>AA−1 = A−1A = I,
</p>
<p>then A is said to be invertible and A−1 is the inverse. A necessary and sufficient requirement is
that the determinant of A, det(A), is non-zero.
</p>
<p>Subspaces and Eigenvalues
</p>
<p>If we are given vectors x1, . . . ,xn ∈ Rd and numbers a1, . . . , an ∈ R then the vector
x = a1x1 + a2x2 + · · ·+ anxn,
</p>
<p>is said to be a linear combination of x1, . . . ,xn. A subspace V of a vector space Rd is a set of vectors
V which is closed under linear combination. That is if x1, · · · , xn ∈ V then
</p>
<p>a1x1 + a2x2 + · · ·+ anxn ∈ V.
All vectors that can be written as a linear combination of a set of vectors x1, . . . ,xn is called
</p>
<p>the span of x1, . . . ,xn. Notice the span is a subspace of Rd.
A set of vectors x1, . . . ,xn is said to be linearly independent if
</p>
<p>0 = a1x1 + a2x2 + · · ·+ anxn,
implies a1 = a2 = · · · = an = 0. Otherwise, they are said to be linearly dependent. For each
subspace V of Rd it is possible to find a set of vectors x1, . . . ,xn, n ≤ d such that x1, . . . ,xd are
linearly independent and such that the span of x1, . . . ,xn is V . Such as set is known as a basis of
V and n is the dimension of the subspace V . The basis is further said to be orthonormal if
</p>
<p>xTi xj = δij ,
</p>
<p>where δij = 1 if i = j and 0 otherwise. Finally, recall that if A is quadratic and there exists a vector
x 6= 0 and a number λ such that
</p>
<p>Ax = λx,
</p>
<p>then x is said to be an eigenvector of A and λ the corresponding eigenvalue. Suppose x1,x2 are
eigenvectors of A with eigenvalues λ1, λ2 and suppose λ1 6= λ2 then x1,x2 are orthogonal:
</p>
<p>xT1 x2 = 0.
</p>
<p>In particular, if A is a d× d symmetric matrix, AT = A, then there exists an orthonormal basis of
eigenvectors for Rd.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>342 A Mathematical Notation
</p>
<p>Analysis
</p>
<p>A function f that maps from a D dimensional space to a single real number will be written as
</p>
<p>f : RD → R,
</p>
<p>to explicitly specify the dimensions of the spaces f map between. This notation, as well as the
notation x ∈ Rd, may appear cumbersome at a first glance however when considering functions
between high-dimensional spaces it will often be a helpful guide to keep track of what the functions
do. For the same reason we will only use this notation when it benefits the readability and not for
formal exactness.
</p>
<p>High-dimensional functions will play a special role in the following and so if we consider a
function which maps from a D dimensional space to a M dimensional space (M &gt; 1) we will write
f with a boldface:
</p>
<p>f : RD → RM .
To give two quick examples, if
</p>
<p>x =
</p>
<p>x1x2
x3
</p>
<p> ,
then we can define f : R3 → R where:
</p>
<p>f(x) = x1 + x2x3,
</p>
<p>or another example g : R3 → R2
</p>
<p>g(x) =
</p>
<p>[
x1 + sin(x2)
x3 + 2e
</p>
<p>−x1
</p>
<p>]
.
</p>
<p>Notice, we may also sometimes write f(x) as f(x1, x2, x3). We assume the reader is familiar with
derivative evaluated at a point x or the partial derivatives evaluated in x
</p>
<p>df
</p>
<p>dx
(x) and
</p>
<p>∂f
</p>
<p>∂x2
(x), (A.2)
</p>
<p>as well as integrals over some or all variables of a function:
</p>
<p>I =
</p>
<p>∫
RD
</p>
<p>f(x) dx, I(x1) =
</p>
<p>∫
R2
f(x1, x2, x3) dx2dx3 (A.3)
</p>
<p>However, knowledge of integrating actual function will not be used and integrals are mostly used
to state theoretical results. Finally we will use ∇f(x) as the divergence of a function f : RD → R
evaluated at x:
</p>
<p>∇f(x) =
</p>
<p>
∂f
∂x1
</p>
<p>(x)
...
</p>
<p>∂f
∂xD
</p>
<p>(x)
</p>
<p></p>
<p></p>
</div>, <div class="page"><p></p>
<p>A Mathematical Notation 343
</p>
<p>Slightly more advanced concepts
</p>
<p>Recall the multivariate Taylor expansion of a function f : RD → R around a point x can be written
as
</p>
<p>f(x+ δ) = f(x) + δT∇f(x) + higher order terms .
For one dimension this is the familiar result
</p>
<p>f(x+ δ) = f(x) + δ
df
</p>
<p>dx
(x) + higher order terms .
</p>
<p>We will use the notation
x∗ = arg min
</p>
<p>x
f(x),
</p>
<p>as the value x∗ that minimizes f and similar arg max to find the maximum. Recall also that if
we wish to find the minimum or maximum of a function f : RD under a constraint that another
function h : RD → R is zero, written as:
</p>
<p>x∗ = arg min
x:h(x)=0
</p>
<p>f(x).
</p>
<p>This can be done by introducing a Lagrange multiplier λ ∈ R and solve the problem
</p>
<p>f(x) + λh(x) = 0,
</p>
<p>by taking derivatives with respect to x and λ and set these equal to zero. That is, by simultaneous
solving the D + 1 equations:
</p>
<p>∇f(x) + λ∇h(x) = 0 and h(x) = 0.
</p>
<p>We will only use this technique once and a reader not familiar with the method of Lagrange
multipliers should consult the many excellent guides available online as texts or videos.
</p>
<p>Probability Theory
</p>
<p>In probability theory, we always consider the probability of an event, i.e. something which either
does or does not occur. The probability of an event is written with an upper-case P and is a number
between 0 and 1. For instance if we consider the outcome of a coin-flip, the outcome that the coin
is heads (or tails) are events which either do or do not occur and so we can let B = 0 denote the
event the coin is heads and B = 1 if the coin is tails we will write
</p>
<p>P (B = 0) = 0.4, P (B = 1) = 0.6,
</p>
<p>to indicate there is a 40% chance the coin is heads. B is called a random or stochastic variable, i.e.
something which outcome is the result of a random process or is otherwise unknown. If we suppose
b corresponds to the actual result of the coin-flip (i.e. a person writes down b = 0 if the coin came
out heads and b = 1 if the coin came out tails) we can write the probability of the outcome as
simply:
</p>
<p>P (B = b)</p>
<p></p>
</div>, <div class="page"><p></p>
<p>344 A Mathematical Notation
</p>
<p>Sometimes this will be abbreviated as
P (b).
</p>
<p>We stress probabilities are computed of events (boolean occurrences) and it is therefore not possible
to talk about the probability of a continuous number, for instance the probability Napoleon was
exactly 1.731 meters tall. For continuous numbers we use the probability density function (sometimes
simply the probability density) which, for a random variable (x1, x2, . . . , xd) ∈ Rd is a non-negative
function
</p>
<p>p : Rd → R+,
which integrates to one: ∫
</p>
<p>Rd
p(x1, . . . , xd)dx1, . . . , dxd = 1,
</p>
<p>where R+ is the interval [0,∞[. Since the integration limits are often redundant we may also write
the above as simply ∫
</p>
<p>Rd
p(x) dx = 1.
</p>
<p>The probability density function is not the same as a probability, however you can obtain proba-
bilities by integrating the probability density function. For d = 1 we can for instance consider the
probability density function
</p>
<p>p(x) =
1√
2π
e−
</p>
<p>x2
</p>
<p>2 ,
</p>
<p>for which
∫
p(x) dx = 1. In this case the probability that the random variable x falls within the
</p>
<p>interval [2, 3] is simply:
</p>
<p>P (x ∈ [2, 3]) =
∫ 3
2
</p>
<p>p(x) dx.
</p>
<p>Notice, this is a proper probability. In the Napoleon example too we are allowed to say consider
the probability Napoleon was between 1.73 and 1.75 meters tall which is a proper event. Confusion
of the probability density function and probabilities is common and may lead to difficulties later.
Probability theory is a rich and interesting mathematical discipline and this introduction does not
do it service; however, in this book we will take a “naive approach” and not dwell on the details.
</p>
<p>Finally the expectation of a function f(x, y) with respect to a random variable x governed by
probability density p is written as
</p>
<p>Ex[f ] =
∫
p(x)f(x, y) dx.</p>
<p></p>
</div>, <div class="page"><p></p>
<p>References
</p>
<p>Rakesh Agrawal, Tomasz Imieliński, and Arun Swami. Mining association rules between sets of
items in large databases. In Acm sigmod record, volume 22, pages 207–216. ACM, 1993.
</p>
<p>Rakesh Agrawal, Ramakrishnan Srikant, et al. Fast algorithms for mining association rules. In
Proc. 20th int. conf. very large data bases, VLDB, volume 1215, pages 487–499, 1994.
</p>
<p>RE Barlow. Introduction to de finetti (1937) foresight: its logical laws, its subjective sources. In
Breakthroughs in statistics, pages 127–133. Springer, 1992.
</p>
<p>Mr. Bayes and Mr. Price. An essay towards solving a problem in the doctrine of chances. by
the late rev. mr. bayes, f. r. s. communicated by mr. price, in a letter to john canton, a. m.
f. r. s. Philosophical Transactions, 53:370–418, 1763. doi: 10.1098/rstl.1763.0053. URL http:
//rstl.royalsocietypublishing.org/content/53/370.short.
</p>
<p>Christoph Bergmeir, Rob J Hyndman, and Bonsoo Koo. A note on the validity of cross-validation
for evaluating autoregressive time series prediction. Computational Statistics &amp; Data Analysis,
120:70–83, 2018.
</p>
<p>C.M. Bishop. Pattern Recognition and Machine Learning. Information science and statis-
tics. Springer, 2013. ISBN 9788132209065. URL https://books.google.dk/books?id=
HL4HrgEACAAJ.
</p>
<p>Remco R Bouckaert and Eibe Frank. Evaluating the replicability of significance tests for comparing
learning algorithms. In Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages
3–12. Springer, 2004.
</p>
<p>L. Breiman, J. Friedman, C.J. Stone, and R.A. Olshen. Classification and Regression Trees.
The Wadsworth and Brooks-Cole statistics-probability series. Taylor &amp; Francis, 1984. ISBN
9780412048418. URL https://books.google.co.in/books?id=JwQx-WOmSyQC.
</p>
<p>Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
David A Burn. 22 designing effective statistical graphs. 1993.
Nitesh V Chawla. Data mining for imbalanced datasets: An overview. In Data mining and knowledge
</p>
<p>discovery handbook, pages 853–867. Springer, 2005.
P Collinson. Of bombers, radiologists, and cardiologists: time to roc. Heart, 80(3):215–217, 1998.
Paulo Cortez, António Cerdeira, Fernando Almeida, Telmo Matos, and José Reis. Modeling wine
</p>
<p>preferences by data mining from physicochemical properties. Decision Support Systems, 47(4):
547–553, 2009.
</p>
<p>Richard T Cox. Probability, frequency and reasonable expectation. American journal of physics,
14(1):1–13, 1946.</p>
<p></p>
<div class="annotation"><a href="http://rstl.royalsocietypublishing.org/content/53/370.short">http://rstl.royalsocietypublishing.org/content/53/370.short</a></div>
<div class="annotation"><a href="http://rstl.royalsocietypublishing.org/content/53/370.short">http://rstl.royalsocietypublishing.org/content/53/370.short</a></div>
<div class="annotation"><a href="https://books.google.dk/books?id=HL4HrgEACAAJ">https://books.google.dk/books?id=HL4HrgEACAAJ</a></div>
<div class="annotation"><a href="https://books.google.dk/books?id=HL4HrgEACAAJ">https://books.google.dk/books?id=HL4HrgEACAAJ</a></div>
<div class="annotation"><a href="https://books.google.co.in/books?id=JwQx-WOmSyQC">https://books.google.co.in/books?id=JwQx-WOmSyQC</a></div>
</div>, <div class="page"><p></p>
<p>346 References
</p>
<p>Belur V Dasarathy and Belur V Sheela. A composite classifier system design: concepts and method-
ology. Proceedings of the IEEE, 67(5):708–713, 1979.
</p>
<p>Bruno De Finetti. La prévision: ses lois logiques, ses sources subjectives. In Annales de l’institut
Henri Poincaré, volume 7, pages 1–68, 1937.
</p>
<p>Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete
data via the em algorithm. Journal of the royal statistical society. Series B (methodological),
pages 1–38, 1977.
</p>
<p>Evelyn Fix and Joseph L Hodges. Discriminatory analysis-nonparametric discrimination: consis-
tency properties. Technical report, DTIC Document, 1951.
</p>
<p>Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of computer and system sciences, 55(1):119–139, 1997.
</p>
<p>J.G. Garnier and A. Quételet. Correspondance mathématique et physique. Number v. 10. Impr.
d’H. Vandekerckhove, 1838. URL https://books.google.dk/books?id=8GsEAAAAYAAJ.
</p>
<p>C.F. Gauß. Theoria Motvs Corporvm Coelestivm In Sectionibvs Conicis Solem Ambientivm. Frid.
Perthes et J. H. Besser, 1809. URL https://books.google.dk/books?id=7jJbAAAAcAAJ.
</p>
<p>Teofilo F Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical Com-
puter Science, 38:293–306, 1985.
</p>
<p>Alan Hájek. “mises redux”—redux: Fifteen arguments against finite frequentism. In Probability,
Dynamics and Causality, pages 69–87. Springer, 1997.
</p>
<p>Alan Hájek. Fifteen arguments against hypothetical frequentism. Erkenntnis, 70(2):211–235, 2009.
Lars Kai Hansen and Peter Salamon. Neural network ensembles. IEEE transactions on pattern
</p>
<p>analysis and machine intelligence, 12:993–1001, 1990.
T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining,
</p>
<p>Inference, and Prediction, Second Edition. Springer Series in Statistics. Springer New York,
2009. ISBN 9780387848587. URL https://books.google.dk/books?id=tVIjmNS3Ob8C.
</p>
<p>Tin Kam Ho. Random decision forests. In Document Analysis and Recognition, 1995., Proceedings
of the Third International Conference on, volume 1, pages 278–282. IEEE, 1995.
</p>
<p>Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal
of educational psychology, 24(6):417, 1933.
</p>
<p>Earl B Hunt, Janet Marin, and Philip J Stone. Experiments in induction. 1966.
G. James, D. Witten, T. Hastie, and R. Tibshirani. An Introduction to Statistical Learning: with
</p>
<p>Applications in R. Springer Texts in Statistics. Springer New York, 2014. ISBN 9781461471370.
URL https://books.google.dk/books?id=at1bmAEACAAJ.
</p>
<p>Edwin T Jaynes. Prior probabilities. IEEE Transactions on systems science and cybernetics, 4(3):
227–241, 1968.
</p>
<p>Edwin T Jaynes. Probability theory: The logic of science. Cambridge university press, 2003.
H. Jeffreys. Theory of Probability. The International series of monographs on physics. At The
</p>
<p>Clarendon Press, 1939. URL https://books.google.dk/books?id=6_ogAAAAMAAJ.
Harold Jeffreys. An invariant form for the prior probability in estimation problems. Proc. R. Soc.
</p>
<p>Lond. A, 186(1007):453–461, 1946.
Stephen C Johnson. Hierarchical clustering schemes. Psychometrika, 32(3):241–254, 1967.
D. Kahneman, P. Slovic, and A. Tversky. Judgment Under Uncertainty: Heuristics and Biases.
</p>
<p>Cambridge University Press, 1982. ISBN 9780521284141. URL https://books.google.co.uk/
books?id=_0H8gwj4a1MC.
</p>
<p>Ron Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection.
pages 1137–1143. Morgan Kaufmann, 1995.</p>
<p></p>
<div class="annotation"><a href="https://books.google.dk/books?id=8GsEAAAAYAAJ">https://books.google.dk/books?id=8GsEAAAAYAAJ</a></div>
<div class="annotation"><a href="https://books.google.dk/books?id=7jJbAAAAcAAJ">https://books.google.dk/books?id=7jJbAAAAcAAJ</a></div>
<div class="annotation"><a href="https://books.google.dk/books?id=tVIjmNS3Ob8C">https://books.google.dk/books?id=tVIjmNS3Ob8C</a></div>
<div class="annotation"><a href="https://books.google.dk/books?id=at1bmAEACAAJ">https://books.google.dk/books?id=at1bmAEACAAJ</a></div>
<div class="annotation"><a href="https://books.google.dk/books?id=6_ogAAAAMAAJ">https://books.google.dk/books?id=6_ogAAAAMAAJ</a></div>
<div class="annotation"><a href="https://books.google.co.uk/books?id=_0H8gwj4a1MC">https://books.google.co.uk/books?id=_0H8gwj4a1MC</a></div>
<div class="annotation"><a href="https://books.google.co.uk/books?id=_0H8gwj4a1MC">https://books.google.co.uk/books?id=_0H8gwj4a1MC</a></div>
</div>, <div class="page"><p></p>
<p>References 347
</p>
<p>DD Kosambi. Statistics in function space. J. Indian Math. Soc, 7(1):76–88, 1943.
A.M. Legendre. Nouvelles méthodes pour la détermination des orbites des comètes. F. Didot, 1805.
</p>
<p>URL https://books.google.dk/books?id=FRcOAAAAQAAJ.
David JC MacKay. Information theory, inference and learning algorithms. Cambridge university
</p>
<p>press, 2003.
James MacQueen et al. Some methods for classification and analysis of multivariate observations. In
</p>
<p>Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1,
pages 281–297. Oakland, CA, USA., 1967.
</p>
<p>Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity.
The bulletin of mathematical biophysics, 5(4):115–133, 1943.
</p>
<p>Claude Nadeau and Yoshua Bengio. Inference for the generalization error. In Advances in neural
information processing systems, pages 307–313, 2000.
</p>
<p>Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe-
matical statistics, 33(3):1065–1076, 1962.
</p>
<p>Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan
Kaufmann, 2014.
</p>
<p>Judea Pearl, Madelyn Glymour, and Nicholas P Jewell. Causal Inference in Statistics: A Primer.
John Wiley &amp; Sons, 2016.
</p>
<p>Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions
of the Royal Society of London. A, 185:71–110, 1894.
</p>
<p>Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. Philosophical
Magazine Series 6, 2(11):559–572, 1901. doi: 10.1080/14786440109462720. URL http://dx.doi.
org/10.1080/14786440109462720.
</p>
<p>Vikas C Raykar and Ramani Duraiswami. Fast optimal bandwidth selection for kernel density
estimation. In SDM, pages 524–528. SIAM, 2006.
</p>
<p>Murray Rosenblatt et al. Remarks on some nonparametric estimates of a density function. The
Annals of Mathematical Statistics, 27(3):832–837, 1956.
</p>
<p>Robert E Schapire. The strength of weak learnability. Machine learning, 5(2):197–227, 1990.
Steve Selvin, M. Bloxham, A. I. Khuri, Michael Moore, Rodney Coleman, G. Rex Bryce, James A.
</p>
<p>Hagans, Thomas C. Chalmers, E. A. Maxwell, and Gary N. Smith. Letters to the editor.
The American Statistician, 29(1):67–71, 1975. ISSN 00031305. URL http://www.jstor.org/
stable/2683689.
</p>
<p>C. E. Shannon. A mathematical theory of communication. The Bell System Technical Journal, 27
(3):379–423, July 1948. ISSN 0005-8580. doi: 10.1002/j.1538-7305.1948.tb01338.x.
</p>
<p>H. Steinhaus. Sur la division des corp materiels en parties. Bull. Acad. Polon. Sci, 1:801–804, 1956.
S. S. Stevens. On the theory of scales of measurement. Science, 103(2684):677–680, 1946. ISSN 0036-
</p>
<p>8075. doi: 10.1126/science.103.2684.677. URL http://science.sciencemag.org/content/103/
2684/677.
</p>
<p>Gilbert W Stewart. On the early history of the singular value decomposition. SIAM review, 35(4):
551–566, 1993.
</p>
<p>Alexander Strehl and Joydeep Ghosh. Cluster ensembles—a knowledge reuse framework for com-
bining multiple partitions. Journal of machine learning research, 3(Dec):583–617, 2002.
</p>
<p>P.N. Tan, M. Steinbach, and V. Kumar. Introduction to Data Mining. Addison-Wesley, 2013. ISBN
9780133128901. URL https://books.google.dk/books?id=_ZQ4MQEACAAJ.
</p>
<p>Andrey Nikolayevich Tikhonov. On the stability of inverse problems. In Dokl. Akad. Nauk SSSR,
volume 39, pages 195–198, 1943.</p>
<p></p>
<div class="annotation"><a href="https://books.google.dk/books?id=FRcOAAAAQAAJ">https://books.google.dk/books?id=FRcOAAAAQAAJ</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1080/14786440109462720">http://dx.doi.org/10.1080/14786440109462720</a></div>
<div class="annotation"><a href="http://dx.doi.org/10.1080/14786440109462720">http://dx.doi.org/10.1080/14786440109462720</a></div>
<div class="annotation"><a href="http://www.jstor.org/stable/2683689">http://www.jstor.org/stable/2683689</a></div>
<div class="annotation"><a href="http://www.jstor.org/stable/2683689">http://www.jstor.org/stable/2683689</a></div>
<div class="annotation"><a href="http://science.sciencemag.org/content/103/2684/677">http://science.sciencemag.org/content/103/2684/677</a></div>
<div class="annotation"><a href="http://science.sciencemag.org/content/103/2684/677">http://science.sciencemag.org/content/103/2684/677</a></div>
<div class="annotation"><a href="https://books.google.dk/books?id=_ZQ4MQEACAAJ">https://books.google.dk/books?id=_ZQ4MQEACAAJ</a></div>
</div>, <div class="page"><p></p>
<p>348 References
</p>
<p>Edward R Tufte, Nora Hillman Goeler, and Richard Benson. Envisioning information, volume 126.
Graphics press Cheshire, CT, 1990.
</p>
<p>Alan M Turing. Computing machinery and intelligence. Mind, pages 433–460, 1950.
Gitte Vanwinckelen and Hendrik Blockeel. On estimating model accuracy with repeated cross-
</p>
<p>validation. In BeneLearn 2012: Proceedings of the 21st Belgian-Dutch Conference on Machine
Learning, pages 39–44, 2012.</p>
<p></p>
</div>]
